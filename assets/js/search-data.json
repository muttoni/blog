{
  
    
        "post0": {
            "title": "Implementing a Deep Neural Network from Scratch - Part 2",
            "content": "Introduction . . Note: This post is the second post of a two-part series. If you landed on this page directly, I recommend following the whole journey and start by reading Part 1 first. . This is Part 2 of our &quot;Implementing a Deep Neural Network from Scratch&quot; series. In this part we will improve on our Linear classifier started in Part 1 and turn our Linear Model into a fully-functioning Deep Neural Network! . As a reminder, our goal is to create a Neural Network that can distinguish all 10 handwritten digits (using the industry standard MNIST dataset). This is also known as a Multi-Label Classifier (a traditional Classifier only has two labels, such as cat or dog, hotdog or not hotdog, etc). In Part 1 we were able to train a simple linear regressor achieving roughly 80% accuracy after 50 epochs. We&#39;ll see in this part how to make it even more powerful with just a simple tweaks, as well as turn it into a Deep Neural Network. . Getting Started . Below is a recap of all the preparatory steps to setup our data pipeline. In order: we download the data, generate a list of file paths, create training and validation tensor stacks from the file paths, convert those tensors from rank-2 (2D matrix) tensors (i.e. size: 28, 28) to rank-1 (1D vector) tensors (i.e. size: 784). . # Requirements !pip install -Uq fastbook from fastbook import * # Download the data path = untar_data(URLs.MNIST) # Import the paths of our training and testing images training = { f&#39;{num}&#39; : (path/f&#39;training/{num}&#39;).ls().sorted() for num in range(10) } testing = { f&#39;{num}&#39; : (path/f&#39;testing/{num}&#39;).ls().sorted() for num in range(10) } # Prepare training tensor stacks training_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in training[f&#39;{num}&#39;] ]) for num in range(10) ] validation_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in testing[f&#39;{num}&#39;] ]) for num in range(10) ] # Convert our 2D image tensors (28, 28) into 1D vectors train_x = torch.cat(training_tensors).view(-1, 28*28) valid_x = torch.cat(validation_tensors).view(-1, 28*28) . . Before generating our labels, we&#39;ll make a change in our labeling structure so as to be more efficient in space complexity as well as be more elegant and extendable for any number of labels in the future. . Improving our Label Structure . Our labels in Part 1 were represented by a vector of size 10 with a &#39;1&#39; flag indicating the correct digit at the corresponding index, and &#39;0&#39; everywhere else. In our loss function, we then compared whether the max index of our predictions was equivalent to the index in our target labels where the index was &#39;1&#39;. Here&#39;s a quick example of how our old labels worked: . A target label $y$ for the digit &#39;3&#39;: . [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] ^ a &#39;1&#39; flagging the correct digit at its index (3) . An example prediction from our model: . [2.0e-3, 1.3e-7, 4.0e-8, 9.9e-1, 2.0e-5, 1.6e-4, 4.4e-9, 7.0e-9, 1.3e-6, 2.1e-4, 4.0e-7] ^ index of max value corresponds to the y label index, indicating a correct prediction. . There is, however, a much more efficient way of representing our labels. Can you guess what it is? If you guessed assigning a value to each of our digits (e.g. an integer corresponding to that digit) and just have that be our $y$ target label, then you guessed right! But in our loss function, how can we match/compare a prediction vector of 10 values to a target label, if the label contains only an integer (e.g. &quot;3&quot;). . To answer this question we need to go back to our beloved Softmax. . $ text{Softmax}(x_i) = frac{e^{x_i}}{ sum_j{e^{x_j}}}$ . This is a great time to read Part 1 if you haven&#39;t already done so. Here&#39;s a quick reminder of how our Softmax works in three steps: . Given a list of values, we calculate the exponential for each value, that is: $e^{n}$. Where $n$ is our value and $e$ is Euler&#39;s number, a special number in Mathematics, and at the heart of the exponential function. | We sum the exponential values (the Sigma symbol $ sum$ means sum). | Take each value in Step 1. and divide it by the sum obtained in Step 2. | The pecularity of Softmax is that when we softmax a list of values, since they are all divided by their sum, all the values sum to 1. This is incredibly useful as it means we only really need 1 value from the list, i.e. in our case the prediction corresponding to the correct label, as the rest of the values are just 1 - that value. If that value increases (i.e. the probability of our correct digit prediction increases), it also means that every other value in the prediction vector is consequently decreasing. This is why Softmax is a critical part of any multi-label classifier where only 1 label out of a group is correct (e.g. model of car), as it helps &quot;hone in&quot; on a single value, and work around that. . Practically, this means we can simplify our label structure and go from a vector of flags to a single integer representing the index of our possible labels. In the case of our MNIST dataset, the labels and indexes match, and will both go from 0 to 10. This means that if our $y$ variable is &#39;0&#39;, our digit is &#39;0&#39;, and we should look at the prediction value of our output vector at index &#39;0&#39;. The same goes for every other number. In cases were our $y$ is not a digit, we would still assign an integer to each possible label and proceed in exactly the same manner. . train_y = torch.from_numpy(np.concatenate([[i]*len(training[f&#39;{i}&#39;]) for i in range(10)])) valid_y = torch.from_numpy(np.concatenate([[i]*len(testing[f&#39;{i}&#39;]) for i in range(10)])) . In our loss and accuracy functions, we&#39;ll also need to update the way we compare results. All it takes is a little code change (our loss function will change a little bit in the next section, for now just focus on the updated labeling mechanism). . def loss_function(self, predictions, targets): # softmax the vectors, each containing 10 probabilities sm_preds = self.softmax(predictions) # generate a range equal to length of prediction batch idx = range(len(predictions)) # from each vector, only pick the correct prediction results = sm_preds[idx, targets] ... . This nifty accessing technique, coupled with Softmax&#39;s characteristic of summing to one, means that with only need a single value from our predictions, not the whole vector, to calculate the prediction loss, as every other value in our prediction vector is completely co-dependent and therefore irrelevant. . We can now load our dataset and dataloaders as we did in Part 1: . # Create datasets to feed into the dataloaders dset = list(zip(train_x, train_y)) dset_valid = list(zip(valid_x, valid_y)) # Setup our dataloders dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=256, shuffle=True) . Softmax Needs a Friend . Before we can proceed to improve and generalize our MNIST model from Part 1, we need to continue talking about Softmax. Yes, I know, by now you probably think I have a secret fetish for Softmax, and you&#39;re probably right. But Softmax needs a little TLC in order to function orders of magnitude better. . What the hell am I going on about? Well, Softmax is based on the exponential function $e^x$, which means that as our $x$ value increases, the exponential values increase...exponentially. The consequence of this is that a parameter that is slightly larger than the rest, when softmax&#39;d, will tend to be exaggerated, squishing the rest. Here is an example (toggle to code block to see the softmax implementation): . # Quick softmax implementation, expecting a list of values (rank-1 tensor) def softmax(v): &quot;&quot;&quot; Given an input vector v, returns an output vector containing the softmax values for each element in the vector &quot;&quot;&quot; exp_v = torch.exp(v.float()) exp_sum = exp_v.sum() return exp_v / exp_sum . . softmax(tensor([1,2,4])) . tensor([0.0420, 0.1142, 0.8438]) . Notice how in the example above, the &#39;4&#39;, when softmax&#39;d, takes a larger-than-proportional share of the probability. As the difference gets larger, this becomes even more evident. This is great when making final predictions as we want the highest probability to shine through, but not when we want a nice stable loss function to help the model improve. As soon as a parameter is slightly larger than the rest, it will be hard for the others to compete and &quot;catch up&quot;. We&#39;ll see how we can convert our naive loss function into an industry standard one, by softening Softmax (ironic, isn&#39;t it?). We&#39;ll then compare performance and you&#39;ll see how a very small change will help get us better and more stable training accuracy in just a few epochs! . Log: a Softmax&#39;s Best Friend . I&#39;ve never liked logs, unless they are made of wood. But it seems logs (aka logarithms) are a crucial part of everyday life, as they help represent exponential values in linear form. I won&#39;t dive too deep into logarithms, as there are plenty of resources that will do a better job at explaining them. Feel free to look them up for a couple minutes before continuing this post just so you have a quick primer. . Since Softmax stems from the exponential function ($e^x$), by taking the log of the softmax&#39;d values we are able to represent numers that are distanced exponentially in a more linear way. This is what I mean by &quot;softening&quot; Softmax, as rather than dealing with the raw values we are dealing with their exponents in base e. This helps stretch the values that were otherwise confined between 0 and 1, to a much larger range...what is the largest range imaginable? Infinity! Actually, for softmax it&#39;s 0 to -Infinity to be precise, and we&#39;ll see why in just a second. . . Tip: Our log function, in the context of Softmax, is always in base $e$, as our values are raised to the power of $e$. In Mathematics, logarithms involving $e$ (i.e. in base $e$) are often referred to as $ln$. . Our log function output (y-axis) can be intuitively thought of as: &quot;to the power of what must we raise $e$ in order to get $x$. This is why log is not defined for $x &lt; 0$ (remember: $e$ is positive, roughly equal to 2.718, so an exponent will never be able to change the sign of a positive number). . From the log plot above, we can see that $log_e(x)$ outputs values from -Infinity as $x$ approaches 0, and approaches 0 when $x$ approaches 1. Consequently, our Softmax values (that cumulatively sum to 1, and therefore always between 0 and 1) will always be negative. Let&#39;s take a look at our previous example with [1,2,4], softmax them again and then take the log. Notice how the results are negative, and the relative ratios between them have softened. . torch.log(softmax(tensor([1,2,4]))) . tensor([-3.1698, -2.1698, -0.1698]) . We can multiply very value by -1 so as to deal with positive numbers, without changing the fact that the highest original number (4) is now the closest to 0...that is...closest to our minimum. . So we now have a function that squeezes original parameters into co-dependent probabilties (via softmax), then stretches them into a broader logarithmic range (via log) to help linearize the rate of growth, and at the same time making it so that higher values turn into something that can be minimized (approach zero). Now doesn&#39;t that sound perfect for a loss function? . Cross-Entropy Loss . What we&#39;ve just seen above is a very popular loss function used in classification called Cross-Entropy Loss. It allows us to leverage the useful characteristics of softmax while retaining a relatively linear curve for parameter values so as to facilitate training. We&#39;ll see that by making this small change in our previous model, our training will be much more effective. . For reference, here is our old code (feel free to toggle it). . class MNISTLinearClassifier: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) # Return digit and vector of probabilities return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.mean().backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) # get the index of max value along 2nd dimension _, tag_indices = yb.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match return corrects.float().mean() # calculate mean def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): predictions = self.softmax(predictions) return torch.where(targets==1, 1-predictions, predictions).mean(-1) def _print(self, *args): if self.verbose: print(*args) # Linear regression using SGD def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Below is our new, updated code. We&#39;ll leverage the new label structure and implement a new loss function by performing all the steps discussed in the previous section: log our softmax values, and turn them positive. Everything else is pretty much identical (aside from our _batch_accuracy function that needed updating due our new labeling structure too). Feel free to diff it into diffchecker to see what changed. . class MNISTLinearClassifier: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): # Cross-Entropy Loss log_sm_preds = torch.log(self.softmax(predictions)) idx = range(len(predictions)) loss = -log_sm_preds[idx, targets] return loss.mean() def _print(self, *args): if self.verbose: print(*args) def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Now remember our old model took 50 epochs (on a good run) to reach 80% accuracy. If we try our new model now, it will consistently beat 80% accuracy within the first epoch! That&#39;s a considerable improvement thanks to a &quot;log&quot; and a negative sign. Goes to show how critical a proper loss function is in training a model effectively. . model = MNISTLinearClassifier(dl, valid_dl, 5, 0.5, True) . model.train() . Epoch #0 0.8151 Epoch #1 0.8507 Epoch #2 0.8656 Epoch #3 0.8777 Epoch #4 0.8789 . It already beat our original model within the first epoch (81.5% accuracy vs 80% after 50 epochs) and reached ~88% accuracy within 5 epochs. That&#39;s awesome! We could try experimenting with various batch sizes and learning rates to see how accurate we can get our model within an arbitrary number of epochs, but this is beyond the scope of this post. Let&#39;s clean up our code further and turn it into a general Linear Classifier. . Generalizing our Linear Model . Up until now, we wanted to build a MNIST classifier. Before we move on and &quot;deepen&quot; our neural network, let&#39;s clean up our code, modularize it and turn it into a general Linear Classifier that can be applied to any dataset, with any number of labels. . Linear Model . We&#39;ll start by generalizing our Linear equation into a standalone &quot;LinearModel&quot;. It will simply initialize a tensor with a desired number of input and output parameters (weights and biases) with random values. The model method will carry out the actual linear equation, and the parameters method will return the weights and biases. . class LinearModel: def __init__(self, inputs, outputs): self.input_size = inputs self.output_size = outputs self.weights, self.bias = self._init_params() def parameters(self): return self.weights, self.bias def model(self, x): return x@self.weights + self.bias def _init_params(self): weights = (torch.randn(self.input_size, self.output_size)).requires_grad_() bias = (torch.randn(self.output_size)).requires_grad_() return weights, bias . Optimizer . We&#39;ll then proceed to strip out our optimizer. Our optimizer will have a step method that will carry out the update for each parameter based on its gradient and an arbitrary learning rate lr (e.g. 0.01). . class SGD_Optimizer: def __init__(self, parameters, lr): self.parameters = list(parameters) self.lr = lr def step(self): for p in self.parameters: p.data -= p.grad.data * self.lr for p in self.parameters: p.grad = None . Now let&#39;s rewrite our Classifier to use these two newly created classes, and run it again on the MNIST dataset to check that it works: . class LinearClassifier: def __init__(self, input_shape, output_shape, train_dl, valid_dl, epochs, lr, optimizer, softmax, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.linear = LinearModel(input_shape, output_shape) self.optimizer = optimizer(self.linear.parameters(), self.lr) self.activation = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) self.optimizer.step() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.activation(self.linear.model(image_tensor)) _, prediction = probabilities.max(-1) return prediction, probabilities def _calc_grad(self, xb, yb): preds = self.linear.model(xb) loss = self._loss_function(preds, yb) loss.backward() def _batch_accuracy(self, xb, yb): predictions = self.activation(xb) _, x_label = xb.max(-1) corrects = x_label == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self.linear.model(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log(self.activation(predictions)) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _print(self, *args): if self.verbose: print(*args) . . train_dl = DataLoader(dset, batch_size=128, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=128, shuffle=True) classifier = LinearClassifier( input_shape=28*28, output_shape=10, train_dl=train_dl, valid_dl=valid_dl, epochs=5, lr=0.1, optimizer=SGD_Optimizer, softmax=True, verbose=True) . classifier.train() . Epoch #0 0.7352 Epoch #1 0.8014 Epoch #2 0.8286 Epoch #3 0.8457 Epoch #4 0.8532 . Like before, we could try experimenting with various batch sizes and learning rates -- here I used a batch_size of 128 and a learning rate lr of 0.1. The advantage of smaller batch sizes is that it allows the model to have more chances to perform gradient descent on the parameters before finishing an epoch, but the gradients may be less representative of the total dataset as they are based on fewer samples. The learning rate is also a critical aspect to experiment with as it determines how much of a step is done at every optimization run. If the learning rate is too big, we risk overshooting the minimum loss and potentially never reaching it, if the learning rate is too low, we won&#39;t make significant progress in training. Keep in mind you don&#39;t always need to guess, as some libraries offer a way to test different learning rates (e.g. fast.ai offers a learning rate finder in it&#39;s Learner class.) . Activating our Network! . As mentioned at the conclusion of Part 1, what we&#39;ve done up to now is create a linear classifier with a self-correcting capability through Stochastic Gradient Descent...not a deep neural network. That is, because a neural network is non-linear, and it is composed of multiple layers. How do we break the linearity and add layers? . Is it by adding one or more linear models after the first one? Yes! These are called layers (and any intermediate layers before our output layer are called &quot;hidden layers&quot;). But adding more layers is not enough. If we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. We need something more inbetween each linear classifier that breaks beyond the sad confines of a linear world: an activation function. . Before we tackle the activation function, let&#39;s recap what activations are. Activations are the calculated outputs from our linear equation(s). This means that in our Linear Model above, by creating 10 parameters for each pixel (remember our input weights are of shape (28*28, 10), our linear equation will output 10 activations per image. Since our Linear Classifier was comprised of only 1 layer of parameters, our 10 activations also correspond to the 10 desired outputs of our model, i.e. when softmax&#39;d each of the 10 items in the activation vector represent the probabilities for each digit [0-9] in our classification task. As we&#39;ll see in just a moment, the activations don&#39;t necessarily need to equal to our outputs, as there can be several layers of linear models in a deep neural network (hence the name: &quot;deep&quot;). . So one layer can output an arbitrary number of features (in practice the activations of that layer) and then feed those features into a second layer as inputs, that will output another set of features for the next layer, and so on, until the last layer where the activations correspond to our final output. . However, as we said earlier, if we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. So how can we add more than 1 linear layers and break the inherent linearity? That&#39;s where the activation function comes into play. . The Activation Function . The activation function acts as a filter through which input parameters are transformed from one layer to the next, so as to break the linear relationship. Any function that is not a linear equation technically acts as an non-linearity (e.g. a log function, an exponential function, etc). You can see a collection of activation functions here. . Among those, one function that works really well, is as simple as can be: the Rectifier function (also known as Rectified Linear Unit, or ReLU). . $f(x) = max(x, 0)$ . That&#39;s it! The ReLU will simply replace any negative input values with 0 (or you can read it as: rectify any negative values to 0). Other input values remain completely unchanged. In code, this is as simple as: . def ReLU(x): return x.max(tensor(0.)) plot_function(ReLU, title=&quot;The ReLU Function&quot;) . This simple transformation is enough to break linearity! Feel free to read more about the ReLU function), but for our purposes this is all we need to continue. . Implementing a Deep Neural Network . . Equipped with our handy ReLU, we are ready to transform our Linear Model into a real Neural Network and allow for more than one linear layer! Now remember, a Neural Network is comprised of multiple linear layers (2 being the minimum not counting the activation function: one layer for the inputs, the activation, and the second layer for the outputs). Any number of layers can be added inbetween and are called hidden layers. There can be as many layers as we want, although the deeper the network, the more sophisticated it will be, meaning it will take longer to train, and the increased number of parameters can absorb a lot more peculiarities from each training image potentially over-fitting the data and not performing as well in validation, testing and production. We&#39;ll touch more on this further on in the post. . What we&#39;re going to be doing now, is effectively implement a very simple version of PyTorch&#39;s nn.Sequential. In nn.Sequential, we can specify $n$ layers and what activation function to use. The activations of any prior layer correspond to the inputs of the next layer. The last layer will always output the activations to be run through the loss function, exactly like in our LinearClassifier. . # Example 1 simple_net = nn.Sequential( nn.Linear(28*28,30), # an input layer of 28*28 with 30 activations nn.ReLU(), # the ReLU activation nn.Linear(30,10) # another linear layer with 30 inputs and 10 activations ) # Example 2 deeper_net = nn.Sequential( nn.Linear(28*28,100),# an input layer of 28*28 with 100 activations nn.ReLU(), # the ReLU activation nn.Linear(100,30), # a hidden layer of 100 inputs and 30 activations nn.ReLU(), # the ReLU activation nn.Linear(30,10) # another linear layer with 30 inputs and 10 activations ) . So how can we replicate something similar to the above? The good news is that we don&#39;t need to change a whole lot from our original LinearClassifier, except to introduce the concept of layers to our Classifier, as well as add the ReLU function. Other parts of the code (SGD_Optimizer and LinearModel) can be re-used as is! . Layers . Taking inspiration from PyTorch&#39;s nn.Sequential mentioned above, we&#39;ll accept layers in a similar fashion, except we&#39;ll ReLU any input and hidden layers (everything but the last layer) automatically. That means we&#39;ll need to implement our DeepClassifier so as to be able to setup our model in the following way: . # Example 1 (note: we still need to implement DeepClassifier) simple_nn = DeepClassifier( LinearModel(28*28, 20), # We&#39;ll ReLU automatically after Layer 1. LinearModel(20, 10) # LinearModel is the same one we used previously ) # Example 2 deeper_nn = DeepClassifier( LinearModel(28*28, 40), # first layer, ReLU&#39;d automatically LinearModel(40, 20), # hidden layer, also ReLU&#39;d automatically LinearModel(20, 10) # output layer ) . We&#39;ll begin by changing the name of our class from LinearClassifier to DeepClassifier. Finally! We&#39;ll also add a self.layers property in the __init__ method that stores the incoming layers provided as a list of arguments when we setup the model. I&#39;d also like to start logging/saving training loss to compare against our validation accuracy stored in self.accuracy_scores (remember it saves results from the _validate_epoch function that runs our weights against the validation data after each epoch). To do this we&#39;ll also add a self.training_loss list to save to. . def __init__(self, *layers): self.accuracy_scores = [] self.training_losses = [] self.layers = layers # add a layers property . Fit . We&#39;ll then change the terminology of our classifier so that instead of train we use fit. This is more consistent with other Deep Learning Libraries. In our fit method, we&#39;ll calculate how many layers self.layers has, and save the last layer&#39;s index (self.last_layer_index) so that we don&#39;t ReLU that layer later on. We&#39;ll also setup an optimizer for each layer that we can call .step from, using our previously defined SGD_Optimizer class. We&#39;ll also store the losses of each batch in an epoch to average out and compare against our validation accuracy. . def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;) self.valid_dl = kwargs.get(&#39;valid_dl&#39;) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] self.epoch_losses = [] # we&#39;ll average this after every epoch self.last_layer_index = len(self.layers)-1 # save last layer&#39;s index. for layer in self.layers: # for each layer, create an Optimizer we can .step() later on self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) # our usual training loop for i in range(self.epochs): for xb, yb in self.train_dl: # notice we use _forward now preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) . Forward &amp; Backward . We&#39;ll also introduce another structural change in the form of how our model operates. Rather than call _calc_grad, we&#39;ll instead group all of our linear calculations and activations in a _forward function, and all of our gradient calculations and optimizations in a _backward function. This makes more sense logically as we first calculate, then backtrack to get the gradients and update our parameters accordingly. . The _forward step loops through each layer in self.layers and calls each LinearModel&#39;s model method (i.e. the famous linear equation xb@w + b). As long as the layer is not the last layer (if layer_idx != self.last_layer_index:), it will also rectify the outputs before passing them on to the next layer. . def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res . The ReLU function is identicaly to the one created above. Pretty simple right? . def _ReLU(self, x): return x.max(tensor(0.0)) . Our backward step _backward entails running our final outputs through the loss function and then calling .backward() to calculate the gradients of each tensor automatically. We&#39;ll also take note of the loss output so we can average each epoch&#39;s average loss in our _validate_epoch function. We&#39;ll then loop through all of our optimizers in self.optimizers and step() them so as to update the parameters according to our learning rate. . def _backward(self, preds, yb): loss = self._loss_function(preds, yb) # keep track of each batch&#39;s loss in a list self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step() . Our loss function also changes slightly, as we&#39;ll switch to using torch.log_softmax directly instead of torch.log followed by torch.softmax. This is because our previous implementation caused issues with gradients returning NaN across multiple layers. . def _loss_function(self, predictions, targets): # we use torch.log_softmax directly log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() . Finally, we can update our _validate_epoch function to condense the batch&#39;s losses into an average so we can compare each epoch&#39;s validation accuracy score vs the average training loss. . def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) # stack all the saved losses from each batch and take the average epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) # clear the array for the next epoch of batches self.epoch_losses = [] # append to the training_losses list self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) . Putting It All Together . And here is our DeepClassifier in all its glory! . class DeepClassifier: &quot;&quot;&quot; A multi-layer Neural Network using ReLU activations and SGD params: layers to use (LinearModels) methods: fit(train_dl, valid_dl, epochs, lr) &quot;&quot;&quot; def __init__(self, *layers): self.accuracy_scores = [] self.training_losses = [] self.layers = layers def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;) self.valid_dl = kwargs.get(&#39;valid_dl&#39;) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] self.epoch_losses = [] self.last_layer_index = len(self.layers)-1 for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) for i in range(self.epochs): for xb, yb in self.train_dl: preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) def predict(self, image_tensor): probabilities = self._forward(image_tensor).softmax(dim=1) _, prediction = probabilities.max(-1) return prediction, probabilities def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res def _backward(self, preds, yb): loss = self._loss_function(preds, yb) self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step() def _batch_accuracy(self, xb, yb): predictions = xb.softmax(dim=1) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) self.epoch_losses = [] self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _ReLU(self, x): return x.max(tensor(0.0)) def _print(self, *args): if self.verbose: print(*args) . . The full code is available in this gist: https://gist.github.com/muttoni/d5ce076fdc83b8f82b9971c5c8bf6b2d . &#160;Testing our Deep Classifier . Now that we&#39;ve implemented our DeepClassifier, let&#39;s test its performance. Before we do, it&#39;s important to discuss the topic of complexity (i.e. depth and breadth). The deeper (more layers) and broader (more activations) we make our network, the more parameters and complexity our model will be able to learn, with the risk of potentially over-fitting our training data. The number of layers and parameters in a network should always be proportional to the amount of data and its complexity. There is no rule to apply, you just need to experiment and see. There&#39;s plenty of discussions around this topic so feel free to browse the web for people&#39;s rules of thumbs. . It&#39;s important to remember, that even the simplest (2 layer) neural network is able to theoretically approximate any function, and for most simple tasks is more than enough! You can try as an exercise to play around with layers and activation sizes to see how training is affected. Remember the accuracy scores outputed from all of the models created until now are always against the validation data, meaning that you may see accuracy decrease over time (a clear sign that the model is overfitting on the training data). . Anyways, enough talk, let&#39;s see it in action! . train_dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=256, shuffle=True) my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 10) ) . my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.1, epochs=100 ) . Epoch #0 Loss: 2.9898 Accuracy: 0.5713 Epoch #1 Loss: 1.2028 Accuracy: 0.6553 Epoch #2 Loss: 1.0008 Accuracy: 0.6754 Epoch #3 Loss: 0.8964 Accuracy: 0.7375 Epoch #4 Loss: 0.8207 Accuracy: 0.7498 Epoch #5 Loss: 0.756 Accuracy: 0.7441 Epoch #6 Loss: 0.7119 Accuracy: 0.7968 Epoch #7 Loss: 0.6837 Accuracy: 0.7902 Epoch #8 Loss: 0.642 Accuracy: 0.8166 Epoch #9 Loss: 0.6159 Accuracy: 0.7876 Epoch #10 Loss: 0.588 Accuracy: 0.8122 Epoch #11 Loss: 0.5736 Accuracy: 0.8244 Epoch #12 Loss: 0.5501 Accuracy: 0.835 Epoch #13 Loss: 0.5312 Accuracy: 0.8494 Epoch #14 Loss: 0.5108 Accuracy: 0.847 Epoch #15 Loss: 0.5047 Accuracy: 0.8305 Epoch #16 Loss: 0.4969 Accuracy: 0.8367 Epoch #17 Loss: 0.4766 Accuracy: 0.8019 Epoch #18 Loss: 0.4663 Accuracy: 0.8555 Epoch #19 Loss: 0.4654 Accuracy: 0.8535 Epoch #20 Loss: 0.4491 Accuracy: 0.8722 Epoch #21 Loss: 0.4398 Accuracy: 0.8579 Epoch #22 Loss: 0.4373 Accuracy: 0.8768 Epoch #23 Loss: 0.4362 Accuracy: 0.8375 Epoch #24 Loss: 0.4235 Accuracy: 0.8765 Epoch #25 Loss: 0.4211 Accuracy: 0.8749 Epoch #26 Loss: 0.4167 Accuracy: 0.8379 Epoch #27 Loss: 0.4098 Accuracy: 0.8618 Epoch #28 Loss: 0.4002 Accuracy: 0.8837 Epoch #29 Loss: 0.3974 Accuracy: 0.8702 Epoch #30 Loss: 0.3911 Accuracy: 0.8827 Epoch #31 Loss: 0.3914 Accuracy: 0.8854 Epoch #32 Loss: 0.3893 Accuracy: 0.8752 Epoch #33 Loss: 0.3807 Accuracy: 0.8861 Epoch #34 Loss: 0.3773 Accuracy: 0.8909 Epoch #35 Loss: 0.3733 Accuracy: 0.8909 Epoch #36 Loss: 0.3733 Accuracy: 0.8849 Epoch #37 Loss: 0.3669 Accuracy: 0.8615 Epoch #38 Loss: 0.3641 Accuracy: 0.8708 Epoch #39 Loss: 0.3621 Accuracy: 0.8679 Epoch #40 Loss: 0.3572 Accuracy: 0.8912 Epoch #41 Loss: 0.3553 Accuracy: 0.8609 Epoch #42 Loss: 0.3526 Accuracy: 0.8836 Epoch #43 Loss: 0.3485 Accuracy: 0.8936 Epoch #44 Loss: 0.3473 Accuracy: 0.9019 Epoch #45 Loss: 0.3433 Accuracy: 0.8969 Epoch #46 Loss: 0.3404 Accuracy: 0.9049 Epoch #47 Loss: 0.3385 Accuracy: 0.8957 Epoch #48 Loss: 0.3361 Accuracy: 0.8951 Epoch #49 Loss: 0.331 Accuracy: 0.8816 Epoch #50 Loss: 0.3327 Accuracy: 0.8783 Epoch #51 Loss: 0.334 Accuracy: 0.8942 Epoch #52 Loss: 0.3297 Accuracy: 0.9052 Epoch #53 Loss: 0.33 Accuracy: 0.9071 Epoch #54 Loss: 0.3244 Accuracy: 0.905 Epoch #55 Loss: 0.3213 Accuracy: 0.9092 Epoch #56 Loss: 0.3224 Accuracy: 0.911 Epoch #57 Loss: 0.319 Accuracy: 0.9026 Epoch #58 Loss: 0.3161 Accuracy: 0.8915 Epoch #59 Loss: 0.316 Accuracy: 0.9078 Epoch #60 Loss: 0.3127 Accuracy: 0.9003 Epoch #61 Loss: 0.3118 Accuracy: 0.9074 Epoch #62 Loss: 0.3106 Accuracy: 0.8938 Epoch #63 Loss: 0.3125 Accuracy: 0.9112 Epoch #64 Loss: 0.3076 Accuracy: 0.9062 Epoch #65 Loss: 0.307 Accuracy: 0.9005 Epoch #66 Loss: 0.3056 Accuracy: 0.8538 Epoch #67 Loss: 0.3034 Accuracy: 0.9003 Epoch #68 Loss: 0.302 Accuracy: 0.8929 Epoch #69 Loss: 0.2992 Accuracy: 0.9079 Epoch #70 Loss: 0.2992 Accuracy: 0.9103 Epoch #71 Loss: 0.2953 Accuracy: 0.9105 Epoch #72 Loss: 0.2958 Accuracy: 0.914 Epoch #73 Loss: 0.2951 Accuracy: 0.9104 Epoch #74 Loss: 0.2921 Accuracy: 0.9086 Epoch #75 Loss: 0.2933 Accuracy: 0.8952 Epoch #76 Loss: 0.2889 Accuracy: 0.9094 Epoch #77 Loss: 0.2878 Accuracy: 0.8972 Epoch #78 Loss: 0.2882 Accuracy: 0.908 Epoch #79 Loss: 0.2867 Accuracy: 0.9147 Epoch #80 Loss: 0.2867 Accuracy: 0.9047 Epoch #81 Loss: 0.2856 Accuracy: 0.9044 Epoch #82 Loss: 0.2822 Accuracy: 0.9166 Epoch #83 Loss: 0.2823 Accuracy: 0.9145 Epoch #84 Loss: 0.2803 Accuracy: 0.9143 Epoch #85 Loss: 0.2796 Accuracy: 0.9025 Epoch #86 Loss: 0.2805 Accuracy: 0.9007 Epoch #87 Loss: 0.2797 Accuracy: 0.9041 Epoch #88 Loss: 0.2775 Accuracy: 0.9074 Epoch #89 Loss: 0.2768 Accuracy: 0.9158 Epoch #90 Loss: 0.2758 Accuracy: 0.9051 Epoch #91 Loss: 0.2744 Accuracy: 0.9112 Epoch #92 Loss: 0.2721 Accuracy: 0.9094 Epoch #93 Loss: 0.2719 Accuracy: 0.8801 Epoch #94 Loss: 0.2721 Accuracy: 0.9114 Epoch #95 Loss: 0.2695 Accuracy: 0.9176 Epoch #96 Loss: 0.2674 Accuracy: 0.9097 Epoch #97 Loss: 0.2689 Accuracy: 0.9131 Epoch #98 Loss: 0.2671 Accuracy: 0.9178 Epoch #99 Loss: 0.2668 Accuracy: 0.9119 . . 91%! (toggle the output to see the various training losses and accuracy scores over each epoch). Let&#39;s also test our beloved sample &#39;5&#39; prediction. . val_5 = (tensor(Image.open(testing[&#39;5&#39;][0])).float()/255).view(-1, 28*28) a = my_nn.predict(val_5) a . (tensor([5]), tensor([[8.7256e-06, 6.7960e-08, 2.6373e-04, 1.8036e-02, 1.8940e-16, 9.4832e-01, 2.3715e-09, 6.1468e-06, 3.3328e-02, 4.1378e-05]], grad_fn=&lt;SoftmaxBackward&gt;)) . It gives us a confidence of 95% that the digit is 5. Nice! Now let&#39;s discuss our results in more detail. . Analyzing Results . Deep Neural Networks need to train longer as there are more parameters to tune. In our case, our 2-layer network after 100 epochs reached 91-92% accuracy with a learning rate of 0.1. Aside from the depth (layers) and breadth (parameters) of the model, learning rate also affects the number of epochs it takes as it &quot;weighs down&quot; the updating &quot;power&quot; of the optimizer so as to not overshoot the minimum loss. . By looking at the results above, we see how the training loss is still has room for improvement (remember, we&#39;re trying to get that as close to 0 as possible). When training a model it&#39;s important to consider over-fitting vs model accuracy. If your validation accuracy and training loss are both moving in the direction you want, it means you still have room to train! It doesn&#39;t make sense to continue training if instead you see the validation accuracy degrading consistently as that would be a clear sign of over-fitting on the training data. . To visualize all this better, let&#39;s create a quick function to visualize how our training loss and validation accuracy evolve over time (epochs). . def plot_results(my_nn): import matplotlib.pyplot as plt x = range(len(my_nn.accuracy_scores)) y1 = my_nn.accuracy_scores y2 = my_nn.training_losses fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(x, y1, &#39;g-&#39;) ax2.plot(x, y2, &#39;b-&#39;) ax1.set_xlabel(&#39;Epochs&#39;) ax1.set_ylabel(&#39;Validation Accuracy&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Training Loss&#39;, color=&#39;b&#39;) plt.title(f&quot;Results with lr={my_nn.lr}&quot;) plt.show() . . plot_results(my_nn) . The graph above shows how, over the course of 100 epochs, our training loss (in blue) and our validation accuracy (in green) have evolved. You&#39;ll see that our validation accuracy jumps around--that&#39;s because our model is only training on the training dataset, and our validation scores are measured on the validation dataset, so they don&#39;t get fed back into the model! This helps keep the model &quot;honest&quot;, and gives us an idea of our our model will perform on unseen data. As both our accuracy and loss are trending in the right direction, we could experiment extending the epoch count and tweaking the learning rate. . Learning Rates . Below are the same graphs with different learning rates, showing how they affect the model&#39;s performance over 100 epochs. As you can see, the higher the learning rate, the faster the accuracy improves, but it causes more jitter as the optimizing steps can overshoot across the loss function. The trick here is to pick one proportional to the amount of epochs you want to train (lower for more epochs, smaller for less epochs). Modern libraries (eg fast.ai) have convenient learning rate finders to take the guess work out. . Note that if you fit your model, and then try to fit it again, it will start from the previous weights, so you should probably pick a smaller learning rate the second time around as the &quot;big&quot; movements have already been done. . . Notice how the lower the learning rate, the smoother the training, but slower the ramp up and accuracy. . Epochs . Now let&#39;s experiment by using a somewhat large 0.5 learning rate and training for 2.5x more epochs (250 total). . my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 10) ) my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.5, epochs=250 ) . Epoch #0 Loss: 1.7751 Accuracy: 0.6389 Epoch #1 Loss: 0.9891 Accuracy: 0.6981 Epoch #2 Loss: 0.7729 Accuracy: 0.7354 Epoch #3 Loss: 0.6788 Accuracy: 0.7412 Epoch #4 Loss: 0.613 Accuracy: 0.8305 Epoch #5 Loss: 0.5669 Accuracy: 0.7853 Epoch #6 Loss: 0.5323 Accuracy: 0.8262 Epoch #7 Loss: 0.4896 Accuracy: 0.8599 Epoch #8 Loss: 0.4714 Accuracy: 0.8681 Epoch #9 Loss: 0.4435 Accuracy: 0.873 Epoch #10 Loss: 0.4322 Accuracy: 0.8764 Epoch #11 Loss: 0.4081 Accuracy: 0.8563 Epoch #12 Loss: 0.3952 Accuracy: 0.8856 Epoch #13 Loss: 0.3825 Accuracy: 0.8837 Epoch #14 Loss: 0.3687 Accuracy: 0.8929 Epoch #15 Loss: 0.3569 Accuracy: 0.8975 Epoch #16 Loss: 0.344 Accuracy: 0.8987 Epoch #17 Loss: 0.3389 Accuracy: 0.9014 Epoch #18 Loss: 0.3296 Accuracy: 0.9057 Epoch #19 Loss: 0.3199 Accuracy: 0.9066 Epoch #20 Loss: 0.3178 Accuracy: 0.9041 Epoch #21 Loss: 0.3072 Accuracy: 0.8971 Epoch #22 Loss: 0.302 Accuracy: 0.899 Epoch #23 Loss: 0.3025 Accuracy: 0.9099 Epoch #24 Loss: 0.2941 Accuracy: 0.9134 Epoch #25 Loss: 0.2913 Accuracy: 0.9169 Epoch #26 Loss: 0.2833 Accuracy: 0.8979 Epoch #27 Loss: 0.2781 Accuracy: 0.9188 Epoch #28 Loss: 0.2759 Accuracy: 0.9042 Epoch #29 Loss: 0.2744 Accuracy: 0.9164 Epoch #30 Loss: 0.2717 Accuracy: 0.9194 Epoch #31 Loss: 0.2653 Accuracy: 0.9067 Epoch #32 Loss: 0.2633 Accuracy: 0.9209 Epoch #33 Loss: 0.2592 Accuracy: 0.9164 Epoch #34 Loss: 0.255 Accuracy: 0.9213 Epoch #35 Loss: 0.255 Accuracy: 0.8712 Epoch #36 Loss: 0.2509 Accuracy: 0.9276 Epoch #37 Loss: 0.2467 Accuracy: 0.9143 Epoch #38 Loss: 0.2446 Accuracy: 0.9174 Epoch #39 Loss: 0.2424 Accuracy: 0.9229 Epoch #40 Loss: 0.2427 Accuracy: 0.91 Epoch #41 Loss: 0.2387 Accuracy: 0.9168 Epoch #42 Loss: 0.2385 Accuracy: 0.9262 Epoch #43 Loss: 0.233 Accuracy: 0.911 Epoch #44 Loss: 0.2315 Accuracy: 0.9234 Epoch #45 Loss: 0.2321 Accuracy: 0.9221 Epoch #46 Loss: 0.228 Accuracy: 0.9074 Epoch #47 Loss: 0.2272 Accuracy: 0.9271 Epoch #48 Loss: 0.2257 Accuracy: 0.9249 Epoch #49 Loss: 0.2232 Accuracy: 0.9246 Epoch #50 Loss: 0.2239 Accuracy: 0.9265 Epoch #51 Loss: 0.2205 Accuracy: 0.9291 Epoch #52 Loss: 0.2181 Accuracy: 0.914 Epoch #53 Loss: 0.2172 Accuracy: 0.9203 Epoch #54 Loss: 0.2151 Accuracy: 0.9182 Epoch #55 Loss: 0.2154 Accuracy: 0.9282 Epoch #56 Loss: 0.215 Accuracy: 0.9323 Epoch #57 Loss: 0.2115 Accuracy: 0.9291 Epoch #58 Loss: 0.2086 Accuracy: 0.9319 Epoch #59 Loss: 0.2092 Accuracy: 0.9187 Epoch #60 Loss: 0.2074 Accuracy: 0.9312 Epoch #61 Loss: 0.2067 Accuracy: 0.926 Epoch #62 Loss: 0.2053 Accuracy: 0.936 Epoch #63 Loss: 0.2039 Accuracy: 0.9301 Epoch #64 Loss: 0.2027 Accuracy: 0.9297 Epoch #65 Loss: 0.2007 Accuracy: 0.934 Epoch #66 Loss: 0.1998 Accuracy: 0.9223 Epoch #67 Loss: 0.2011 Accuracy: 0.9325 Epoch #68 Loss: 0.1985 Accuracy: 0.8839 Epoch #69 Loss: 0.199 Accuracy: 0.918 Epoch #70 Loss: 0.1978 Accuracy: 0.9266 Epoch #71 Loss: 0.1965 Accuracy: 0.9358 Epoch #72 Loss: 0.1954 Accuracy: 0.9352 Epoch #73 Loss: 0.1931 Accuracy: 0.9331 Epoch #74 Loss: 0.1925 Accuracy: 0.9264 Epoch #75 Loss: 0.1929 Accuracy: 0.9303 Epoch #76 Loss: 0.1907 Accuracy: 0.9367 Epoch #77 Loss: 0.1906 Accuracy: 0.9142 Epoch #78 Loss: 0.1906 Accuracy: 0.9322 Epoch #79 Loss: 0.1887 Accuracy: 0.9319 Epoch #80 Loss: 0.187 Accuracy: 0.9371 Epoch #81 Loss: 0.1875 Accuracy: 0.9312 Epoch #82 Loss: 0.1848 Accuracy: 0.9226 Epoch #83 Loss: 0.1835 Accuracy: 0.9337 Epoch #84 Loss: 0.1837 Accuracy: 0.9375 Epoch #85 Loss: 0.1836 Accuracy: 0.9378 Epoch #86 Loss: 0.1829 Accuracy: 0.9172 Epoch #87 Loss: 0.1828 Accuracy: 0.935 Epoch #88 Loss: 0.1811 Accuracy: 0.93 Epoch #89 Loss: 0.1806 Accuracy: 0.9255 Epoch #90 Loss: 0.1786 Accuracy: 0.9409 Epoch #91 Loss: 0.1798 Accuracy: 0.9369 Epoch #92 Loss: 0.1777 Accuracy: 0.9365 Epoch #93 Loss: 0.1774 Accuracy: 0.9386 Epoch #94 Loss: 0.176 Accuracy: 0.9317 Epoch #95 Loss: 0.1775 Accuracy: 0.939 Epoch #96 Loss: 0.1745 Accuracy: 0.9374 Epoch #97 Loss: 0.1752 Accuracy: 0.9396 Epoch #98 Loss: 0.1733 Accuracy: 0.9371 Epoch #99 Loss: 0.1734 Accuracy: 0.9242 Epoch #100 Loss: 0.1725 Accuracy: 0.9359 Epoch #101 Loss: 0.174 Accuracy: 0.9354 Epoch #102 Loss: 0.1716 Accuracy: 0.8946 Epoch #103 Loss: 0.1701 Accuracy: 0.9367 Epoch #104 Loss: 0.1712 Accuracy: 0.9365 Epoch #105 Loss: 0.1681 Accuracy: 0.9356 Epoch #106 Loss: 0.1673 Accuracy: 0.9351 Epoch #107 Loss: 0.1679 Accuracy: 0.9362 Epoch #108 Loss: 0.1677 Accuracy: 0.9446 Epoch #109 Loss: 0.1669 Accuracy: 0.9212 Epoch #110 Loss: 0.166 Accuracy: 0.9394 Epoch #111 Loss: 0.1648 Accuracy: 0.9427 Epoch #112 Loss: 0.1647 Accuracy: 0.9346 Epoch #113 Loss: 0.1641 Accuracy: 0.938 Epoch #114 Loss: 0.1635 Accuracy: 0.9395 Epoch #115 Loss: 0.1631 Accuracy: 0.9348 Epoch #116 Loss: 0.1634 Accuracy: 0.94 Epoch #117 Loss: 0.1619 Accuracy: 0.9359 Epoch #118 Loss: 0.1617 Accuracy: 0.9387 Epoch #119 Loss: 0.1615 Accuracy: 0.941 Epoch #120 Loss: 0.1605 Accuracy: 0.9393 Epoch #121 Loss: 0.1594 Accuracy: 0.9404 Epoch #122 Loss: 0.1589 Accuracy: 0.9408 Epoch #123 Loss: 0.1586 Accuracy: 0.9412 Epoch #124 Loss: 0.1577 Accuracy: 0.9394 Epoch #125 Loss: 0.1577 Accuracy: 0.937 Epoch #126 Loss: 0.1582 Accuracy: 0.9396 Epoch #127 Loss: 0.1568 Accuracy: 0.9353 Epoch #128 Loss: 0.1571 Accuracy: 0.9432 Epoch #129 Loss: 0.1561 Accuracy: 0.9309 Epoch #130 Loss: 0.1553 Accuracy: 0.9401 Epoch #131 Loss: 0.1557 Accuracy: 0.9359 Epoch #132 Loss: 0.1532 Accuracy: 0.9406 Epoch #133 Loss: 0.1532 Accuracy: 0.937 Epoch #134 Loss: 0.1541 Accuracy: 0.9371 Epoch #135 Loss: 0.1526 Accuracy: 0.9378 Epoch #136 Loss: 0.1518 Accuracy: 0.9429 Epoch #137 Loss: 0.152 Accuracy: 0.9403 Epoch #138 Loss: 0.1517 Accuracy: 0.9401 Epoch #139 Loss: 0.1507 Accuracy: 0.9383 Epoch #140 Loss: 0.1511 Accuracy: 0.9437 Epoch #141 Loss: 0.1498 Accuracy: 0.9434 Epoch #142 Loss: 0.1498 Accuracy: 0.9444 Epoch #143 Loss: 0.1502 Accuracy: 0.9408 Epoch #144 Loss: 0.149 Accuracy: 0.9457 Epoch #145 Loss: 0.1475 Accuracy: 0.9382 Epoch #146 Loss: 0.1483 Accuracy: 0.9436 Epoch #147 Loss: 0.1476 Accuracy: 0.9399 Epoch #148 Loss: 0.1469 Accuracy: 0.9373 Epoch #149 Loss: 0.1466 Accuracy: 0.9434 Epoch #150 Loss: 0.1458 Accuracy: 0.9418 Epoch #151 Loss: 0.1468 Accuracy: 0.941 Epoch #152 Loss: 0.1457 Accuracy: 0.9421 Epoch #153 Loss: 0.1459 Accuracy: 0.9434 Epoch #154 Loss: 0.1457 Accuracy: 0.943 Epoch #155 Loss: 0.1445 Accuracy: 0.944 Epoch #156 Loss: 0.1438 Accuracy: 0.9339 Epoch #157 Loss: 0.1442 Accuracy: 0.9331 Epoch #158 Loss: 0.1432 Accuracy: 0.9449 Epoch #159 Loss: 0.1437 Accuracy: 0.9409 Epoch #160 Loss: 0.1429 Accuracy: 0.9454 Epoch #161 Loss: 0.1426 Accuracy: 0.9386 Epoch #162 Loss: 0.1423 Accuracy: 0.9441 Epoch #163 Loss: 0.1415 Accuracy: 0.9434 Epoch #164 Loss: 0.1409 Accuracy: 0.9437 Epoch #165 Loss: 0.1414 Accuracy: 0.9368 Epoch #166 Loss: 0.1413 Accuracy: 0.9432 Epoch #167 Loss: 0.1405 Accuracy: 0.9458 Epoch #168 Loss: 0.14 Accuracy: 0.9451 Epoch #169 Loss: 0.1401 Accuracy: 0.9401 Epoch #170 Loss: 0.1391 Accuracy: 0.946 Epoch #171 Loss: 0.1391 Accuracy: 0.9427 Epoch #172 Loss: 0.1385 Accuracy: 0.9467 Epoch #173 Loss: 0.1383 Accuracy: 0.944 Epoch #174 Loss: 0.138 Accuracy: 0.9447 Epoch #175 Loss: 0.1378 Accuracy: 0.944 Epoch #176 Loss: 0.1367 Accuracy: 0.9349 Epoch #177 Loss: 0.1369 Accuracy: 0.943 Epoch #178 Loss: 0.1375 Accuracy: 0.9405 Epoch #179 Loss: 0.1365 Accuracy: 0.9472 Epoch #180 Loss: 0.1364 Accuracy: 0.9111 Epoch #181 Loss: 0.1372 Accuracy: 0.9347 Epoch #182 Loss: 0.1349 Accuracy: 0.9419 Epoch #183 Loss: 0.1344 Accuracy: 0.947 Epoch #184 Loss: 0.1343 Accuracy: 0.9422 Epoch #185 Loss: 0.1337 Accuracy: 0.9431 Epoch #186 Loss: 0.1338 Accuracy: 0.9449 Epoch #187 Loss: 0.1329 Accuracy: 0.9446 Epoch #188 Loss: 0.1337 Accuracy: 0.9471 Epoch #189 Loss: 0.1334 Accuracy: 0.9419 Epoch #190 Loss: 0.1332 Accuracy: 0.945 Epoch #191 Loss: 0.1331 Accuracy: 0.9424 Epoch #192 Loss: 0.1324 Accuracy: 0.9388 Epoch #193 Loss: 0.1319 Accuracy: 0.9441 Epoch #194 Loss: 0.1322 Accuracy: 0.9436 Epoch #195 Loss: 0.1315 Accuracy: 0.9414 Epoch #196 Loss: 0.1313 Accuracy: 0.9418 Epoch #197 Loss: 0.1309 Accuracy: 0.9415 Epoch #198 Loss: 0.1308 Accuracy: 0.9414 Epoch #199 Loss: 0.1304 Accuracy: 0.9364 Epoch #200 Loss: 0.1299 Accuracy: 0.9451 Epoch #201 Loss: 0.1282 Accuracy: 0.9452 Epoch #202 Loss: 0.1289 Accuracy: 0.9432 Epoch #203 Loss: 0.1285 Accuracy: 0.946 Epoch #204 Loss: 0.1285 Accuracy: 0.9405 Epoch #205 Loss: 0.1284 Accuracy: 0.9372 Epoch #206 Loss: 0.1291 Accuracy: 0.9208 Epoch #207 Loss: 0.1286 Accuracy: 0.9455 Epoch #208 Loss: 0.1275 Accuracy: 0.946 Epoch #209 Loss: 0.1274 Accuracy: 0.9418 Epoch #210 Loss: 0.1262 Accuracy: 0.9455 Epoch #211 Loss: 0.1276 Accuracy: 0.9294 Epoch #212 Loss: 0.1265 Accuracy: 0.9461 Epoch #213 Loss: 0.1262 Accuracy: 0.9446 Epoch #214 Loss: 0.1262 Accuracy: 0.9404 Epoch #215 Loss: 0.1264 Accuracy: 0.9455 Epoch #216 Loss: 0.1255 Accuracy: 0.94 Epoch #217 Loss: 0.1254 Accuracy: 0.9456 Epoch #218 Loss: 0.1251 Accuracy: 0.9431 Epoch #219 Loss: 0.125 Accuracy: 0.9403 Epoch #220 Loss: 0.1251 Accuracy: 0.9384 Epoch #221 Loss: 0.1238 Accuracy: 0.9441 Epoch #222 Loss: 0.1246 Accuracy: 0.9445 Epoch #223 Loss: 0.1244 Accuracy: 0.9426 Epoch #224 Loss: 0.1243 Accuracy: 0.9451 Epoch #225 Loss: 0.1228 Accuracy: 0.9445 Epoch #226 Loss: 0.1238 Accuracy: 0.9471 Epoch #227 Loss: 0.1229 Accuracy: 0.9419 Epoch #228 Loss: 0.1226 Accuracy: 0.9423 Epoch #229 Loss: 0.1217 Accuracy: 0.9407 Epoch #230 Loss: 0.1216 Accuracy: 0.9418 Epoch #231 Loss: 0.1222 Accuracy: 0.9415 Epoch #232 Loss: 0.1211 Accuracy: 0.942 Epoch #233 Loss: 0.1221 Accuracy: 0.937 Epoch #234 Loss: 0.1217 Accuracy: 0.9425 Epoch #235 Loss: 0.1208 Accuracy: 0.9453 Epoch #236 Loss: 0.1208 Accuracy: 0.9472 Epoch #237 Loss: 0.1204 Accuracy: 0.9354 Epoch #238 Loss: 0.1201 Accuracy: 0.9421 Epoch #239 Loss: 0.1191 Accuracy: 0.9463 Epoch #240 Loss: 0.1196 Accuracy: 0.9423 Epoch #241 Loss: 0.12 Accuracy: 0.9432 Epoch #242 Loss: 0.1188 Accuracy: 0.9446 Epoch #243 Loss: 0.1187 Accuracy: 0.9443 Epoch #244 Loss: 0.1181 Accuracy: 0.9442 Epoch #245 Loss: 0.1185 Accuracy: 0.9457 Epoch #246 Loss: 0.118 Accuracy: 0.9451 Epoch #247 Loss: 0.1175 Accuracy: 0.943 Epoch #248 Loss: 0.1181 Accuracy: 0.9462 Epoch #249 Loss: 0.117 Accuracy: 0.9393 . . plot_results(my_nn) . While overall we achieved better accuracy (around 94%), we can start to see that after epoch ~175 the accuracy flattens out without significant improvement. This is a sign that now are model is just overfitting or that our learning rate is too coarse to fine-tune the model above ~94% accuracy. We&#39;ll see this issue a lot clearer if we plot the results again starting from epochs #100 onwards. . import matplotlib.pyplot as plt x = range(len(my_nn.accuracy_scores)) y1 = my_nn.accuracy_scores y2 = my_nn.training_losses fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(x[100:], y1[100:], &#39;g-&#39;) ax2.plot(x[100:], y2[100:], &#39;b-&#39;) ax1.set_xlabel(&#39;Epochs&#39;) ax1.set_ylabel(&#39;Validation Accuracy&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Training Loss&#39;, color=&#39;b&#39;) plt.title(f&quot;Results with lr={my_nn.lr}, from epoch 100&quot;) plt.show() . . Lastly, let&#39;s also check what happens if we train with lower learning rates (0.005 and 0.01) and train for even longer (500 epochs). Given the large number of outputs I will omit the code snippets, but you get the hand of it by now! . . In the case of lr=0.005, after 500 epochs final accuracy was around 88%, and the loss around 0.39 due to the low steps. The trends of both lines suggest it could do with more training. . . For lr=0.01, we were able to achieve better accuracy (91%) but the loss function still hadn&#39;t completely plateaued (roughly around 0.30). Let&#39;s try again with a larger learning rate of 0.1 (10x bigger!) . . With lr=0.01 were able to reach 93% accuracy and get the loss down to 0.18! but as you can see, the larger the learning rate, the rougher our learning curve (in terms of accuracy). This is because our parameters jump around much more. Let&#39;s choose a learning curve inbetween 0.01 and 0.1 (i.e. 0.05) and train for 1000 epochs! . . With lr=0.05, after 1000 epochs we ended up with around 94.5% accuracy. However we already reached 94.5% accuracy back in epochs 700-800. When you train for an arbitrary number of epochs it could be that depending on your chosen configuration, your model reaches an optimal accuracy somewhere in the middle of the training. This is why many frameworks implement an option called early stopping rounds whereby you can instruct the model to stop training when accuracy hasn&#39;t improved after $n$ rounds. From the image above it seems like there isn&#39;t much more room to grow as the curves flatten out after epochs 750 or so. However, if we zoom in on our graph, while rickety, we can see there&#39;s still a clear trend on both metrics and could be worth fitting for a second cycle, perhaps with a lower learning rate. . . Hopefully all these experiments show that machine learning is a craft of delicately balancing different constraints and making trade-offs, with only loose guidelines to guide our decision making! It is definitely not an exact science with strict, clear cut rules. There are so many parameters to take into consideration: batch sizes, learning rates, epochs, dataset size, layer count, parameter count. And that&#39;s not including other important decisions such as what type of loss function to use (spoiler alert, cross-entropy loss is only one of many!), what activation function to use, etc. This is is why machine learning is a field where there&#39;s plenty of room for experimentation and is full of unexplored territory. . Conclusions . This marks the end of this series of building a Deep Neural Network from scratch. I realize these are two very long posts, but rather than split them up for the sake of splitting them up, I decided to condense everything into 2 parts so it&#39;s easier to keep track of links and follow along step by step. As linked above, the final DeepClassifier code is available here. Follow the steps in the series and experiment with all the possible parameter choices! . I really hope you enjoyed the series and learned something along the way. If you want to reach out, feel free to find my email in the &quot;About&quot; page or leave a comment below. With that, I&#39;ll close and wish you all the best in your machine learning journey! . Aknowledgements . This series could not have been possible without the incredible resource that the fast.ai course is! I highly recommend you check it out as you will learn most of what&#39;s covered here and more! Most of what we covered in this series is part of Lessons 4 and 5 of the 2020 course. A big thank you to Jeremy and the fast.ai team for creating such a wonderful and inspiring course. .",
            "url": "https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html",
            "relUrl": "/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Implementing a Deep Neural Network from Scratch - Part 1",
            "content": "Introduction . One of the best ways to truly understand an algorithm, an API, or really any practical concept, is to build it yourself. In this blog post, we will implement a Deep Neural Network Multi-Label Text Classifier from scratch using Python, debunking the impression that modern Machine Learning is an obscure black box full of mind-bendingly complicated math. . Our goal will be to create a Neural Network that can distinguish all 10 handwritten digits (using the industry standard MNIST dataset). This is also known as a Multi-Label Classifier (a single-label classifier only has two labels, such as cat or dog, hotdog or not hotdog, etc). I&#39;ll be basing this post on Lesson 4 of the excellent fast.ai tutorial on Deep Learning, where Jeremy Howard shows how to distinguish whether a digit is a 3 or a 7 (a single label classifier). We&#39;ll need to change some things to make it work for all ten digits (0-9). . To truly test your understanding, I encourage you to try this yourself with a different type of dataset. . Terminology . Most concepts that appear complicated are actually quite simple once you look at its building blocks. Complexity often arises from an aggregation of simplicity, and with Neural Networks this is exactly the case. The foundation of a Neural Network is collection of neurons. A neuron takes in a value, does a mathematical operation and outputs another value. That&#39;s it. The reason Neural Networks are powerful, is in the how: how the neurons work together, and how the output values in each neuron are calculated. . If you remember from high school math, one of the simplest equations is that of a linear function (i.e. the equation of a straight line): . $y = textbf{a}x + textbf{b}$ . $ textbf{a}$ and $ textbf{b}$ are constants (i.e. numbers like 2 or 3). For any value $x$ that we feed in, $ textbf{a}$ influences that slope (or gradient) of the line, $ textbf{b}$ determines the offset of the line along the $y$ axis. . . Here is an interactive example where you can see how $x$ is affected by changes in $ textbf{a}$ and $ textbf{b}$. . Guess what: each neuron in a Neural Network is a linear function almost identical to the above. The main differences lie in how we call each parameter and how the values of the function are affected once we start connecting neurons together. This is why we refer to the neurons as &quot;linear units&quot;. From here on out, we&#39;ll call them linear units or LU(s). . The function for a linear unit is equivalent to the one we saw earlier: . $y = wx + b$ . Almost identical right? The only difference is that $a$ turned into a $w$. This is because at the heart of a linear unit, the input is affected by a weight $w$ and a bias $b$. Here&#39;s another way to look at it, visually: . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G bias bias neuron aka L.U. neuron aka L.U. bias&#45;&gt;neuron aka L.U. output output neuron aka L.U.&#45;&gt;output input input weight weight input&#45;&gt;weight weight&#45;&gt;neuron aka L.U. Why so much fuss about Neural Networks? Well, the power of neural networks is that theoretically they are able to approximate any function, at any level of accuracy. This is called the Universal Approximation Theorem. If you&#39;re interested in diving deeper into how this works, I recommend reading this post that visually (and interactively) shows how Neural Networks are able to approximate any function. . We&#39;ve made some bold claims: something that can replicate any known function with any level of theoretical accuracy...and just thanks to a simple linear function? Well, yes! And no. As mentioned ealier, the real magic of a Neural Network lies not in how each neuron (linear unit) behaves independently, but how they behave as a whole. We&#39;ll see how this works as we build our Neural Network. . By the way, if you need a little more context on the topics covered above, I highly recommend the Kaggle course on the subject. . Getting Started . Now that we&#39;ve understood what lies at the foundation of a Neural Network, we can start implementing it. Let&#39;s begin by importing and taking a look at the data we will be using. . !pip install -Uq fastbook from fastbook import * # Download the MNIST dataset using fast.ai&#39;s convenient function path = untar_data(URLs.MNIST) . Our data is made up of two directories: training/ and testing/. In each folder, the PNGs of the digits (0 to 9) are grouped into folders themselves. The training dataset has a total of 60,000 reference images and the testing a total of 10,000 (so roughly a 15-16% validation split). . mnist_png/ (70,000 files) ├── training/ (60,000 files) │ ├── 0/ │ │ ├── 44897.png │ │ ├── 28230.png │ │ └── ... │ ├── 1/ │ │ ├── 44680.png │ │ ├── 37659.png │ │ └── ... │ ├── ... │ └── 9/ └── testing/ (10,000 files) └── ... (same structure as training/) . The reason we need the two folders is that we&#39;ll &quot;learn&quot; how to recognize the digits using the larger training set, and measure our performance accuracy against the testing dataset. If we were to test our performance on the dataset we learn from, we would always inevitably get 100% accuracy as the model learns to distinguish individual files. This sounds great, but the reality is it would perform terribly in a production environment when it encounters images of digits it has never seen before. . training = { f&#39;{num}&#39; : (path/f&#39;training/{num}&#39;).ls().sorted() for num in range(10) } testing = { f&#39;{num}&#39; : (path/f&#39;testing/{num}&#39;).ls().sorted() for num in range(10) } . zero_path = training[&#39;0&#39;][0] zero = Image.open(zero_path) zero . In order to work with our images, we need to convert this image into a numerical representation. Usually, a 2D image is represented by a matrix array (i.e. a Python list, a Javascript array, etc). There are a couple Python libraries that have extended the standard list object to be a bit more flexible (e.g. a numpy array). The object we&#39;ll be using is a PyTorch tensor. PyTorch is a popular Deep Learning Python Library. While tensor may sound like a fancy name, it&#39;s nothing more than an array that has a couple extra features. From here on out, when we refer to a tensor, we mean an array. Let&#39;s convert our zero to a tensor and visualize it numerically: . tensor(zero) . . You don&#39;t have to be Neo (from The Matrix) to make out the zero above made up of numbers. This is because each value of the array represents a pixel value, in the case of grey scale, from 0 to 255 (from pure black to pure white). . Given that this is a 2D matrix, we can access specific parts of the tensor using notation such as zero[5:10,5:10] which will output the rows from index 5 (included) to 10 (excluded) and the same for the columns. . z_t = tensor(zero) z_t[6:10,10:15] # This will output rows 7-10 (index 6-9) and columns 11-15 (index 10-14) . tensor([[ 0, 0, 0, 54, 227], [ 0, 10, 60, 224, 252], [ 0, 163, 252, 252, 252], [ 51, 238, 253, 253, 190]], dtype=torch.uint8) . Let&#39;s visualize the tensor a little better: . df = pd.DataFrame(z_t) df.style.set_properties(**{&#39;font-size&#39;:&#39;5pt&#39;}).background_gradient(&#39;Greys&#39;) . . This is the anatomy of our digits: 28 rows long by 28 columns wide, making a total of 784 pixels each. Each pixel is represented by a value from 0-255. . Before we start, let&#39;s first ponder on how we could, without knowing anything but the structure of the data, teach a computer how to differentiate between the digits. We&#39;ll do that in the next section, and use it as a baseline to measure our Neural Network against. . Establishing a Baseline . We&#39;ll begin our implementation by implementing something that isn&#39;t a neural network. It sounds counter-intuitive, but in order to get a good understanding of the performance of our model, we should always have a simple baseline with which to compare it against. Usually a good baseline is something simple, easy to create, and easy to understand. Seeing as how we&#39;re building everything from scratch, we may as well build our baseline from scratch as well. . In our case, there are several ways to approach this problem without knowing a single thing about machine learning. One approach could be just comparing the pixel similarity of the digits. This can be done by counting the average number of non-0 pixels for each image, or measuring the average pixel value of each image, or the average sum of pixel values, etc). Let&#39;s pick comparing average pixel values as the &quot;algorithm&quot; for our baseline algorithm. . Average Pixel Value . To calculate average pixel value, we will take the average value of each pixel coordinate across every respective digit&#39;s training image&#39;s tensor. For example, we take all the &#39;5&#39; image tensors, we look at the first coordinate (1,1) in each tensor and we take the average value. We repeat the process for each coordinate until we create a map of average values across the 28x28 grid of pixels. An easy way to do this is to create a tensor that &quot;houses&quot; all of the individual image tensors. This will take our tensor from 2 dimensions, to 3 (also called a rank-3 tensor). This allows us to work with all of the tensors as a single stack, and conduct mathematical operations such as tensor.mean() without looping. We&#39;ll see later why that&#39;s not just convenient, but very important for performance. . Going into the third dimenion sounds complicated, but all we&#39;re really doing is nesting each of the digit&#39;s various PNGs&#39; tensors into a larger tensor. We do this with PyTorch&#39;s stack method which does just that. While we&#39;re at it, we&#39;ll also normalize our pixel values to be from 0 to 1 by dividing by 255 after converting them to float. . # For each digit from 0 to 9, we create a stacked tensor # containing the tensors for each PNG image, after converting # each pixel value to a float and dividing it by 255. training_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in training[f&#39;{num}&#39;] ]) for num in range(10) ] # Preview the shape of the stacked tensors of the &quot;zero&quot; digit PNGs training_tensors[0].shape . torch.Size([5923, 28, 28]) . We now have a list of stacked tensors (i.e. a list of stacked tensors of each digit). Each stacked tensor has $n$ tensors for each PNG image (rows), and each row contains a 28x28 tensor representing that PNG&#39;s pixel values normalized from 0 to 1. . We can check this by inspecting the first item in our list of training_tensors: the stacked tensor of &quot;zero&quot; PNGs has shape torch.Size([5923, 28, 28]): because it has 5923 rows (equal to the number of training images), each containing a 28x28 tensor (equal to the pixels in each image). If you&#39;re having trouble picturing a 3D tensor in your head, think of it like an array of array of arrays (or a list of list of lists in Python). . Calculating Average Pixel values . The convenient part of having used tensors, is that now we can quickly calculate the mean across a desired dimension (in our case: flatten each stack into a single 28x28 tensor containing the average values of each pixel in the stack). . Tip: Think of it like printing out all the PNG images for a specific digit and stacking each page one on top of the other. That&#8217;s exactly what we did with out stacked tensor. And now, with special X-Ray vision, we are going to reduce the stack to a single page by taking the average value for each printed pixel across the stack of pages. . training_means = [t.mean(0) for t in training_tensors] # Let&#39;s display the &quot;mean&quot; tensor of the digits f, axarr = plt.subplots(2,5) digit = 0 for i in range(2): for j in range(5): axarr[i,j].imshow(training_means[digit], cmap=&quot;gray&quot;) digit += 1 . Pretty cool right? That is the &quot;average&quot; pixel value for each digit, representing in a way the ideal digit according to our baseline algorithm that will compare the validation digits against this mean. The value we will be measuring is called distance. Our validation digits will of course be very different from these reference means, but hopefully the pixel values will be nearest in distance to the mean pixel value of the correct digit! Let&#39;s see if that&#39;s the case... . Measuring Distance . In order to measure the distance between our reference means and the validation digits, we need to subtract them. However we can&#39;t simply subtract them, as that would create negative numbers and our pixel values go from 0 to 1. . To avoid dealing with negative numbers, there are two common measures of distance that get rid of negative numbers altogether: . mean absolute difference (aka L1 Norm) | root mean squared error (aka RMSE or L2 Norm). | . If these sound confusing you&#39;ll see how simple they are in just a second. . Mean Absolute Difference: $mean( abs( a - b ) )$. For each prediction, we subtract our prediction from the actual value, turn that difference into a positive number (i.e. remove the negative sign if there is one) and then average all the these differences together together. . Root Mean Squared Error / RMSE: $ sqrt[]{mean((a-b)^2)}$. For each prediction, we subtract our prediction from the actual value, square the result (so as to turn it into a positive number) and then take the square root, so as to &quot;cancel out&quot; the squaring. . In code, they are even easier: . def mean_absolute_difference(a, b): return (a - b).abs().mean() def rmse(a, b): return ((a - b)**2).mean().sqrt() . Now that we&#39;ve defined them, let&#39;s pick a random image from the validation set (e.g. the first image in the &#39;5&#39; in the testing list) and calculate its distance from the training set&#39;s reference mean. . val_5 = tensor(Image.open(testing[&#39;5&#39;][0])).float()/255 # val_5.shape =&gt; torch.Size([28, 28]) # Let&#39;s compare it to the reference mean of 5 mean_absolute_difference(training_means[5], val_5), rmse(training_means[5], val_5) . (tensor(0.1683), tensor(0.2988)) . mad_all = [mean_absolute_difference(training_means[num], val_5) for num in range(10)] rmse_all = [rmse(training_means[num], val_5) for num in range(10)] . Remember, we want the lowest distance (either measured via Mean Absolute Difference, or RMSE) to be the one between our PNG and the digit&#39;s &quot;reference mean&quot; (i.e. the mean we calculated representing the average of all the training digits of that type). . We can already see that for this particular 5 in our dataset, our distance measures are actually lower for other numbers (e.g. 3, 8). This isn&#39;t desired, but also isn&#39;t too surprising, as the handwritten digits are visually very similar. Let&#39;s see if this holds up on average for all the 5&#39;s, and whether our algorithm will perform better for other digits. . We&#39;ll continue by loading all of our validation images: . validation_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in testing[f&#39;{num}&#39;] ]) for num in range(10) ] # Preview the shape of the stacked validation tensors of the &quot;zero&quot; digit validation_tensors[0].shape . torch.Size([980, 28, 28]) . We need to write a function that takes each tensor stack in our validation_tensors list (one for each digit), and for each tensor in the stack measures the distance from the corresponding mean. . Thanks to PyTorch&#39;s efficient use of broadcasting, we can feed in a stack as $a$ and a single tensor as $b$ (or viceversa) and PyTorch will automatically stack the single tensor multiple times to match the length of the stack. This allows the computation to be done using low-level C on the GPU (i.e. in parallel) and is several orders of magnitude faster than a regular Python for loop. Thankfully, our distance function doesn&#39;t need to change a whole lot, the only change being to add along which axes to calculate the mean in our tensor stack (see commented code below). We&#39;ll need another couple of functions to help us deal with multiple digits and multiple tensor stacks and aggregate all the scores. . You can read the source code below, but here&#39;s a quick explanation: for each stack of tensors (e.g. tensors of all of the &#39;3&#39; images) in our validation set, we calculate the distance from each digit&#39;s reference mean and check that the lowest distance is that of the correct digit&#39;s reference mean. . def distance(a, b): # Let&#39;s go with mean absolute error for now return (a - b).abs().mean((-1,-2)) # the last two axes (e.g. 28x28) def is_correct(tensors, means, correct_index): # make a list of the wrong digits&#39; means indices wrong_means = [i for i in range(10)] wrong_means.pop(correct_index) # calculate the distance from the correct reference mean correct_distance = distance(tensors, means[correct_index]) # calculate and compare the distance to each wrong digit&#39;s reference mean # and then checking that the distance is the lowest across all digits. # each &#39;wdX&#39; contains a tensor of Booleans -- eg tensor([True, False, ... ]) wd1 = correct_distance &lt; distance(tensors, means[wrong_means[0]]) wd2 = correct_distance &lt; distance(tensors, means[wrong_means[1]]) wd3 = correct_distance &lt; distance(tensors, means[wrong_means[2]]) wd4 = correct_distance &lt; distance(tensors, means[wrong_means[3]]) wd5 = correct_distance &lt; distance(tensors, means[wrong_means[4]]) wd6 = correct_distance &lt; distance(tensors, means[wrong_means[5]]) wd7 = correct_distance &lt; distance(tensors, means[wrong_means[6]]) wd8 = correct_distance &lt; distance(tensors, means[wrong_means[7]]) wd9 = correct_distance &lt; distance(tensors, means[wrong_means[8]]) # now we &#39;and&#39; all the Boolean tensors together wdf = torch.bitwise_and(wd1, torch.bitwise_and(wd2, torch.bitwise_and(wd3, torch.bitwise_and(wd4, torch.bitwise_and(wd5, torch.bitwise_and(wd6, torch.bitwise_and(wd7, torch.bitwise_and(wd8, wd9)))))))) return wdf.float().mean() def accuracy(stacks, means): accuracy_map = [is_correct(stacks[i], means, i) for i in range(10)] accuracy_tot = (sum(accuracy_map) / len(accuracy_map)).item() return accuracy_tot, accuracy_map . tot, map = accuracy(validation_tensors, training_means) print(&quot;Tot. Accuracy: {:.1%}&quot;.format(tot)) print(&quot; n&quot;.join([&quot;Digit {} : {:.1%}&quot;.format(i, map[i]) for i in range(10)])) # Plot results for a visual inspection plt.title(&quot;Baseline Performance Across Digits&quot;) plt.axhline(tot, color=&#39;red&#39;, linewidth=2) plt.xticks(range(0, 10)) plt.xlabel(&#39;Digits&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.bar(range(len(map)), map, align=&#39;center&#39;) plt.plot(tot) plt.show() . Tot. Accuracy: 66.1% Digit 0 : 81.5% Digit 1 : 99.8% Digit 2 : 42.3% Digit 3 : 60.9% Digit 4 : 66.8% Digit 5 : 32.6% Digit 6 : 78.7% Digit 7 : 76.5% Digit 8 : 44.3% Digit 9 : 77.6% . It seems our simple mathematical comparison of average pixel values already achieves 66.1% overall accuracy across the validation set! This is impressive considering some digits have similar strokes (e.g. 3, 5 and 8). You can see from the bar chart above, some digits perform better than others, with the &#39;1&#39; digit recognition achieving near perfect accuracy (99.8%) in stark contrast to the &#39;5&#39; digit (32.6%). . So we&#39;ve covered terminology, we&#39;ve established a simple baseline based on pixel arithmetic. Let&#39;s start building this darn deep neural network! . Thinking Like a Neural Network . While &quot;Deep Learning&quot; sounds sophisticated, the underlying learning process under the hood doesn&#39;t sound quite as futuristic: &quot;try, rinse, repeat&quot;. However, the real magic lies in the details, more specifically in how it &quot;rinses&quot; each cycle. After every cycle, it will adjust its own parameters to be &quot;better&quot;. We&#39;ll see how it does this in just a second. Now let&#39;s reflect on our simple baseline method. . Our baseline approach uses a very deterministic, binary algorithm for deciding whether a digit is correct or not: it will compare pixel distance with each reference mean and pick the mean where the distance is lowest. Considering that the input pixels and reference pixels never change, for each image our model will either be correct 100% of the time, or it will fail 100% of the time. There is no feedback loop, no way for the model to adjust along the way and no &quot;randomness&quot;. . Remember that our Linear Units have parameters. More specifically, each Linear Unit has a weight and a bias: . $y = textbf{w}x + textbf{b}$ . The beauty of parameters is that they can be tweaked, and the output $y$ will be different, much like we saw in our linear equation at the beginning of the post. . With this simple equation as the engine for its neurons, a Neural Network is able to approximate anything. The approximation process is possible due to the iterative nature of trying a set of parameters, adjusting them and trying again. But how, you may be wondering, does a model choose the new set of parameters? Is it a random choice? Is it decided by a function? The answer is an inclusive &quot;yes&quot;: it starts off with random values and gradually adjusts them according to a function: the loss function. The loss function measures how accurate the prediction is. The optimizer function then updates the parameters so as to minimize the loss function. It does this by looking at the rate of change (the gradient, aka slope) of each parameter as it was fed into the loss function and updates them (also called &quot;to step&quot;) in the direction that will minimize the loss function. And this process has a fancy name, called Stochastic Gradient Descent. . Descent because we want to get to make our way to the lower end of the loss function, | Gradient because we use the gradient of each parameter to find in what direction to update them to make that descent, and | Stochastic because it involves a litte bit of randomness, meaning it&#39;s not deterministic. | . Let&#39;s summarize all these steps again: . We initialize the parameters (weights and biases for each input) to random values | Make a prediction using those parameters | Measure the accuracy of the prediction using a loss function | We calculate how to update our parameters so as to minimize the loss function (remember: lower is better) using the gradients of the parameters&#39; change as a result of the loss function. This is done via the optimizer function. | We update our parameters and predict again, repeating the cycle until we are satisfied or the model stops improving significantly. | &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G initialize initialize predict predict initialize&#45;&gt;predict measure loss measure loss predict&#45;&gt;measure loss step based on gradient step based on gradient measure loss&#45;&gt;step based on gradient step based on gradient&#45;&gt;predict repeat stop stop step based on gradient&#45;&gt;stop Let&#39;s begin by implementing something that follows these rules. The reason I call it something is that you&#39;ll see later why it&#39;s not exactly a Deep Neural Network. But it will help us learn along the way and take it step by step. . Primordial Neural Network . Our first Neural Network will start with a parameter for each pixel value of the input image ($28 times 28 = 784$), and will be fed directly into our loss function to measure our prediction accuracy. That means that our output needs to be a list of 10 probabilities--each probability corresponding to the likelihood that our reference image corresponds to a specific digit. These probabilities will need to sum to 1, as the classification is mutually exclusive (i.e. a &#39;3&#39; can only be a &#39;3&#39;). Other multi-label classification approaches are inclusive, meaning each input could have multiple labels (e.g. deciding if an image contains a person and/or a car and/or any other object). . How do we condense all of our parameters into a list of probabilities? Say hello to my little friend: Softmax. . The Softmax Function . The beautify of the Softmax function is that it normalizes a group of values from 0 to 1 in a way so that each value is interrelated with the others: together they sum to 1. This is perfect for generating a list of interrelated probabilities. If we were creating non-mutually exclusive classification, we would use Softmax&#39;s cousin Sigmoid, that normalizes each value from 0 to 1 independently. I&#39;ll leave diving deeper on the topic of Softmax vs Sigmoid as homework--you can start with this brilliant blog post. . Let&#39;s admire Softmax in all of its glory: . $ text{Softmax}(x_i) = frac{e^{x_i}}{ sum_j{e^{x_j}}}$ . Don&#39;t worry, it&#39;s a lot simpler than it looks, and is even simpler once we code it. Here&#39;s a quick explanation of how our Softmax works in three steps: . Given a list of values, we calculate the exponential for each value, that is: $e^{n}$. Where $n$ is our value and $e$ is Euler&#39;s number, a special number in Mathematics, and at the heart of the exponential function. | We sum the exponential values (the Sigma symbol $ sum$ means sum). | Take each value in Step 1. and divide it by the sum obtained in Step 2. | As Jeremy Howard says: a mathematical concept often sounds complicated until you code it. So let&#39;s code it. Below is my quick softmax implementation: . def softmax(v): &quot;&quot;&quot; Given an input vector v, returns an output vector containing the softmax values for each element in the vector &quot;&quot;&quot; exp_v = torch.exp(v.float()) exp_sum = exp_v.sum() return exp_v / exp_sum . softmax(tensor([1,2,3])) . tensor([0.0900, 0.2447, 0.6652]) . Datasets . Now that we have Softmax in our toolbelt, we can move on with the implementation. Let&#39;s start by preparing our data. We&#39;ll take our tensor stacks. Let&#39;s reduce the dimensions of our data by transforming each image tensor from a matrix of 28x28 to a vector (a list) of size 28*28 (meaning each pixel is in a series, not in a 2D coordinate space) . train_x = torch.cat(training_tensors).view(-1, 28*28) valid_x = torch.cat(validation_tensors).view(-1, 28*28) . train_x.shape, valid_x.shape . (torch.Size([60000, 784]), torch.Size([10000, 784])) . As you can see from the output of train_x.shape, we went from a rank-3 tensor (60000, 28, 28) to a rank-2 tensor (60000, 784). We now want as many outputs from our model as number of labels (in our case digits). So we need to flag for each training image which is the correct digit. To do this we&#39;ll populate a tensor of length 10 as our corresponding target labels (also referred to as $y$ variables. In that case our inputs are usually referred to as $X$, representing our features) and add a &quot;1&quot; to flag the corresponding index in the tensor corresponding to our digit. You can see what it looks like for each digit below. . Each row represents the correct label representation for each digit. For example, the first row is the output labels valid for all the &#39;zero&#39; tensors (index 0 is set to 1, and the rest are 0), the second being the output labels for all the &#39;one&#39; tensors (index 1 is set to 1 and the rest are 0), etc. In a way, the 1&#39;s location inside the indicates 100% probability that that index represents the correct digit. Ideally, we want our model to output the exact same output. . # Every digit&#39;s tensor stack will have a label # with the corresponding index flagged as &#39;1&#39;. [[0]*i + [1] + [0]*(9-i) for i in range(10)] . [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]] . Let&#39;s associate a target label for each tensor in our stack, with the &#39;1&#39; corresponding to the digit it represents. . train_y = torch.from_numpy(np.concatenate([[[0]*i + [1] + [0]*(9-i)]*len(training[f&#39;{i}&#39;]) for i in range(10)])) valid_y = torch.from_numpy(np.concatenate([[[0]*i + [1] + [0]*(9-i)]*len(testing[f&#39;{i}&#39;]) for i in range(10)])) . train_y.shape, valid_y.shape, train_y[0] . (torch.Size([60000, 10]), torch.Size([10000, 10]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])) . Let&#39;s now merge them into a dataset. A dataset contains both $X$ and $y$ variables so that the model knows what $y$ to compare the prediction of $X$ against. We&#39;ll populate a training dataset with which to train the parameters, and a validation dataset that the parameters will never &quot;learn&quot; from, with which to benchmark our training against. A model may be very good at learning the peculiarities of its training data but may end up performing very poorly on its validation data as the features learned are not generalizable. This phenomenon is referred to as over-fitting. . # into datasets by zipping them so as to create a list of tuples (image tensor vector, label ) dset = list(zip(train_x, train_y)) dset_valid = list(zip(valid_x, valid_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])) . Parameters . What we&#39;ll do now is start defining functions that we&#39;ll end up merging into a single model class. The first function we&#39;ll need is a function that simply initiates random numbers, and these random numbers will be our weights and biases for each pixel. . We&#39;ll use the .requires_grad_() in-place operation to ask PyTorch to track the gradients, as we&#39;ll need them later to backtrack and update our parameters according to the loss function. . def init_params(sizeD1, sizeD2=0): if sizeD2 == 0: return (torch.randn(sizeD1)).requires_grad_() else: return (torch.randn(sizeD1, sizeD2)).requires_grad_() . weights = init_params(28*28, 10) # the &#39;w&#39; of the linear equation bias = init_params(10) # the &#39;b&#39; of the linear equation . weights.shape, bias.shape . (torch.Size([784, 10]), torch.Size([10])) . Linear Equation . Next we need a function that performs the famous Linear Unit (neuron) operation that we&#39;ve been talking about since the beginning of the post. It&#39;s as simple as: . def linear_eq(x): return x@weights + bias . Next we&#39;ll feed our training data train_x into our linear equation, which will associate the parameters we created earlier via matrix multiplication. That&#39;s what the @ stands for in our function above. This gives us a first set of predictions (preds) that should perform quite poorly, more or less as well as picking the digits at random. . preds = linear_eq(train_x) preds . tensor([[-11.9867, -4.2706, 4.6806, ..., -1.3517, -0.0395, 13.5840], [ -6.6797, -4.4483, 1.4827, ..., -8.9409, 2.2570, 10.0881], [-22.7994, 2.2295, 11.1160, ..., -1.8488, 25.5523, 22.3240], ..., [-16.7386, 4.0141, 3.9273, ..., 1.5960, 9.7961, 12.3801], [-14.1562, -3.5679, 1.7541, ..., 1.6506, 16.5308, 10.3213], [-11.3688, 4.5965, 5.7170, ..., 10.0353, 4.4967, 9.5438]], grad_fn=&lt;AddBackward0&gt;) . _, max_indices = preds.max(-1) # get the index of max value along 2nd dimension _, tag_indices = train_y.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match corrects.float().mean() # calculate mean . tensor(0.1204) . We see here that randomly initialized parameters will perform more or less like...well...a random choice. In our test above we got a 12% performance rate versus a random chance of 10% (choosing a random number from 1 to 10). This is expected and a good sense check that our model parameters are initialized randomly. Now comes the fun part. Let&#39;s create a loss function against which it can measure itself. After that, we&#39;ll see how can create an optimizer function that will decide how to update the parameters to minimize the loss function, so as to continuously improve cycle after cycle. . We&#39;ll first softmax our parameters so that they are between 0 and 1, and we&#39;ll check whether the outputs match the targets. . # so we can work in multiple dimensions easily. # It works exactly like our own softmax() def loss_function(predictions, targets): sm = nn.Softmax(dim=-1) # instantiate PyTorch&#39;s softmax in the 2nd dimension predictions = sm(predictions) # calculate the softmax across the 2nd dimension return torch.where(targets==1, 1-predictions, predictions).mean(-1) . loss_function(preds, train_y) . torch.Size([60000, 10]) . tensor([0.2000, 0.2000, 0.2000, ..., 0.0141, 0.1996, 0.1299], grad_fn=&lt;MeanBackward1&gt;) . The Optimizer . Once our model instantiates random parameter values, makes a prediction and measures the first prediction against the loss function, we now have &quot;report card&quot; for how it performed. Staying with that analogy, now is the time to read our report card (our &quot;grades&quot; are the distances from the target value) and fix our parameters so we get a better grade the next time around, using the teacher&#39;s feedback (our gradients). . We first need to get our gradients, and then update our parameters using a simple formula in Stochastic Gradient Descent. This is what the formula looks like for each parameter $w$: . $ w := w - eta nabla Q({w}) $ . As usual, this looks super complicated until you see it in code: . loss = loss_function(preds, yb) # Get the gradients loss.mean().backward() # Optimize our parameters for p in [weights, bias]: p.data -= p.grad*lr # this line is the formula above p.grad.zero_() . All we&#39;re doing is just subtracting the gradient! To get the gradient, p.grad, we need to use PyTorch&#39;s .backward() functionality on the output of our loss_function, $Q$, so it can calculate the gradients for all the parameters in our weight and bias tensors as a result of the loss function. This is done automatically thanks to the fact that we added .requires_grad_() earlier to our parameters. . If you have a keen eye, you&#39;ll see in the code that we muliply the p.grad by a value called lr. This is the learning rate, $ eta$ in the formula. This is just a weighing factor we use to reduce the amount of movement along the loss function from one update to the next. Typical values range from 0.001 to 0.1. Why is it important to not go too much in one direction? If you imagine a loss function like a parabola, our optimal point is at the bottom of the parabola. If we overshoot, we could be bouncing around from one side of the parabola to the other. A smaller step-based approach helps prevent that. . Batches . Before we combine everything together into a model, let&#39;s introduce the concept of batches. Rather than measure predictions across all the dataset before estimating our performance, research has found that doing it with batches of training data is significantly faster and yields good results. This is also referred to as mini-batches. It&#39;s a compromise between having to run your function against all the values in the training dataset every time (therefore requiring a lot of resources when you have millions of items to process), or doing it for each single item in the dataset, which would be fast, but would not be representative of the group. Either extremes are bad for different reasons, so a good compromise is selecting a random batch of samples every time, and running the prediction-loss-step cycle on that batch thereby updating the parameters. A good batch size is small enough to be performant and large enough to be somewhat representative of the overall dataset. When we complete a round of batches (i.e. all of our samples are used in training exactly once), we have completed an epoch. . For batching, we&#39;ll use fast.ai&#39;s DataLoader. While it offers a lot of additional functionality, all we&#39;ll use it for is as an iterator that will split up our inputs into batches to feed into the model until all the samples are fed through. . dl = DataLoader(dset, batch_size=128, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=128, shuffle=True) . Our MNIST Model . Now that our dataset is conveniently split up into batches, we can combine all the different functionality we saw up until now and create an MNIST Model class to train. Below is a (naive and specific) implementation of a Model specifically trained to deal with the characteristics of our MNIST task. . Our class has the following characteristics (feel free to skip ahead and just read the source code directly) . Inputs . This is what the model needs to instantiate: . train_dl: our training DataLoader | valid_dl: our validation DataLoader | epochs: the number of epochs (complete rounds of the training data to perform) | lr: our learning rate | verbose: which just flags whether we want to print out feedback. | . Functions . These are the functions our model uses to perform the operations. They are prefixed with an _ to differentiate them from our methods. . _print(): is just a simple print function that only prints if flag verbose is true. | _init_params(): our initializes parameters. | _linear_eq(): our linear equation above. | _loss_function(): our loss function above. | _calc_grad(): takes the input training data, runs it through _linear_eq to get predictions and calculates the loss via the _loss_function. Then calls .backwards() on the loss results to get the gradients. | _batch_accuracy(): softmaxes a prediction and compares it against the corresponding label. That is, the index with the max value in the vector (highest probability) should be the same as the index with the &#39;1&#39; flag in our label vector. True if it&#39;s the case, False otherwise. Then returns the mean result, representing our batch accuracy. This is used in our _validate_epoch function and only used with validation data. | _validate_epoch(): once a training epoch is complete, we run our model through each batch of our validation data and cumulatively aggregate accuracies of each batch obtained via _batch_accuracy into an overall epoch score. | . Methods . Our model has two main methods: train and predict. . train: for each epoch, for each batch performs the training (via _calc_grad), the optimizing and validates each epoch (via _validate_epoch). | predict: similar to _batch_accuracy but only works on a single input image. It expects an image tensor as input and runs the tensor through its current parameters, outputting its predicted digit as well as the probabilities for other digits. | . class MNISTLinearRegression: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) # Return digit and vector of probabilities return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.mean().backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) # get the index of max value along 2nd dimension _, tag_indices = yb.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match return corrects.float().mean() # calculate mean def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): predictions = self.softmax(predictions) return torch.where(targets==1, 1-predictions, predictions).mean(-1) def _print(self, *args): if self.verbose: print(*args) # Linear regression using SGD def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Now that we&#39;ve created it, let&#39;s try it in action! . model = MNISTLinearRegression(dl, valid_dl, 50, 1, True) . model.train() . Epoch #0 0.225 Epoch #1 0.2896 Epoch #2 0.335 Epoch #3 0.3665 Epoch #4 0.4239 Epoch #5 0.4645 Epoch #6 0.486 Epoch #7 0.5055 Epoch #8 0.5214 Epoch #9 0.5243 Epoch #10 0.5343 Epoch #11 0.5356 Epoch #12 0.5389 Epoch #13 0.5466 Epoch #14 0.5463 Epoch #15 0.5468 Epoch #16 0.5506 Epoch #17 0.5522 Epoch #18 0.553 Epoch #19 0.5556 Epoch #20 0.5578 Epoch #21 0.5579 Epoch #22 0.5573 Epoch #23 0.5617 Epoch #24 0.5599 Epoch #25 0.5615 Epoch #26 0.5599 Epoch #27 0.5615 Epoch #28 0.5604 Epoch #29 0.5602 Epoch #30 0.5666 Epoch #31 0.5702 Epoch #32 0.5837 Epoch #33 0.6356 Epoch #34 0.6464 Epoch #35 0.6619 Epoch #36 0.677 Epoch #37 0.6907 Epoch #38 0.6961 Epoch #39 0.7083 Epoch #40 0.7107 Epoch #41 0.7188 Epoch #42 0.7252 Epoch #43 0.7276 Epoch #44 0.7353 Epoch #45 0.7479 Epoch #46 0.7646 Epoch #47 0.7808 Epoch #48 0.7929 Epoch #49 0.8005 . Not bad! After 50 epochs we reached 80% accuracy, way ahead of our baseline! I tried running this multiple times and the results vary, depending on the initial parameters and the learning rate. . Remember the example &#39;5&#39; we used earlier in the post to try out our baseline average pixel value approach? If you remember, it didn&#39;t even work properly as the distance to the reference means &#39;3&#39; and &#39;8&#39; was closer! Let&#39;s take that same image and see what our model predicts. . val_5 = (tensor(Image.open(testing[&#39;5&#39;][0])).float()/255).view(-1, 28*28) a = model.predict(val_5) a . (tensor([5]), tensor([[1.6423e-13, 7.6946e-22, 1.9228e-15, 2.0405e-04, 1.1598e-18, 9.9980e-01, 6.9507e-23, 2.2210e-20, 5.8148e-25, 1.8752e-18]], grad_fn=&lt;SoftmaxBackward&gt;)) . Yay. It works! It correctly predicted the &#39;5&#39; digit, with probability 99.98%. . Next Steps . You may have noticed that we called our model a &quot;MNISTLinearRegressor&quot;. I have to break it to you, what we&#39;ve done up to now is essentially create a linear regressor with a self-correcting capability through Stochastic Gradient Descent...not a deep neural network. . We&#39;ve also got some aspects of our current model we need to improve: . our loss function applies Softmax naively, which makes for a very &quot;hit or miss&quot; training performance depending on our initial parameters. The drawback of Softmax is it tends to be &quot;harsh&quot; on parameters by exasperating one over the rest. This is great when making final predictions and want 1 probability to shine through, but not when we want a nice stable loss function to help the model improve. We&#39;ll see how we can convert our naive loss function into an industry standard one, by softening Softmax (ironic, isn&#39;t it?). This will help get us better and more stable training accuracy in less than 4-5 epochs! | our model is also quite &quot;rigid&quot; in the sense that it&#39;s only built for MNIST. We&#39;ll generalize it into a general Linear Model, that accepts any number of labels in a much more elegant way! | Lastly, a neural network is non-linear. How do we break this linearity and make our model orders of magnitude more powerful? | . All of this coming up in Part 2! .",
            "url": "https://muttoni.github.io/blog/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html",
            "relUrl": "/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding DataBlocks and DataLoaders in fast.ai",
            "content": "Introduction . Coming from SciKit and TensorFlow, when I first started working with PyTorch and fast.ai I quickly realized they have a very opinionated (but convenient!) way to deal with data, through the DataBlocks and DataLoaders APIs. In this post we will quickly go over what they are (you can check the official documentation if you want to dive a little deeper), and understand how they work together. If you are not 100% clear about the difference between a Datablock and a DataLoader, this blog post hopefully will shed some light. . DataBlock . A Data block is nothing more than a pipeline for data assembly. When you initially create a DataBlock, you won’t need to specify any data. What you will need to specify, however, is a set of rules for how to treat your data when it does flow in. It doesn’t care about what you’ll do with it, it just cares about how you want it gathered, classified and split. . In order to create a Data block you need to specify . what types of data to expect for your input (aka features) and target variables (aka labels) | how to get the data | how to differentiate features from the target variables, | how to split the data for training (train &amp; validation set) | Let’s see how to do that. Below is an example DataBlock that we would create if we were looking to create a Convolutional Neural Network (CNN) to recognize different species of cats (i.e. a classification model). . cats = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . The four main steps mentioned above are exactly the first four (required) arguments of a DataBlock: . blocks: is where you define the types of data your model will work with. Usually you will specify at least two blocks: one that represents your independent (input) variable, and one that represents your dependent (target) variable. You can also specify multiple input/output variables. | get_items: a function that will actually go and pick up the data when necessary (more on this later) | splitter: how to split up the data in a training and validation set. The seed is optional and only added for replicability | get_y: how to extract the target (dependent) variable from the data. In the case of our cat classifier, this will be by looking at the parent folder, and fast.ai provides a built in function called parent_label.. | item_tfms is an optional argument that we can include to specify any additional processing that needs to be carried out when we flow our data through. In this case, we will resize all images to 128x128. We can specify other transforms, such as item_tfms=Resize(128, ResizeMethod.Squish)) which will resize and squish our images to fit, or item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;) to resize and pad any leftover space with black. This method is incredibly powerful as it also supports data augmentation. This is beyond the scope of this blog post, but just know that item_tfms allows you to pre-process your data before it hits your model. | In that snippet of code, what we’ve done is specify a template through which to create DataLoaders. What are they you might ask? Well, read on! . Tip: For more information on DataBlocks, I highly recommend Aman’s blog post covering the DataBlocks API. This is also where I got the image for the DataBlocks overview. . DataLoader and DataLoaders . Now that we’ve defined a DataBlock, and we’ve specified exactly how our data needs to be structured, categorized and processed, we can start actually feeding in the data for our model to train on. We load this data in with…you guessed it… a Data loader. This is where DataLoaders come in. A DataLoaders is an iterator class that our DataBlock will call to load data according to the rules that we’ve specified in specific chunks (called batch size). . A DataLoader in fast.ai is a superset of the PyTorch DataLoader, with more helpful callbacks and flexibility. Whereas the Data block knows how to structure the data, the Loader knows how to work with it in the context of training a machine learning model – i.e. how much to feed to the model at once (batch size), how many processes to spawn to load the data, how much memory to allocate and many more. . A DataLoaders (note the plural), is a thin class that automatically generates multiple DataLoader (singular) objects based on the rules specified in our DataBlock. . Conclusion and TLDR . We’ve seen what they are, now let’s re-iterate how they work together and what their differences are: . A DataBlock is the data pipeline. A template that we create that has NO data, but has all the context on how to work with it. For example, how to split up the data, the data types of our features and targets/labels, how to extract the labels from the underlying data (e.g. folder name). | A DataLoader doesn’t care about preparing data, it expects the data ready to go and only cares about how to load the data (e.g. whether in parallel or in a single process) as well as feeding the data to the model in batches (i.e. batch size) | A DataLoaders is a thin wrapper for more than one DataLoader. | . In the context of training a model, you will first create a DataBlock, specify all data processing pipelines, and then load your data through your DataBlock via the .dataloaders property, like so: . path = Path(&#39;cats&#39;) #e.g. a dir structure such as &#39;cats/sphynx/001.jpg&#39; dls = cats.dataloaders(path) # our data is ready to be fed into a model learn = cnn_leaner(dls, resent18, metrics=error_rate) learn.fine_tune(5) . Hope this post helps clarify what they are and how these two structures work together. As my knowledge of fast.ai grows, I’ll be sure to update this post .",
            "url": "https://muttoni.github.io/blog/machine-learning/fastai/2020/12/26/datablocks-vs-dataloaders.html",
            "relUrl": "/machine-learning/fastai/2020/12/26/datablocks-vs-dataloaders.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Event Gallery",
            "content": "Events . Over the last couple years I spoke at A LOT of events (over 300+). That’s what happens when you become a Technical Evangelist! I’ve been terrible at keeping track of photos from all the events, but will add as I come across them! . Notable speaking engagements: AWS re:Invent, Web Summit, IFA Berlin, Droidcon, Codemotion, Codetalks Hamburg, Cambridge University, Imperial College London, Euroconsumer, University College London, Alexa workshops in USA (Seattle), UK/IE (England, Ireland, Scotland), Germany (Berlin, Cologne, Frankfurt), Italy, Spain and hundreds more! . . .",
            "url": "https://muttoni.github.io/blog/events/2020/12/22/event-gallery.html",
            "relUrl": "/events/2020/12/22/event-gallery.html",
            "date": " • Dec 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi, thanks for taking the time to visit my blog. If you don’t know me, my name is Andrea Muttoni. Over the last 6 years I’ve been working at Amazon and I’ve covered all sorts of roles: from Kindle book pricing, to Technical Product Management and more recently Tech Evangelism. . My biggest passion is exploring the bottomless wonders of technology and music. Two areas where I will always feel like a Morty1, inspired and dragged along by all the genius Ricks of the world. . Main areas of interest: . Technical Product Management | Public speaking &amp; Developer Relations | Machine Learning | Music production | Anything that combines any of the above. | . Experience: . I have practical coding proficiency in various programming stacks, mainly NodeJS and web development. Over the years, I’ve built various services, tools, and browser extensions. For Machine Learning, mainly Python. | I have spoken at more than 300 events in all shapes and sizes: keynotes with thousands of executives, all the way to small hands-on coding workshops with 20 developers. Notable speaking engagements: AWS re:Invent, Web Summit, IFA Berlin, Droidcon, Codemotion, Codetalks Hamburg, Cambridge University, Imperial College London, Euroconsumer, University College London, Alexa workshops in USA (Seattle), UK/IE (England, Ireland, Scotland), Germany (Berlin, Cologne, Frankfurt), Italy, Spain and hundreds more! . | For a traditional CV, you can find me on LinkedIn. | . Certifications: . What brings you here? Have a question? Want to chat? Let me know by shooting me a note2 . Check my favicon. (Morty is a character from the show Rick and Morty) &#8617; . | You can reach me at my last name followed by my first name, at (f+1)mail. You can figure it out! Hopefully bots can’t…but probably will. &#8617; . |",
          "url": "https://muttoni.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://muttoni.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}