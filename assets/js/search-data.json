{
  
    
        "post0": {
            "title": "Using GraphQL to Build My Own Cloudflare Analytics Page",
            "content": "I’ve recently started a side project which has gotten more popular than I expected (~100k users). It’s also part of the reason I haven’t posted since January. While I won’t reveal what it is just yet, it did allow me to experiment with the Jamstack (which I’m really liking). The “JAM” in Jamstack stands for JavaScript, APIs and Markdown. It’s a way to architect an application so that it can be decoupled from a central server, pre-rendered and therefore hosted anywhere as a static site. With Jamstack, the entire front end is prebuilt into static pages and assets during a build process. The big upside to this architecture is that sites can be served directly from a CDN, without a backend server…at all! . Enter Cloudflare . As a primarily AWS user, I decided to try something different and use Cloudflare, having heard great things. The onboarding process was very easy, mainly centered around changing Name Servers. Once that was setup (and the obligatory 48 hour “DNS-related-lag-that-makes-you-doubt-your-config” lag passed), I was up and running with Cloudflare. I setup a simple page rule to cache everything and I was on my merry way. . Cloudflare Dashboard Analytics . Now one great thing about Cloudflare, is that it gives you very simple analytics right from the dashboard (requests, views, cached data served, uncached data served, etc). This was great for the first couple months as I used it as a simple way to monitor traffic. Here’s what it looks like: . . Upon clicking on “View more analytics” Cloudflare gives you access to more fine-grained data, but I quickly realized after the first month after launch that I couldn’t extend the analytics window past 30 days, so I needed a way to query that data myself. . So I was only interested in two things: . Total Requests (aggregate &amp; daily) | Views (aggregate &amp; daily) | Cloudflare Analytics GraphQL API . At first, I thought I needed some paid subscription or enterprise support of some sort. But I knew Cloudflare was a dev-friendly company so I kept digging. Eventually I found out they have a GraphQL endpoint dedicated to querying Analytics data. . So I whipped out GraphiQL and followed their brilliant guide . Authentication . Authenticating is quite easy, all I needed to do was add two HTTP headers in the API requests, one of which was the X-AUTH-EMAIL header, with value set to your email. The other was the specially generated bearer Authorization token (or alternatively I also tried using my global API key via the X-AUTH-KEY header). . . Querying for Views and Requests . Once the headers were setup, I pointed GraphiQL to https://api.cloudflare.com/client/v4/graphql and tried a couple of their example queries to test authentication. Success! However none of their examples had the fields I was interested in: requests and views. So after a little look at their schema I wrote the following two queries below using the httpRequests1dGroups data set and querying for requests and pageViews. The first query gives me a daily breakdown of views and requests, and the second gives a total aggregate. . GraphiQL is really easy to setup and use. Here’s what it looks like once you setup your headers, and write your first query (yes, that’s my real data!). If you’re wondering what happened on the 16th of Feb, well the Reddit Hug-of-Life™ happened. . . Daily Views &amp; Requests . query { viewer { zones(filter: {zoneTag: &quot;801c28fc7...d90178b2&quot; }) { httpRequests1dGroups( filter: { date_gt : &quot;2021-02-01&quot; } orderBy: [date_ASC] limit: 10000 ) { dimensions { date } sum { requests, pageViews, } } } } } . Which outputs: . { &quot;data&quot;: { &quot;viewer&quot;: { &quot;zones&quot;: [ { &quot;httpRequests1dGroups&quot;: [ { &quot;dimensions&quot;: { &quot;date&quot;: &quot;2021-02-16&quot; }, &quot;sum&quot;: { &quot;pageViews&quot;: 49396, &quot;requests&quot;: 201875 } }, { &quot;dimensions&quot;: { &quot;date&quot;: &quot;2021-02-17&quot; }, &quot;sum&quot;: { &quot;pageViews&quot;: 42540, &quot;requests&quot;: 175328 } }, // ... ] } ] } }, &quot;errors&quot;: null } . Aggregated Views &amp; Requests . query { viewer { zones(filter: {zoneTag: &quot;801c28fc7...d90178b2&quot; }) { httpRequests1dGroups( filter: { date_gt : &quot;2021-02-01&quot; } limit: 10000 ) { sum { requests, pageViews } } } } } . Which outputs: . { &quot;data&quot;: { &quot;viewer&quot;: { &quot;zones&quot;: [ { &quot;httpRequests1dGroups&quot;: [ { &quot;sum&quot;: { &quot;pageViews&quot;: 356899, &quot;requests&quot;: 1329471 } } ] } ] } }, &quot;errors&quot;: null } . As you can see, both queries are almost identical except the second doesn’t have a date dimension and corresponding orderBy logic. . Conclusions . What’s great about this setup is I have full control of what analytics data I see, and I can integrate both the query (via a simple fetch) as well as the output into a visual web dashboard and have a realtime view of how my website is doing. . For example: . function fetchAPI(url, email, apiKey, zoneTag) { let request = new Request(url) let query = {&quot;query&quot;:&quot;...the stringified query here...&quot;, &quot;variables&quot;:{}} let init = { method: &#39;POST&#39;, body: JSON.stringify(query) } request.headers.set(&#39;x-auth-key&#39;, apiKey) request.headers.set(&#39;x-auth-email&#39;, email) return fetch(request, init) } // More info if interested: // https://developers.cloudflare.com/analytics/graphql-api/tutorials/build-your-own-analytics . If you’re using Cloudflare, give it a go, and let me know if you build something cool! .",
            "url": "https://muttoni.github.io/blog/cloudflare/2021/04/15/Using-GraphQL-To-Query-Your-Cloudflare-Analytics.html",
            "relUrl": "/cloudflare/2021/04/15/Using-GraphQL-To-Query-Your-Cloudflare-Analytics.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Ensure Consistent LightGBM Predictions in Production",
            "content": "Recently I’ve been working on productionizing a regression model to accurately estimate used car values. This was following a request from a friend who owns a car dealership and wants to know two things: 1) how much a customer’s car will be worth in 3-4 years when they trade it in for a new model, and 2) what’s the trade-in value now of a new customer’s used car. I was looking for a new project so I decided to help him out. . The model uses a LightGBM booster with ~6-10k estimators (depending on the number of features used). It’s been quite the adventure, and I will write a blog post on the end-to-end process sometime in the future. In short, the process consists of: . Scraping data with Scrapy on multiple car sites via an Amazon EC2 instance and merging the data with other proprietary data. | Aggregating and cleaning the data and outputting a single, uniform dataset for model training. This is by far the most delicate and important step! | Training the model, benchmarking it against other methods including RF, XGBoost and Deep Learning using embeddings. | Deploying the model to production, making it accessible via an API endpoint. | Creating a web app that acts as a pretty customer frontend that queries the API for predictions. | This is all well and good, but having never worked with LightGBM before, some problems arose around Step 4, when I first started exporting the model and trying to replicate predictions in the production environment. . . Exporting a LightGBM Model . Now right off the bat, let’s just say that LightGBM is awesome– it’s an efficient gradient boosting framework that uses tree-based learning. It’s very efficient, uses lower memory than other tree/boosting methods and supports dealing with categorical label-encoded variables. However, I had a couple frustrations when porting my model from my prototyping environment to production. . Let’s first recap the various ways in which you can export a model. We’ll assume we have a standard booster that we need to save. Specific parameters are beyond the scope of this post. . import lightgbm as lgb # Our training model... bst = lgb.LGBMRegressor().fit(X, y) . Serializing with pickle or joblib . Two common ways to export any Sci-Kit model is pickle or joblib. They are quite similar, as joblib uses pickle as a protocol under the hood. Pickling is essentially a process of serializing a python object structure by converting the underlying object hierarchy into a byte stream. What’s not so great about pickling is that the resulting bytestream is hard to inspect unless unpickled (or generated using the oldest Protocol, v0). It also represents a potential security risk as a pickle could contain malicious code, and an untrusted pickle file opened without precautions could lead to naughty code being arbitrarily executed. joblib extends pickle by supporting compression helping serialize objects a bit more efficiently. . # Using pickle import pickle pickle.dump(bst, open(&#39;model.pkl&#39;, &#39;wb&#39;)) # Using joblib import joblib joblib.dump(bst, &#39;model.pkl&#39;) . To de-serialize (import) your model later, you would use: . # Using pickle import pickle bst = pickle.load(open(&#39;model.pkl&#39;, &#39;rb&#39;)) # Using joblib import joblib bst = joblib.load(&#39;model.pkl&#39;) . Once imported, it is theoretically the same as the original model. This is not always the case (read on!). . Exporting using LightGBM’s save_model . LightGBM also offers its own export functionality which can be called directly from the booster itself. The method is called .save_model. The method outputs a clear, human-readable text file. . # Saving the model using LightGBM&#39;s save_model method bst.booster_.save_model(&#39;model.txt&#39;) . To de-serialize (import) your model later, you would use: . # Importing the model using LightGBM&#39;s save_model method bst = lgb.Booster(model_file=&#39;model.txt&#39;) . Again, once imported, it is theoretically the same as the original model. However there’s some important considerations that I found out the hard way. . Inconsistent Predictions in Production . So you’re having a blast prototyping away on your Jupyter Notebook, getting unbelievable predictive accuracy, eye-wateringly low validation loss. Your team, manager and CEO are popping the champagne and patting you on the back saying “you did it, you rockstar!”. . Feeling invincible, you export the model and deploy it to production, start feeding it live data and your initial glee quickly turns to gut-wrenching dread as the predictions are all over the place. What went wrong?! . The first immediate hypothesis is that there’s something wrong with the file. You try exporting the model again, but the predictions are different, and just as wild. You start to question your sanity. You start sweating, a cold panic sweat. Your brain is taunting you: “All of that champage…for what?” . Then you remember you were reading this blog post, and continue on reading. . Common Reasons for Inconsistent LightGBM Predictions in Production . Environment Consistency . Goes without saying, that first and foremost you should ensure environment consistency. Make sure that your Python environment is identical to the one that you used in your model creation step. This means Python version, dependencies’ versions, pip requirements, etc. . If your production environment needs to maintain a different dependency set for some reason, an alternative would be to re-train your model in your production environment so you’re sure the same exact packages generating and saving your model are the same ones opening your model and making predictions. . Another good way to ensure consistency is to use virtualenv, generate a pip requirements list using pip freeze &gt; requirements.txt. In production, you would copy over the requirements file and install your dependencies by using pip install -r requirements.txt. Done! . Feature (Column) Ordering . The second reason is feature ordering. In my experience, after having ruled out the potential issues above, the real reason for wildly wrong results was column ordering! Simple as that. It was difficult to debug when using pickled files as the byte stream is not human readable. However, if you save your model using the text-based Booster.save_model format, you can inspect the model.txt. . Here’s an excerpt: . tree version=v3 num_class=1 num_tree_per_iteration=1 label_index=0 max_feature_idx=12 objective=regression feature_names=year color_cat km material_interiors_cat cv model_cat years_old transmission_cat fuel_cat seats cc doors brand_cat feature_infos=[2006:2021] -1:3:8:11:1:5:14:13:4:15:2:10:6:7:12:0 [500:2920000] -1:0:2:6:3:1:4 [40:796] -1:71:410:281:200:138:318:302:187:199:57:408 ... tree_sizes=3607 3727 3577 ... ... the file continues for 200,000 more lines . In the 8th line you see there’s a feature_names attribute where the columns (aka features) are listed. This ordering is very important! Make sure that any DataFrame or NumPy Array used for predictions follows this same ordering. . How to Ensure Consistent Column Ordering . A manual way to ensure consistent column ordering this is to extract the feature names from the model.txt file at the line called feature_names and splitting them based on the space character, for example: cols = &#39;feature1 feature2 feature3&#39;.split(&#39; &#39;). This is what you would do at 3AM when you are debugging with a tired mind (in my defense, it was quite late). Don’t do this. . The better way is to read the feature names directly using LightGBM’s built-in Booster.feature_name method. You can then reindex your production DataFrame / or array based on the order of those columns. In Pandas you would do this in the following way: . # In our production environment... import lightgbm as lgb # Load the booster bst = lgb.Booster(model_file=&#39;model.txt&#39;) # Get the model&#39;s features in the correct order cols = bst.feature_name() # -&gt; [&#39;feat1&#39;, &#39;feat2&#39;, &#39;feat3&#39;, ...] # Use col to reindex the prediction DataFrame df = df.reindex(columns=cols) # -&gt; df now has the same col ordering as the model # Get predictions that are consistent! predictions = bst.predict(df) . Conclusions . I’ll be sure to update this post as I uncover other quirks with productionizing this LightGBM model. So far I’m appreciating the compact size (compared to a Random Forest for example) and quick prediction speed! . That’s all for now. Good luck with your model deployment and hope you find this useful! .",
            "url": "https://muttoni.github.io/blog/machine-learning/2021/01/22/Fixing-Inconsistent-LightGBM-Predictions-In-Production.html",
            "relUrl": "/machine-learning/2021/01/22/Fixing-Inconsistent-LightGBM-Predictions-In-Production.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Tale of the Stochastic Markdown Bug",
            "content": "I’ve recently started writing a blog as a way to “slow down” my learning and document things for my future self. Today I was typing out some quick notes for a blog post in markdown on VSCode and was making liberal use of subheadings, as one does when explaining a topic in a structured way. I like to type without distractions so I don’t preview things right away. Upon saving and previewing the post some headings simply did not render. I tried reloading multiple times, editing the spacing, ensuring there was 1 space between the hashtags and the words, anything I could think of. Here’s a condensed example of what the issue looked like: . The markdown (heading type is irrelevant, and horizontal lines added for demo confinement): . ## Subheading that doesn&#39;t work ## Subheading that works . And the output: . . ## Subheading that doesn’t work . Subheading that works . . What the heck?! . Blaming Fastpages . Now for some context: the post was LONG, and full of tables, image placeholders pointing to images I had not yet created, randomly indented lists and missing line breaks after the headings. I immediately thought some weird combo had broken the parser. Furthermore, my blog is currently powered by fastpages, an opinionated customization layer on top of Jekyll that allows for quick publishing of Jupyter notebooks as blog posts hosted on github.io with a slightly tweaked minima interface (further tweaked by me). Chances are you’re still looking at it right now (highly recommended by the way). . Given the recent launch of fastpages, I suspected there was a bug in the markdown rendering pipeline, mainly due to the fact that my post made heavy use of lists and tables, potentially causing some parsing conflicts. So I tried fiddling with the text itself, trying to see if I could get around the parsing bug. I tried adding text before/after the heading, removing earlier parts of the post to see if a strange character in a code block was causing butterfly effects in the parsing several paragraphs down… nothing! . Establishing a Ground Truth . To check whether fastpages was the culprit, I then searched for a couple online markdown previewers as well as VSCode’s own markdown preview tab. Suprise suprise…the markdown text above didn’t work there either. Here’s an example: . . . So I installed the most popular markdown linter VSCode extension (Markdown Lint) to check whether I had made a mistake somewhere along the way that had propagated into an incorrect parsing of the headings. The linter identified several issues such as extra line breaks where there shouldn’t have been, trailing spaces, missing line breaks before a list–everything…but that damn heading. After fixing everything that was flagged, the problem persisted and the heading seemed perfectly A-OK for the linter! . . Ok, something’s up. . Occam’s Razor . It’s in situations like these, Occam’s Razor always comes to mind: . Occam’s razor, or law of parsimony is the problem-solving principle that “entities should not be multiplied without necessity”, or more simply, the simplest explanation is usually the right one. - Wikipedia . There was something I hadn’t tried. I had only cut, copied, pasted, adjusted, changed the title, moved the title around…but I had never actually tried to delete it and write it from scratch. . A moment of deep calm surrounds me. I hit the backspace character repeatedly until the whole heading disappears–including the line break for good measure. I press enter again and type a new heading slowly. It works. Damn you Occam! But why? Well, Occam again: if there’s a problem with the heading…the problem is probably in the heading, not everywhere else. So I start dissecting the heading and evalute possibilities: . The original line break was a weird unix/windows mix that is causing conflicts (unlikely, using macOS) | The original hashtags were special character equivalents (…are you drunk?) | The space inbetween the hashtags and the heading text was a special character (only NOW you think of this?) | . I have never heard of “special” hashtag characters, and I had tried to add and remove the linebreaks multiple times, and I ruled both out more or less immediately–in my heart I already knew the answer. Had reading HN daily really not taught me anything? The internet is full of examples of how whitespace can be leveraged to exploit form vulnerabilities, perform XSS injections and in general mess with software in fun and interesting ways. So I copied the space character from a heading that worked, and from the heading that didn’t work and fired up the old Javascript console. I took out my scalpel for the job, good ol’ String.prototype.charCodeAt(), and got to work. . . A-ha! Those numbers are decimal charcodes. I’ll give you a hint: 32 is the decimal charcode for a regular space. Can you guess what 160 is? If you’re a webdev keep it to yourself, don’t spoil it for the others! Charcode 160, aka &amp;nbsp; in HTML, is called a non-breaking space. Essentially identical to a normal space in every single way, including its unique ability to go undetected in Markdown Lint, except for the fact that it’s a completely different character. It’s usually used when you don’t want a trailing space or the space between two HTML tags to be automatically ignored. Ironic that it was ignored by both the markdown parser and the markdown linter. . Spaces are quite a rabbit hole. After a little research I found out there are (at least) 19 types of spaces. . U+0009 Horizontal tab (HT) U+0020 Space U+00A0 Non-break space U+1680 Ogham space mark U+180E Mongolian vowel separator U+2000 En quad U+2001 Em quad U+2002 En space U+2003 Em space U+2004 Three-per-em space U+2005 Four-per-em space U+2006 Six-per-em space U+2007 Figure space U+2008 Punctuation space U+2009 Thin space U+200A Hair space U+202F Narrow no-break space U+205F Medium mathematical space U+3000 Ideographic space . Granted it’s unlikely that all 19 will insidiously occupy my markdown documents, I’ll never trust an innocent looking space ever again! . ## Conclusions . Adventurers, please be wary of the insidiousness of the evil non-breaking space. It will strike when you least expect it. Oh, and I made Markdown Lint aware of the issue, so hopefully it won’t happen to you. Side note: upon trying other online markdown tools, the “whitespace award” goes to dillinger.io for correctly marking the non-breaking space as an illegal character. . There’s only one latent question: how did that non-breaking space get there in the first place? Good question. If I find out, I’ll update this post. If you find out, please reach out on Twitter at muttonia@! Current suspects are: . having copied and pasted the file from a non-markdown format (i.e. an untitled VSCode file, into a new .md file.) | fastpages accidentally altering the contents in its (first) rendering pass if the post is not properly formatted with a header metadata block at the beginning (which it wasn’t) | me being an idiot | . Addendum: Mystery Solved . Turns out I was an idiot. With an Italian keyboard layout, the # hashtag character is created by pressing the Alt Gr modifier key followed by the à key. Here’s the keyboard layout for those not familiar: . . It also turns out, that Alt Gr followed by the space key generates a non-breaking space (note: if this is common knowledge, I will bow my head in silence and sacrifice my Caps Lock key to the typing gods, preventing it’s reincarnation as a remapped CMD or CTRL key). . So what happened? Well, when typing with haste, after inserting hashtags for the heading, my thumbs occasionally pressed space before the Alt Gr key had time to fully unpress, causing a non-breaking space to sneak in the heading. Since the spaces look identical, there was no visual queue on VSCode or Markdown Lint that a problematic character had sneaked in. The stochastic nature of the issue (i.e. depending entirely on my typing speed and finger timing) meant that it also re-occured at random in my early troubleshooting further confusing my bug triaging. I told you those damn spaces are insidious. . So I guess the ultimate Occam’s Razor axiom is: if something goes wrong, it’s probably your own damn fault. So, whenever you are facing a furiously difficult bug, remember Occam’s Razor: . Occam’s razor, or law of parsimony is the problem-solving principle that “entities should not be multiplied without necessity”, or more simply, the simplest explanation is that you messed up. - Wikipedia, and me .",
            "url": "https://muttoni.github.io/blog/misc/2021/01/19/The-Tale-Of-A-Markdown-Bug.html",
            "relUrl": "/misc/2021/01/19/The-Tale-Of-A-Markdown-Bug.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "AWS for Machine Learning Notes Part 1 - Streaming Real-Time Data with Kinesis",
            "content": "Over the next several months I will cover various AWS services in the context of Machine Learning. The goal is to summarize the main components and tools in a concise way (e.g. cliff notes), as well as show some hands-on examples. The main reason for doing this is I’m studying for the AWS Certified Machine Learning Specialty Exam. In this first part, we will look at the start of any machine learning adventure: the data! In particular, how to deal with live, streaming data. . Amazon Kinesis . We live in a world where data is constantly being generated, and often the value of that data diminishes rapidly with time. Think of use-cases such as needing to react to financial indicators within milliseconds, aggregating clickstream data to optimize a user’s journey on a website or creating real time dashboards to monitor server cluster performance of an MMORPG. . It’s important therefore to be able to gather, process, evaluate (and sometimes store) that data quickly. Amazon Kinesis is a family of services that offers this functionality. It is made up of 4 main components: . Kinesis Data Streams (KDS) | Kinesis Data Firehose (KDF) | Kinesis Video Streams (KVS) | Kinesis Data Analytics (KDA) | . Let’s look at each individually. . Kinesis Data Streams . Kinesis (Data) Streams is a service that allows us to process data in real-time. It’s fast–once collected it’s able to offload it to a data consumer application (e.g. S3, Lambda, etc) within 70 milliseconds! That’s (literally) faster than a blink of an eye! Try blinking right now. It takes humans 100-400 ms to blink. So assuming a low-end average time of 150ms, in the time it took you to blink, Kinesis did what it needed to do in half the time. Ok I’m getting off topic. . Let’s look at the general structure of how Kinesis Streams are structured. . . # A more concise way to look at the above: Data Producers -&gt; Kinesis Streams (Shards) -&gt; Data Consumers (EC2, Lambda, KDA, EMR) -&gt; [Optional] Storage, Analysis (S3, Dynamo, Redshift...) . A typical Kinesis Data Streams application reads data from a data stream as data records. A data stream is a group of data records. Data records are grouped and distributed in shards (see below). Storing data is OPTIONAL with default retention being 24h, extendable to 7 days. Recommended when some action needs to be performed on the data. . Shards . A shard is a collection of data that is sequenced. Using the train example from acloud.guru: if we were to compare a data stream to a train line with trains traveling thousands of miles per millisecond: . each train is a shard, with its own partition key | each car of the train is a data record, with its own sequence number | the passengers in each car are the bytes that make up the data blob. Each car can fit up to 1MB of data. | . Useful facts: . Each shard consists of a sequence of data records. Shards can ingest 1000 records / sec, and support 5 read transactions per second. | Total bandwidth is therefore | A data record is the single unit of data (composed of seq number, partition key and blob, up to 1MB) | Transient data store: retention period 24h (default) to 7 days | Re-sharding is possible by monitoring producers and consumers via an API call | . Kinesis Producer Library (KPL) . Library to use Kinesis Data Streams and stream data | automatic and configurable retry mechanism | processing delay due to higher packing efficiencies / better performance | Java Wrapper | . Kinesis Client Library (KCL) . Library to consume streams | . Kinesis API (via the AWS SDK) . low level operations and manual configs of the two above | everything is manually handled (PutRecords, GetRecords) | no delays in processing | any AWS SDK (any language, not just Java) | . Examples of when to use Kinesis Data Streams: . Process and evaluate logs immediately | Run real-time analytics on clickstream data and process it within seconds | Feed data directly into a ML model | Real world example: Zillow uses it to collect public record data and MLS listings and update home value estimates in real-time. | . Kinesis Data Firehose . Delivery service for streaming data | Can support processing tools via Lambda | Supports Redshift, S3, Splunk | Supports events (e.g. S3 events to publish to dynamo) | Kinesis Data Streams as shards and has data retention vs | Firehose doesn’t have Shards, and storage is primary objective | . Examples of when to use Kinesis Firehose: . Easily collect streaming data from IoT devices, embedded systems, etc | Run ETL jobs on streaming data | Processing is optional | Final destination is S3 or other data store | Data retention is not important | . Kinesis Video Streams . stream video and images in real time (from security cameras, audio feeds, radar data, etc) | Continuous and Batch consumers (e.g. EC2) via fragments and frames a.k.a. KVS applications | Can be stored after or before processing | . Examples of when to use Kinesis Video Streams: . Real time streaming video data | Batch process and store streaming video | Feed streaming data into other AWS services | Ex: Amazon cloud cam -&gt; KVS to EC2 -&gt; identifies person -&gt; sends notification (Lambda) &amp; stores image (S3) | . Kinesis Video Analytics . Continuously read and process data in real-time from KDS or KDF | Run real time SQL queries (rename columns, joins, etc) | Store in S3, Redshift, other BI tools | . Examples of when to use KDA: . Run real-time SQL queries on streaming data | construct apps that provide insight into data | create metrics, dashboards, monitoring, notifs, alarms | output query results into S3 or other datasources | stream ETL jobs | . Examples . Objective Kinesis service to use Why . Stream Apache log files directly from several EC2 instances and store them into Redshift | Kinesis Data Firehose | Kinesis Data Analytics for preprocessing -&gt; Redshift. KDF is for storing streamed data to a final destination. First data is stored to S3 then moved to RS | . Stream live video coverage of a sporting event to distribute to customers in near real time | Kinesis Video Stream | Process video data and feed into other AWS services (e.g. Streaming EC2 instance or S3 or CloudFront) | . Transform real-time streaming data and immediately feed into a custom ML application | Kinesis Data Stream | Storing is optional, our primary goal is immediate consumption, transform it and feed it into an ML model | . Query real-time data, create metric graphs and store output into S3 | Kinesis Streams via Kinesis Data Analytics | KDA supports running SQL queries on streaming data and then store or feed the output to other services (e.g. S3) | . Hands on Lab . (still being written) .",
            "url": "https://muttoni.github.io/blog/machine-learning/aws/2021/01/18/AWS-for-Machine-Learning-Part-1-Kinesis.html",
            "relUrl": "/machine-learning/aws/2021/01/18/AWS-for-Machine-Learning-Part-1-Kinesis.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Implementing a Learning Rate Finder from Scratch",
            "content": "Introduction . In this post we will implement a learning rate finder from scratch. A learning rate finder helps us find sensible learning rates for our models to train with, including minimum and maximum values to use in a cyclical learning rate policy. Both concepts were invented by Leslie Smith and I suggest you check out his paper1! . We&#39;ll implement the learning rate finder (and cyclical learning rates in a future post) into our Deep Neural Network created from scratch in the 2-part series Implementing a Deep Neural Network from Scratch. Check that out first if you haven&#39;t read it already! . Understanding the Learning Rate . Before we start, what is the learning rate? The learning rate is just a value we multiply our gradients by in Stochastic Gradient Descent before updating our parameters with those values. Think of it like a &quot;weight&quot; that reduces the impact of each step change so as to ensure we are not over-shooting our loss function. If you imagine our loss function like a parabola, and our parameters starting somehwere along the parabola, descending along by a specific amount will bring us further &quot;down&quot; in the parabola, to it&#39;s minimum point eventually. If the step amount is too big however, we risk overshooting that minimum point. That&#39;s where the learning rate comes into play: it helps us achieve very small steps if desired. . To refresh your memory, here is what happens in the optimization step of Stochastic Gradient Descent (Part 1 of our DNN series explains this formula in more detail, so read that first): . $ w := w - eta nabla Q({w}) $ . Our parameter $w$ is updated by subtracting its gradient calculated with respect to the loss function $ nabla Q({w})$ after multiplying it by a weighing factor $ eta$. That weighing factor $ eta$ is our learning rate! . It&#39;s even easier in code: . # part of the .step() method in our SGD_Optimizer for p in self.parameters: p.data -= p.grad.data * self.lr . This is outlined in the .step method of our optimizer (check the setup code in the next section). As we saw towards the end of Part 2 of our Implementing a Deep Neural Network from Scratch series, the learning rate has a big impact on training for our model: the lower the learning rate, the more epochs required to reach a given accuracy, the higher the learning rate, the higher risk of overshooting (or never reaching) the minimum loss. There&#39;s a lot more factors at play that we cover in that series, but for now let&#39;s just consider the learning rate. . Another aspect of learning rates is that a single value is rarely optimal for the duration of the training. We could say that its efficacy degrades over time (time measured in batches/epochs). That&#39;s why common techniques include decreasing the learning rate by a step-wise fixed amount or by an exponentially decreasing amount during the course of the training. The logic being that as the model is further into training and is approaching the minimum loss, it needs less pronounced updates (steps), and therefore would benefit from smaller increments. . The Learning Rate Finder . The learning rate finder, or more appropriately learning rate range finder, is a method outlined in a paper by Leslie Smith written in 2015[^1]. The paper introduces the concept of cyclical learning rates (i.e. repeatedly cycling between learning rates inbetween a set minimum and a maximum has shown to be effective for training--a method we will implement from scratch in a future post!) whereby the minimum and maximum values to cycle through are found by a function that Leslie Smith defines as the &quot;LR range test&quot;. Here&#39;s what the author himself has to say about it: . There is a simple way to estimate reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. This test is enormously valuable whenever you are facing a new architecture or dataset.1 . So where does the learning rate finder come into play? Well, it helps us find how learning rates affect our training loss, helping us spot a &quot;sweet spot&quot; of ranges that maximize the loss. That extremes of that range will also be minimum and maximum values to use in the cyclical learning rates policy Leslie Smith outlines in his paper -- before we can implement cyclical learning rates, we need to implement a learning rate range finder! . To give you an idea of what we&#39;re trying to create, let&#39;s see fast.ai&#39;s implementation, probably one of the first frameworks to implement this LR range test. fast.ai offers a convenient helper function called the Learning Rate Finder (Learner.lr_finder()) that helps us see the effect a variety of learning rates have on our model&#39;s training performance as well as suggest a min_grad_lr where the gradient of the training loss is steepest: . Let&#39;s implement something similar! . &#160;Getting Started . Below is a recap of all the preparatory steps to setup our data pipeline. In order, we&#39;ll download the data, generate a list of file paths, create training and validation tensor stacks from the file paths, convert those tensors from rank-2 (2D matrix) tensors (i.e. size: 28, 28) to rank-1 (1D vector) tensors (i.e. size: 784). We&#39;ll then generate labels corresponding to the digit index and merge the input tensors and labels into datasets to create DataLoader. This will provide us with minibatches of sample data to run our SGD along. . # Requirements # !pip install -Uq fastbook # includes all the common imports (plt, fastai, pytorch, etc) # Download the data path = untar_data(URLs.MNIST) # Import the paths of our training and testing images training = { f&#39;{num}&#39; : (path/f&#39;training/{num}&#39;).ls().sorted() for num in range(10) } testing = { f&#39;{num}&#39; : (path/f&#39;testing/{num}&#39;).ls().sorted() for num in range(10) } # Prepare training tensor stacks training_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in training[f&#39;{num}&#39;] ]) for num in range(10) ] validation_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in testing[f&#39;{num}&#39;] ]) for num in range(10) ] # Convert our 2D image tensors (28, 28) into 1D vectors train_x = torch.cat(training_tensors).view(-1, 28*28) valid_x = torch.cat(validation_tensors).view(-1, 28*28) # Generate our labels based on the digit each image represents train_y = torch.from_numpy(np.concatenate([[i]*len(training[f&#39;{i}&#39;]) for i in range(10)])) valid_y = torch.from_numpy(np.concatenate([[i]*len(testing[f&#39;{i}&#39;]) for i in range(10)])) # Create datasets to feed into the dataloaders dset = list(zip(train_x, train_y)) dset_valid = list(zip(valid_x, valid_y)) # Setup our dataloders dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=256, shuffle=True) . . We&#39;ll also import the code that we created in our series on Implementing a Deep Neural Network from Scratch in Python. The code we&#39;ll need is our general purpose LinearModel, our SGD optimizer SGD_Optimizer and our beloved DeepClassifier. Feel free to toggle the code below to see how each one works. . # General purpose Linear Model class LinearModel: def __init__(self, inputs, outputs,): self.input_size = inputs self.output_size = outputs self.weights, self.bias = self._init_params() def parameters(self): return self.weights, self.bias def model(self, x): return x@self.weights + self.bias def _init_params(self): weights = (torch.randn(self.input_size, self.output_size)).requires_grad_() bias = (torch.randn(self.output_size)).requires_grad_() return weights, bias . . # General purpose SGD Optimizer class SGD_Optimizer: def __init__(self, parameters, lr): self.parameters = list(parameters) self.lr = lr def step(self): for p in self.parameters: p.data -= p.grad.data * self.lr for p in self.parameters: p.grad = None . . # General Purpose Classifier class DeepClassifier: &quot;&quot;&quot; A multi-layer Neural Network using ReLU activations and SGD params: layers to use (LinearModels) methods: fit(train_dl, valid_dl, epochs, lr) and predict(image_tensor) &quot;&quot;&quot; def __init__(self, *layers): self.accuracy_scores = [] self.training_losses = [] self.layers = layers def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;) self.valid_dl = kwargs.get(&#39;valid_dl&#39;) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] self.epoch_losses = [] self.last_layer_index = len(self.layers)-1 for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) for i in range(self.epochs): for xb, yb in self.train_dl: preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) def predict(self, image_tensor): probabilities = self._forward(image_tensor).softmax(dim=1) _, prediction = probabilities.max(-1) return prediction, probabilities def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res def _backward(self, preds, yb): loss = self._loss_function(preds, yb) self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step() def _batch_accuracy(self, xb, yb): predictions = xb.softmax(dim=1) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) self.epoch_losses = [] self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _ReLU(self, x): return x.max(tensor(0.0)) def _print(self, *args): if self.verbose: print(*args) . . Note: If you&#39;re looking for a walkthrough of the code above, make sure you read the 2 part series on implementing a deep neural network from scratch! I would especially focus on the second part. Here are the links: Part 1, Part 2. . &#160;Requirements . Now that our code and model is setup, we need to recap what we need to add to our DeepClassifier in order to properly support a lr_finder method. . Our goal: a method called lr_finder that when called performs a round of training (aka fitting) for a predetermined number of epochs, starting with a very small learning rate, increasing it exponentially every minibatch (why exponentially? So that we have an equal representation of small learning rate values vs large ones -- more on this later). We can sort out specifics later. . Warning: The lr_finder can&#8217;t start with random parameters, it should start with the current parameter state of the model, without affecting it during training! So we&#8217;ll need to clone our parameters. . We&#39;ll also need to update our code to keep track of training loss at every minibatch (across epochs), and store the respective learning rates used for each minibatch. . Now, before we continue, I want to make a big disclaimer: the additions we&#39;ll make today are going to be &quot;patches&quot; added on to a codebase that is, to put it lightly, fragile. The code above was created for demonstration purposes only, and essentially contains the bare essentials to make a deep neural network classifier work properly. The sole use of this code, for me, is to tinker with it and try out ideas and concepts as I come across them. . While I&#39;m tempted to re-write everything from scratch and build it into a more modular/generalized framework, it would introduce abstractions that for the purposes of our learning process would make &quot;getting&quot; the concepts more difficult, as effective abstractions inevitably hide away the lower level workings of a piece of code. . So I apologize for the entropy that we are about to introduce into this already &quot;entropic&quot; code. We might look into making it more robust in a future post. In the meantime, you are more than welcome to take this and refactor it to your liking! . With the disclaimer out of the way, let&#39;s recap how we normally used this DeepClassifier. We would instantiate the class and pass in the desired layers. Like so: . my_nn = DeepClassifier( # example usage LinearModel(28*28, 20), # a layer with 28*28 inputs (pixels), 20 activations LinearModel(20, 10) # a layer with 20 inputs and 10 activations/outputs ) . Each layer contains the input and output parameter counts. Every layer&#39;s autput is ReLU&#39;d automatically, except for the last one that uses Cross-Entropy Loss based on log and softmax. Read Part 2 of the DNN from scratch series to see exactly how this works. . We then call the fit method and pass in the data, the learning rate and the epochs to train for: . my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.1, epochs=100 ) . . Note: Note the limitations here: the learning rate is fixed--the very purpose of a learning rate finder is that it helps us map the effect of a range of learning rates. Furthermore, the data is linked with the fit method, meaning that if we don&#8217;t call .fit, we won&#8217;t have any data inside our classifier. . So we must change our code to allow for data to be fed in at the instantiation stage of the DeepClassifier class. We&#39;ll also need to move some of the properties that we used to create in our fit method, at the instantiation step. First however, let&#39;s tweak our dependencies LinearModel and SGD_Optimizer. . &#160;SGD_Optimizer . Given our learning rate needs to change over the course of each batch, we&#39;ll first need to be able to pass in a custom learning rate to our SGD optimizer&#39;s step method (where the updates are multiplied by the learning rate, as seen above). In our original code, the learning rate was set on instantiation and never touched again. We&#39;ll change it so the step method accepts a keyworded argument called lr, otherwise it defaults to its parameter self.lr. . class SGD_Optimizer: def __init__(self, parameters, lr): self.parameters = list(parameters) self.lr = lr def step(self, **kwargs): # add **kawrgs lr = kwargs.get(&#39;lr&#39;, self.lr) # get &#39;lr&#39; if set, otherwise set to self.lr for p in self.parameters: p.data -= p.grad.data * lr for p in self.parameters: p.grad = None . LinearModel . In our LinearModel we need to introduce some changes. We need two new methods that our lr_finder can use to copy and set parameters so as not to overwrite the actual model parameters. We&#39;ll introduce two new methods: copy_parameters (note the parameters method already acts as a get), and a set_parameters. These are quite self-explanatory and the only tricky thing is to make sure to clone and detach any copies from the gradient calculations, as well as reset gradient tracking when setting the parameters. See the code below: . # General purpose Linear Model class LinearModel: def __init__(self, inputs, outputs,): self.input_size = inputs self.output_size = outputs self.weights, self.bias = self._init_params() def parameters(self): return self.weights, self.bias def copy_parameters(self): return self.weights.clone().detach(), self.bias.clone().detach() def set_parameters(self, parameters): self.weights, self.bias = parameters self.weights.requires_grad_() self.bias.requires_grad_() def model(self, x): return x@self.weights + self.bias def _init_params(self): weights = (torch.randn(self.input_size, self.output_size)).requires_grad_() bias = (torch.randn(self.output_size)).requires_grad_() return weights, bias . . DeepClassifier . Prep Work . Now we can proceed with updating DeepClassifier. We&#39;ll start by by changing our __init__ method as follows: . def __init__(self, *layers, **kwargs): # added **kwargs self.layers = layers self.accuracy_scores = [] self.training_losses = [] # Moved all of the following from the &#39;fit&#39; method self.optimizers = [] self.epoch_losses = [] self.train_dl = kwargs.get(&#39;train_dl&#39;, None) self.valid_dl = kwargs.get(&#39;valid_dl&#39;, None) self.lr = kwargs.get(&#39;lr&#39;, 1e-2) self.last_layer_index = len(self.layers)-1 # We&#39;ll use this Boolean to determine whether to save # the individual batch losses in a list, instead of # averaging them at every epoch. We need to do this # as we&#39;ll need to plot them against each learning rate. self.save_batch_losses = False # if save_batch_losses is True, we save them here self.batch_losses = [] . We essentially moved a bunch of stuff from the fit method to the __init__ method so as to be able to access them from our upcoming lr_finder method. You can read the code above to see what changed. . The backward method will need to accept an optional lr and pass it on to the step method of our optimizers. This allows us to feed in a dynamic learning rate. at each step function, which is exactly what we need. . We also have a self.save_batch_losses that flags whether our individual batch losses (that usually get aggregated and averaged per epoch) should be persisted at batch granularity in self.batch_losses. This is necessary with the lr_finder because we need to plot individual batch losses with individual learning rates that changed every batch. . def _backward(self, preds, yb, **kwargs): lr = kwargs.get(&#39;lr&#39;, self.lr) loss = self._loss_function(preds, yb) if self.save_batch_losses: self.batch_losses.append(loss.item()) else: self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step(lr=lr) . Implementing lr_finder() . Believe it or not, those are all the changes we need to make lr_finder work! Now we are to implement the new method. Here it is in all its glory, and we&#39;ll go line by line. . def lr_finder(self): base_lr = 1e-6 max_lr = 1e+1 epochs = 3 current_lr = base_lr self.old_params = [layer.copy_parameters() for layer in self.layers] self.batch_losses = [] self.lr_finder_lrs = [] self.save_batch_losses = True batch_size = self.train_dl.bs samples = len(self.train_dl.dataset) iters = epochs * round(samples / batch_size) step_size = abs(math.log(max_lr/base_lr) / iters) print(batch_size, samples, iters, step_size) for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(), base_lr)) while current_lr &lt;= max_lr: for xb, yb in self.train_dl: if current_lr &gt; max_lr: break preds = self._forward(xb) self._backward(preds, yb, lr=current_lr) self.lr_finder_lrs.append(current_lr) current_lr = math.e**(math.log(current_lr)+step_size*2) # clean up self.save_batch_losses = False self.optimizers = [] for i in range(len(self.old_params)): self.layers[i].set_parameters(self.old_params[i]) . . Let&#39;s look at the code in detail: . base_lr = 1e-7 max_lr = 1e+1 epochs = 3 current_lr = base_lr . Our brand new lr_finder method accepts a base_lr learning rate to start the range of experimentation. It will stop when it reaches max_lr. epochs is an arbitrary number that we set to determine how many epochs to run the finder for. We also set a current_lr to the minimum lr we want to test and this variable will be incremented at every step. The first three parameters are critical as they determine how many steps will be carried out, as the steps are determined by batch size, total sample size and epochs, like so: . batch_size = self.train_dl.bs samples = len(self.train_dl.dataset) iters = epochs * round(samples / batch_size) step_size = abs(math.log(max_lr/base_lr) / iters) . In my particular implementation, the step size step_size is determined by the range of learning rates to try out, divided by the iterations the model will go through (number of epochs multiplied by the number of batches in our sample). . Now you may wonder why we are dividing the maximum learning rate by the minimum learning rate, instead of subtracting. The reason is: logs! Since we are dealing with logs, rather than do: $ frac{max _lr - min _lr}{iters}$, we do: $ frac{log( frac{max _lr}{min _lr})}{iters}$. Rather than divide the learning rates linearly, which, when plotted on log scale, has the effect of condensing the majority of the learning rates towards the higher end, I opted to increment our step size exponentially. To do this we need to take the log so that we deal with exponents and can later increment the next learning rate by $e^{current _lr+step _size}$. This will ensure that when we plot our learning rates on a log scale, our iterations will be evenly spaced out. . self.old_params = [layer.copy_parameters() for layer in self.layers] self.lr_finder_lrs = [] self.save_batch_losses = True . Next we save the old parameters so we can reset our model later. We create a list to store our learning rates and set the flag save_batch_losses to True, so that our _backward function knows to save them in our batch_losses list. The two lists lr_finder_lrs and batch_losses are the two lists that we will plot! . for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(), base_lr)) while current_lr &lt;= max_lr: for xb, yb in self.train_dl: if current_lr &gt; max_lr: break preds = self._forward(xb) self._backward(preds, yb, lr=current_lr) self.lr_finder_lrs.append(current_lr) current_lr = math.e**(math.log(current_lr)+step_size) . Next we proceed as if we were a fit method, by initiating optimizers for each layer and going through our training cycle. As you can see, the loop is a little different, where we don&#39;t worry about epochs, we just check that the current_lr is less than or equal to the max_lr. We then go through each batch, apply _forward, then apply _backward and then make sure to save our current learning rate and update the learning rate for the next batch. Since our step_size is exponent-based (i.e. we logged it at the beginning, giving us the exponent of $e$ that will give us that value), we need to increment the exponent of $e$ by that step size. We do this by taking the log of our current learning rate and incrementing that by the step_size which is already logarithmic. . The formula is as follows: $ current _lr = e^{log(current _lr) + step _size}$ . Once our training rounds are complete, the last thing left to do is cleanup! . self.save_batch_losses = False self.optimizers = [] for i in range(len(self.old_params)): self.layers[i].set_parameters(self.old_params[i]) . We reset our flag save_batch_losses, reset our optimizers and copy back the saved parameters into the model. Again, the reason we want to save the parameters is so that we can apply the lr_finder at any stage of our fitting cycles without it affecting our parameter state. . Here is the updated DeepClassifier code: . class DeepClassifier: &quot;&quot;&quot; A multi-layer Neural Network using ReLU activations and SGD params: layers to use (LinearModels) methods: - fit(train_dl, valid_dl, epochs, lr) - predict(image_tensor) - lr_finder() &quot;&quot;&quot; def __init__(self, *layers, **kwargs): self.layers = layers self.accuracy_scores = [] self.training_losses = [] self.optimizers = [] self.epoch_losses = [] self.batch_losses = [] self.train_dl = kwargs.get(&#39;train_dl&#39;, None) self.valid_dl = kwargs.get(&#39;valid_dl&#39;, None) self.last_layer_index = len(self.layers)-1 self.save_batch_losses = False self.lr = 0.1 def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;, self.train_dl) self.valid_dl = kwargs.get(&#39;valid_dl&#39;, self.valid_dl) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) for i in range(self.epochs): for xb, yb in self.train_dl: preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) def predict(self, image_tensor): probabilities = self._forward(image_tensor).softmax(dim=1) _, prediction = probabilities.max(-1) return prediction, probabilities def lr_finder(self): base_lr = 1e-6 max_lr = 1e+1 epochs = 3 current_lr = base_lr self.old_params = [layer.copy_parameters() for layer in self.layers] self.lr_finder_lrs = [] self.save_batch_losses = True batch_size = self.train_dl.bs samples = len(self.train_dl.dataset) iters = epochs * round(samples / batch_size) step_size = abs(math.log(max_lr/base_lr) / iters) for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(), base_lr)) while current_lr &lt;= max_lr: for xb, yb in self.train_dl: if current_lr &gt; max_lr: break preds = self._forward(xb) self._backward(preds, yb, lr=current_lr) self.lr_finder_lrs.append(current_lr) current_lr = math.e**(math.log(current_lr)+step_size) # clean up self.save_batch_losses = False self.optimizers = [] for i in range(len(self.old_params)): self.layers[i].set_parameters(self.old_params[i]) def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res def _backward(self, preds, yb, **kwargs): lr = kwargs.get(&#39;lr&#39;, self.lr) loss = self._loss_function(preds, yb) if self.save_batch_losses: self.batch_losses.append(loss.item()) else: self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step(lr=lr) def _batch_accuracy(self, xb, yb): predictions = xb.softmax(dim=1) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) self.epoch_losses = [] self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _ReLU(self, x): return x.max(tensor(0.0)) def _print(self, *args): if self.verbose: print(*args) . . &#160;Demo . Now let&#39;s try it out! We instantiate a new DeepClassifier by specifying the layers and passing in the data so we can run our lr_finder before running fit (where in our old code we would normally feed in our dataset). . my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 10), train_dl=dl, valid_dl=valid_dl ) my_nn.lr_finder() . Now let&#39;s create a quick function to plot the results (this can easily be turned into a method as well, but beyond the scope of this post: . def plot_lr_loss(my_nn): import matplotlib.pyplot as plt x = my_nn.lr_finder_lrs y = my_nn.batch_losses fig, ax = plt.subplots() ax.plot(x, y, &#39;g-&#39;) ax.set_xlabel(&#39;Learning Rates&#39;) ax.set_ylabel(&#39;Training Loss&#39;, color=&#39;g&#39;) plt.title(f&quot;Results with lr={my_nn.lr}&quot;) plt.xscale(&#39;log&#39;) plt.show() . plot_lr_loss(my_nn) . . Analyzing the lr_finder plot . As rules of thumb, fastai recommends picking the LR where the slope is steepest, or pick the end of the slop minimum and divide it by 10. In our case it seems our point of steepest slope is between $10^{-3}$ and $10^{-2}$. We definitely don&#39;t want to pick anything beyond 5e-1 as it seems the loss is flattening before picking up again. We also probably don&#39;t want to pick anything before $10^{-4}$ as the loss stays flat meaning it would take a lot of epochs before we see any significant improvement. If I were to pick a learning rate I would probably go with $10^{-2}$ for the first couple cycles, and then move to a finer one (e.g. $10^{-3}$). . Conclusion . Learning rates are a critical part of training a neural network using SGD and a good learning rate will not only help you get closest to the minimum loss, but will also speed up your training (i.e. less epochs to reach a specific accuracy). . Experiment with lr_finder yourself! Suggestions: try adding weights to our step_size increments (e.g. *2) and try incrementing it linearly instead of logarithmically and see how it affects the plot. . If you have suggestions for what to implement next, comment below! Hope you found this useful and all the best in your machine learning journey. . References . 1. Leslie N. Smith. (v6 2017, v1 2015). Cyclical Learning Rates for Training Neural Networks Link↩ .",
            "url": "https://muttoni.github.io/blog/machine-learning/2021/01/08/Implementing-a-Learning-Rate-Finder-from-Scratch.html",
            "relUrl": "/machine-learning/2021/01/08/Implementing-a-Learning-Rate-Finder-from-Scratch.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Implementing a Deep Neural Network from Scratch - Part 2",
            "content": "Introduction . . Note: This post is the second post of a two-part series. If you landed on this page directly, I recommend following the whole journey and start by reading Part 1 first. . This is Part 2 of our &quot;Implementing a Deep Neural Network from Scratch&quot; series. In this part we will improve on our Linear classifier started in Part 1 and turn our Linear Model into a fully-functioning Deep Neural Network! . As a reminder, our goal is to create a Neural Network that can distinguish all 10 handwritten digits (using the industry standard MNIST dataset). This is also known as a Multi-Label Classifier (a traditional Classifier only has two labels, such as cat or dog, hotdog or not hotdog, etc). In Part 1 we were able to train a simple linear regressor achieving roughly 80% accuracy after 50 epochs. We&#39;ll see in this part how to make it even more powerful with just a simple tweaks, as well as turn it into a Deep Neural Network. . Getting Started . Below is a recap of all the preparatory steps to setup our data pipeline. In order: we download the data, generate a list of file paths, create training and validation tensor stacks from the file paths, convert those tensors from rank-2 (2D matrix) tensors (i.e. size: 28, 28) to rank-1 (1D vector) tensors (i.e. size: 784). . # Requirements !pip install -Uq fastbook from fastbook import * # Download the data path = untar_data(URLs.MNIST) # Import the paths of our training and testing images training = { f&#39;{num}&#39; : (path/f&#39;training/{num}&#39;).ls().sorted() for num in range(10) } testing = { f&#39;{num}&#39; : (path/f&#39;testing/{num}&#39;).ls().sorted() for num in range(10) } # Prepare training tensor stacks training_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in training[f&#39;{num}&#39;] ]) for num in range(10) ] validation_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in testing[f&#39;{num}&#39;] ]) for num in range(10) ] # Convert our 2D image tensors (28, 28) into 1D vectors train_x = torch.cat(training_tensors).view(-1, 28*28) valid_x = torch.cat(validation_tensors).view(-1, 28*28) . . Before generating our labels, we&#39;ll make a change in our labeling structure so as to be more efficient in space complexity as well as be more elegant and extendable for any number of labels in the future. . Improving our Label Structure . Our labels in Part 1 were represented by a vector of size 10 with a &#39;1&#39; flag indicating the correct digit at the corresponding index, and &#39;0&#39; everywhere else. In our loss function, we then compared whether the max index of our predictions was equivalent to the index in our target labels where the index was &#39;1&#39;. Here&#39;s a quick example of how our old labels worked: . A target label $y$ for the digit &#39;3&#39;: . [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] ^ a &#39;1&#39; flagging the correct digit at its index (3) . An example prediction from our model: . [2.0e-3, 1.3e-7, 4.0e-8, 9.9e-1, 2.0e-5, 1.6e-4, 4.4e-9, 7.0e-9, 1.3e-6, 2.1e-4, 4.0e-7] ^ index of max value corresponds to the y label index, indicating a correct prediction. . There is, however, a much more efficient way of representing our labels. Can you guess what it is? If you guessed assigning a value to each of our digits (e.g. an integer corresponding to that digit) and just have that be our $y$ target label, then you guessed right! But in our loss function, how can we match/compare a prediction vector of 10 values to a target label, if the label contains only an integer (e.g. &quot;3&quot;). . To answer this question we need to go back to our beloved Softmax. . $ text{Softmax}(x_i) = frac{e^{x_i}}{ sum_j{e^{x_j}}}$ . This is a great time to read Part 1 if you haven&#39;t already done so. Here&#39;s a quick reminder of how our Softmax works in three steps: . Given a list of values, we calculate the exponential for each value, that is: $e^{n}$. Where $n$ is our value and $e$ is Euler&#39;s number, a special number in Mathematics, and at the heart of the exponential function. | We sum the exponential values (the Sigma symbol $ sum$ means sum). | Take each value in Step 1. and divide it by the sum obtained in Step 2. | The pecularity of Softmax is that when we softmax a list of values, since they are all divided by their sum, all the values sum to 1. This is incredibly useful as it means we only really need 1 value from the list, i.e. in our case the prediction corresponding to the correct label, as the rest of the values are just 1 - that value. If that value increases (i.e. the probability of our correct digit prediction increases), it also means that every other value in the prediction vector is consequently decreasing. This is why Softmax is a critical part of any multi-label classifier where only 1 label out of a group is correct (e.g. model of car), as it helps &quot;hone in&quot; on a single value, and work around that. . Practically, this means we can simplify our label structure and go from a vector of flags to a single integer representing the index of our possible labels. In the case of our MNIST dataset, the labels and indexes match, and will both go from 0 to 10. This means that if our $y$ variable is &#39;0&#39;, our digit is &#39;0&#39;, and we should look at the prediction value of our output vector at index &#39;0&#39;. The same goes for every other number. In cases were our $y$ is not a digit, we would still assign an integer to each possible label and proceed in exactly the same manner. . train_y = torch.from_numpy(np.concatenate([[i]*len(training[f&#39;{i}&#39;]) for i in range(10)])) valid_y = torch.from_numpy(np.concatenate([[i]*len(testing[f&#39;{i}&#39;]) for i in range(10)])) . In our loss and accuracy functions, we&#39;ll also need to update the way we compare results. All it takes is a little code change (our loss function will change a little bit in the next section, for now just focus on the updated labeling mechanism). . def loss_function(self, predictions, targets): # softmax the vectors, each containing 10 probabilities sm_preds = self.softmax(predictions) # generate a range equal to length of prediction batch idx = range(len(predictions)) # from each vector, only pick the correct prediction results = sm_preds[idx, targets] ... . This nifty accessing technique, coupled with Softmax&#39;s characteristic of summing to one, means that with only need a single value from our predictions, not the whole vector, to calculate the prediction loss, as every other value in our prediction vector is completely co-dependent and therefore irrelevant. . We can now load our dataset and dataloaders as we did in Part 1: . # Create datasets to feed into the dataloaders dset = list(zip(train_x, train_y)) dset_valid = list(zip(valid_x, valid_y)) # Setup our dataloders dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=256, shuffle=True) . Softmax Needs a Friend . Before we can proceed to improve and generalize our MNIST model from Part 1, we need to continue talking about Softmax. Yes, I know, by now you probably think I have a secret fetish for Softmax, and you&#39;re probably right. But Softmax needs a little TLC in order to function orders of magnitude better. . What the hell am I going on about? Well, Softmax is based on the exponential function $e^x$, which means that as our $x$ value increases, the exponential values increase...exponentially. The consequence of this is that a parameter that is slightly larger than the rest, when softmax&#39;d, will tend to be exaggerated, squishing the rest. Here is an example (toggle to code block to see the softmax implementation): . # Quick softmax implementation, expecting a list of values (rank-1 tensor) def softmax(v): &quot;&quot;&quot; Given an input vector v, returns an output vector containing the softmax values for each element in the vector &quot;&quot;&quot; exp_v = torch.exp(v.float()) exp_sum = exp_v.sum() return exp_v / exp_sum . . softmax(tensor([1,2,4])) . tensor([0.0420, 0.1142, 0.8438]) . Notice how in the example above, the &#39;4&#39;, when softmax&#39;d, takes a larger-than-proportional share of the probability. As the difference gets larger, this becomes even more evident. This is great when making final predictions as we want the highest probability to shine through, but not when we want a nice stable loss function to help the model improve. As soon as a parameter is slightly larger than the rest, it will be hard for the others to compete and &quot;catch up&quot;. We&#39;ll see how we can convert our naive loss function into an industry standard one, by softening Softmax (ironic, isn&#39;t it?). We&#39;ll then compare performance and you&#39;ll see how a very small change will help get us better and more stable training accuracy in just a few epochs! . Log: a Softmax&#39;s Best Friend . I&#39;ve never liked logs, unless they are made of wood. But it seems logs (aka logarithms) are a crucial part of everyday life, as they help represent exponential values in linear form. I won&#39;t dive too deep into logarithms, as there are plenty of resources that will do a better job at explaining them. Feel free to look them up for a couple minutes before continuing this post just so you have a quick primer. . Since Softmax stems from the exponential function ($e^x$), by taking the log of the softmax&#39;d values we are able to represent numers that are distanced exponentially in a more linear way. This is what I mean by &quot;softening&quot; Softmax, as rather than dealing with the raw values we are dealing with their exponents in base e. This helps stretch the values that were otherwise confined between 0 and 1, to a much larger range...what is the largest range imaginable? Infinity! Actually, for softmax it&#39;s 0 to -Infinity to be precise, and we&#39;ll see why in just a second. . . Tip: Our log function, in the context of Softmax, is always in base $e$, as our values are raised to the power of $e$. In Mathematics, logarithms involving $e$ (i.e. in base $e$) are often referred to as $ln$. . Our log function output (y-axis) can be intuitively thought of as: &quot;to the power of what must we raise $e$ in order to get $x$. This is why log is not defined for $x &lt; 0$ (remember: $e$ is positive, roughly equal to 2.718, so an exponent will never be able to change the sign of a positive number). . From the log plot above, we can see that $log_e(x)$ outputs values from -Infinity as $x$ approaches 0, and approaches 0 when $x$ approaches 1. Consequently, our Softmax values (that cumulatively sum to 1, and therefore always between 0 and 1) will always be negative. Let&#39;s take a look at our previous example with [1,2,4], softmax them again and then take the log. Notice how the results are negative, and the relative ratios between them have softened. . torch.log(softmax(tensor([1,2,4]))) . tensor([-3.1698, -2.1698, -0.1698]) . We can multiply very value by -1 so as to deal with positive numbers, without changing the fact that the highest original number (4) is now the closest to 0...that is...closest to our minimum. . So we now have a function that squeezes original parameters into co-dependent probabilties (via softmax), then stretches them into a broader logarithmic range (via log) to help linearize the rate of growth, and at the same time making it so that higher values turn into something that can be minimized (approach zero). Now doesn&#39;t that sound perfect for a loss function? . Cross-Entropy Loss . What we&#39;ve just seen above is a very popular loss function used in classification called Cross-Entropy Loss. It allows us to leverage the useful characteristics of softmax while retaining a relatively linear curve for parameter values so as to facilitate training. We&#39;ll see that by making this small change in our previous model, our training will be much more effective. . For reference, here is our old code (feel free to toggle it). . class MNISTLinearClassifier: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) # Return digit and vector of probabilities return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.mean().backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) # get the index of max value along 2nd dimension _, tag_indices = yb.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match return corrects.float().mean() # calculate mean def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): predictions = self.softmax(predictions) return torch.where(targets==1, 1-predictions, predictions).mean(-1) def _print(self, *args): if self.verbose: print(*args) # Linear regression using SGD def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Below is our new, updated code. We&#39;ll leverage the new label structure and implement a new loss function by performing all the steps discussed in the previous section: log our softmax values, and turn them positive. Everything else is pretty much identical (aside from our _batch_accuracy function that needed updating due our new labeling structure too). Feel free to diff it into diffchecker to see what changed. . class MNISTLinearClassifier: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): # Cross-Entropy Loss log_sm_preds = torch.log(self.softmax(predictions)) idx = range(len(predictions)) loss = -log_sm_preds[idx, targets] return loss.mean() def _print(self, *args): if self.verbose: print(*args) def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Now remember our old model took 50 epochs (on a good run) to reach 80% accuracy. If we try our new model now, it will consistently beat 80% accuracy within the first epoch! That&#39;s a considerable improvement thanks to a &quot;log&quot; and a negative sign. Goes to show how critical a proper loss function is in training a model effectively. . model = MNISTLinearClassifier(dl, valid_dl, 5, 0.5, True) . model.train() . Epoch #0 0.8151 Epoch #1 0.8507 Epoch #2 0.8656 Epoch #3 0.8777 Epoch #4 0.8789 . It already beat our original model within the first epoch (81.5% accuracy vs 80% after 50 epochs) and reached ~88% accuracy within 5 epochs. That&#39;s awesome! We could try experimenting with various batch sizes and learning rates to see how accurate we can get our model within an arbitrary number of epochs, but this is beyond the scope of this post. Let&#39;s clean up our code further and turn it into a general Linear Classifier. . Generalizing our Linear Model . Up until now, we wanted to build a MNIST classifier. Before we move on and &quot;deepen&quot; our neural network, let&#39;s clean up our code, modularize it and turn it into a general Linear Classifier that can be applied to any dataset, with any number of labels. . Linear Model . We&#39;ll start by generalizing our Linear equation into a standalone &quot;LinearModel&quot;. It will simply initialize a tensor with a desired number of input and output parameters (weights and biases) with random values. The model method will carry out the actual linear equation, and the parameters method will return the weights and biases. . class LinearModel: def __init__(self, inputs, outputs): self.input_size = inputs self.output_size = outputs self.weights, self.bias = self._init_params() def parameters(self): return self.weights, self.bias def model(self, x): return x@self.weights + self.bias def _init_params(self): weights = (torch.randn(self.input_size, self.output_size)).requires_grad_() bias = (torch.randn(self.output_size)).requires_grad_() return weights, bias . Optimizer . We&#39;ll then proceed to strip out our optimizer. Our optimizer will have a step method that will carry out the update for each parameter based on its gradient and an arbitrary learning rate lr (e.g. 0.01). . class SGD_Optimizer: def __init__(self, parameters, lr): self.parameters = list(parameters) self.lr = lr def step(self): for p in self.parameters: p.data -= p.grad.data * self.lr for p in self.parameters: p.grad = None . Now let&#39;s rewrite our Classifier to use these two newly created classes, and run it again on the MNIST dataset to check that it works: . class LinearClassifier: def __init__(self, input_shape, output_shape, train_dl, valid_dl, epochs, lr, optimizer, softmax, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.linear = LinearModel(input_shape, output_shape) self.optimizer = optimizer(self.linear.parameters(), self.lr) self.activation = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) self.optimizer.step() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.activation(self.linear.model(image_tensor)) _, prediction = probabilities.max(-1) return prediction, probabilities def _calc_grad(self, xb, yb): preds = self.linear.model(xb) loss = self._loss_function(preds, yb) loss.backward() def _batch_accuracy(self, xb, yb): predictions = self.activation(xb) _, x_label = xb.max(-1) corrects = x_label == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self.linear.model(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log(self.activation(predictions)) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _print(self, *args): if self.verbose: print(*args) . . train_dl = DataLoader(dset, batch_size=128, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=128, shuffle=True) classifier = LinearClassifier( input_shape=28*28, output_shape=10, train_dl=train_dl, valid_dl=valid_dl, epochs=5, lr=0.1, optimizer=SGD_Optimizer, softmax=True, verbose=True) . classifier.train() . Epoch #0 0.7352 Epoch #1 0.8014 Epoch #2 0.8286 Epoch #3 0.8457 Epoch #4 0.8532 . Like before, we could try experimenting with various batch sizes and learning rates -- here I used a batch_size of 128 and a learning rate lr of 0.1. The advantage of smaller batch sizes is that it allows the model to have more chances to perform gradient descent on the parameters before finishing an epoch, but the gradients may be less representative of the total dataset as they are based on fewer samples. The learning rate is also a critical aspect to experiment with as it determines how much of a step is done at every optimization run. If the learning rate is too big, we risk overshooting the minimum loss and potentially never reaching it, if the learning rate is too low, we won&#39;t make significant progress in training. Keep in mind you don&#39;t always need to guess, as some libraries offer a way to test different learning rates. For example, fast.ai offers a learning rate finder in its Learner class. I also wrote a post on implementing a learning rate finder from scratch. . Activating our Network! . As mentioned at the conclusion of Part 1, what we&#39;ve done up to now is create a linear classifier with a self-correcting capability through Stochastic Gradient Descent...not a deep neural network. That is, because a neural network is non-linear, and it is composed of multiple layers. How do we break the linearity and add layers? . Is it by adding one or more linear models after the first one? Yes! These are called layers (and any intermediate layers before our output layer are called &quot;hidden layers&quot;). But adding more layers is not enough. If we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. We need something more inbetween each linear classifier that breaks beyond the sad confines of a linear world: an activation function. . Before we tackle the activation function, let&#39;s recap what activations are. Activations are the calculated outputs from our linear equation(s). This means that in our Linear Model above, by creating 10 parameters for each pixel (remember our input weights are of shape (28*28, 10), our linear equation will output 10 activations per image. Since our Linear Classifier was comprised of only 1 layer of parameters, our 10 activations also correspond to the 10 desired outputs of our model, i.e. when softmax&#39;d each of the 10 items in the activation vector represent the probabilities for each digit [0-9] in our classification task. As we&#39;ll see in just a moment, the activations don&#39;t necessarily need to equal to our outputs, as there can be several layers of linear models in a deep neural network (hence the name: &quot;deep&quot;). . So one layer can output an arbitrary number of features (in practice the activations of that layer) and then feed those features into a second layer as inputs, that will output another set of features for the next layer, and so on, until the last layer where the activations correspond to our final output. . However, as we said earlier, if we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. So how can we add more than 1 linear layers and break the inherent linearity? That&#39;s where the activation function comes into play. . The Activation Function . The activation function acts as a filter through which input parameters are transformed from one layer to the next, so as to break the linear relationship. Any function that is not a linear equation technically acts as an non-linearity (e.g. a log function, an exponential function, etc). You can see a collection of activation functions here. . Among those, one function that works really well, is as simple as can be: the Rectifier function (also known as Rectified Linear Unit, or ReLU). . $f(x) = max(x, 0)$ . That&#39;s it! The ReLU will simply replace any negative input values with 0 (or you can read it as: rectify any negative values to 0). Other input values remain completely unchanged. In code, this is as simple as: . def ReLU(x): return x.max(tensor(0.)) plot_function(ReLU, title=&quot;The ReLU Function&quot;) . This simple transformation is enough to break linearity! Feel free to read more about the ReLU function), but for our purposes this is all we need to continue. . Implementing a Deep Neural Network . . Equipped with our handy ReLU, we are ready to transform our Linear Model into a real Neural Network and allow for more than one linear layer! Now remember, a Neural Network is comprised of multiple linear layers (2 being the minimum not counting the activation function: one layer for the inputs, the activation, and the second layer for the outputs). Any number of layers can be added inbetween and are called hidden layers. There can be as many layers as we want, although the deeper the network, the more sophisticated it will be, meaning it will take longer to train, and the increased number of parameters can absorb a lot more peculiarities from each training image potentially over-fitting the data and not performing as well in validation, testing and production. We&#39;ll touch more on this further on in the post. . What we&#39;re going to be doing now, is effectively implement a very simple version of PyTorch&#39;s nn.Sequential. In nn.Sequential, we can specify $n$ layers and what activation function to use. The activations of any prior layer correspond to the inputs of the next layer. The last layer will always output the activations to be run through the loss function, exactly like in our LinearClassifier. . # Example 1 simple_net = nn.Sequential( nn.Linear(28*28,30), # an input layer of 28*28 with 30 activations nn.ReLU(), # the ReLU activation nn.Linear(30,10) # another linear layer with 30 inputs and 10 activations ) # Example 2 deeper_net = nn.Sequential( nn.Linear(28*28,100),# an input layer of 28*28 with 100 activations nn.ReLU(), # the ReLU activation nn.Linear(100,30), # a hidden layer of 100 inputs and 30 activations nn.ReLU(), # the ReLU activation nn.Linear(30,10) # another linear layer with 30 inputs and 10 activations ) . So how can we replicate something similar to the above? The good news is that we don&#39;t need to change a whole lot from our original LinearClassifier, except to introduce the concept of layers to our Classifier, as well as add the ReLU function. Other parts of the code (SGD_Optimizer and LinearModel) can be re-used as is! . Layers . Taking inspiration from PyTorch&#39;s nn.Sequential mentioned above, we&#39;ll accept layers in a similar fashion, except we&#39;ll ReLU any input and hidden layers (everything but the last layer) automatically. That means we&#39;ll need to implement our DeepClassifier so as to be able to setup our model in the following way: . # Example 1 (note: we still need to implement DeepClassifier) simple_nn = DeepClassifier( LinearModel(28*28, 20), # We&#39;ll ReLU automatically after Layer 1. LinearModel(20, 10) # LinearModel is the same one we used previously ) # Example 2 deeper_nn = DeepClassifier( LinearModel(28*28, 40), # first layer, ReLU&#39;d automatically LinearModel(40, 20), # hidden layer, also ReLU&#39;d automatically LinearModel(20, 10) # output layer ) . We&#39;ll begin by changing the name of our class from LinearClassifier to DeepClassifier. Finally! We&#39;ll also add a self.layers property in the __init__ method that stores the incoming layers provided as a list of arguments when we setup the model. I&#39;d also like to start logging/saving training loss to compare against our validation accuracy stored in self.accuracy_scores (remember it saves results from the _validate_epoch function that runs our weights against the validation data after each epoch). To do this we&#39;ll also add a self.training_loss list to save to. . def __init__(self, *layers): self.accuracy_scores = [] self.training_losses = [] self.layers = layers # add a layers property . Fit . We&#39;ll then change the terminology of our classifier so that instead of train we use fit. This is more consistent with other Deep Learning Libraries. In our fit method, we&#39;ll calculate how many layers self.layers has, and save the last layer&#39;s index (self.last_layer_index) so that we don&#39;t ReLU that layer later on. We&#39;ll also setup an optimizer for each layer that we can call .step from, using our previously defined SGD_Optimizer class. We&#39;ll also store the losses of each batch in an epoch to average out and compare against our validation accuracy. . def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;) self.valid_dl = kwargs.get(&#39;valid_dl&#39;) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] self.epoch_losses = [] # we&#39;ll average this after every epoch self.last_layer_index = len(self.layers)-1 # save last layer&#39;s index. for layer in self.layers: # for each layer, create an Optimizer we can .step() later on self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) # our usual training loop for i in range(self.epochs): for xb, yb in self.train_dl: # notice we use _forward now preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) . Forward &amp; Backward . We&#39;ll also introduce another structural change in the form of how our model operates. Rather than call _calc_grad, we&#39;ll instead group all of our linear calculations and activations in a _forward function, and all of our gradient calculations and optimizations in a _backward function. This makes more sense logically as we first calculate, then backtrack to get the gradients and update our parameters accordingly. . The _forward step loops through each layer in self.layers and calls each LinearModel&#39;s model method (i.e. the famous linear equation xb@w + b). As long as the layer is not the last layer (if layer_idx != self.last_layer_index:), it will also rectify the outputs before passing them on to the next layer. . def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res . The ReLU function is identicaly to the one created above. Pretty simple right? . def _ReLU(self, x): return x.max(tensor(0.0)) . Our backward step _backward entails running our final outputs through the loss function and then calling .backward() to calculate the gradients of each tensor automatically. We&#39;ll also take note of the loss output so we can average each epoch&#39;s average loss in our _validate_epoch function. We&#39;ll then loop through all of our optimizers in self.optimizers and step() them so as to update the parameters according to our learning rate. . def _backward(self, preds, yb): loss = self._loss_function(preds, yb) # keep track of each batch&#39;s loss in a list self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step() . Our loss function also changes slightly, as we&#39;ll switch to using torch.log_softmax directly instead of torch.log followed by torch.softmax. This is because our previous implementation caused issues with gradients returning NaN across multiple layers. . def _loss_function(self, predictions, targets): # we use torch.log_softmax directly log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() . Finally, we can update our _validate_epoch function to condense the batch&#39;s losses into an average so we can compare each epoch&#39;s validation accuracy score vs the average training loss. . def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) # stack all the saved losses from each batch and take the average epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) # clear the array for the next epoch of batches self.epoch_losses = [] # append to the training_losses list self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) . Putting It All Together . And here is our DeepClassifier in all its glory! . class DeepClassifier: &quot;&quot;&quot; A multi-layer Neural Network using ReLU activations and SGD params: layers to use (LinearModels) methods: fit(train_dl, valid_dl, epochs, lr) &quot;&quot;&quot; def __init__(self, *layers): self.accuracy_scores = [] self.training_losses = [] self.layers = layers def fit(self, **kwargs): self.train_dl = kwargs.get(&#39;train_dl&#39;) self.valid_dl = kwargs.get(&#39;valid_dl&#39;) self.epochs = kwargs.get(&#39;epochs&#39;, 5) self.lr = kwargs.get(&#39;lr&#39;, 0.1) self.verbose = kwargs.get(&#39;verbose&#39;, True) self.optimizers = [] self.epoch_losses = [] self.last_layer_index = len(self.layers)-1 for layer in self.layers: self.optimizers.append(SGD_Optimizer(layer.parameters(),self.lr)) for i in range(self.epochs): for xb, yb in self.train_dl: preds = self._forward(xb) self._backward(preds, yb) self._validate_epoch(i) def predict(self, image_tensor): probabilities = self._forward(image_tensor).softmax(dim=1) _, prediction = probabilities.max(-1) return prediction, probabilities def _forward(self, xb): res = xb for layer_idx, layer in enumerate(self.layers): if layer_idx != self.last_layer_index: res = layer.model(res) res = self._ReLU(res) else: res = layer.model(res) return res def _backward(self, preds, yb): loss = self._loss_function(preds, yb) self.epoch_losses.append(loss) loss.backward() for opt in self.optimizers: opt.step() def _batch_accuracy(self, xb, yb): predictions = xb.softmax(dim=1) _, max_indices = xb.max(-1) corrects = max_indices == yb return corrects.float().mean() def _validate_epoch(self, i): accs = [self._batch_accuracy(self._forward(xb), yb) for xb, yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) epoch_loss = round(torch.stack(self.epoch_losses).mean().item(), 4) self.epoch_losses = [] self.training_losses.append(epoch_loss) self._print(f&#39;Epoch #{i}&#39;, &#39;Loss:&#39;, epoch_loss, &#39;Accuracy:&#39;, score) def _loss_function(self, predictions, targets): log_sm_preds = torch.log_softmax(predictions, dim=1) idx = range(len(predictions)) results = -log_sm_preds[idx, targets] return results.mean() def _ReLU(self, x): return x.max(tensor(0.0)) def _print(self, *args): if self.verbose: print(*args) . . The full code is available in this gist: https://gist.github.com/muttoni/d5ce076fdc83b8f82b9971c5c8bf6b2d . &#160;Testing our Deep Classifier . Now that we&#39;ve implemented our DeepClassifier, let&#39;s test its performance. Before we do, it&#39;s important to discuss the topic of complexity (i.e. depth and breadth). The deeper (more layers) and broader (more activations) we make our network, the more parameters and complexity our model will be able to learn, with the risk of potentially over-fitting our training data. The number of layers and parameters in a network should always be proportional to the amount of data and its complexity. There is no rule to apply, you just need to experiment and see. There&#39;s plenty of discussions around this topic so feel free to browse the web for people&#39;s rules of thumbs. . It&#39;s important to remember, that even the simplest (2 layer) neural network is able to theoretically approximate any function, and for most simple tasks is more than enough! You can try as an exercise to play around with layers and activation sizes to see how training is affected. Remember the accuracy scores outputed from all of the models created until now are always against the validation data, meaning that you may see accuracy decrease over time (a clear sign that the model is overfitting on the training data). . Anyways, enough talk, let&#39;s see it in action! . train_dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=256, shuffle=True) my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 10) ) . my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.1, epochs=100 ) . Epoch #0 Loss: 2.9898 Accuracy: 0.5713 Epoch #1 Loss: 1.2028 Accuracy: 0.6553 Epoch #2 Loss: 1.0008 Accuracy: 0.6754 Epoch #3 Loss: 0.8964 Accuracy: 0.7375 Epoch #4 Loss: 0.8207 Accuracy: 0.7498 Epoch #5 Loss: 0.756 Accuracy: 0.7441 Epoch #6 Loss: 0.7119 Accuracy: 0.7968 Epoch #7 Loss: 0.6837 Accuracy: 0.7902 Epoch #8 Loss: 0.642 Accuracy: 0.8166 Epoch #9 Loss: 0.6159 Accuracy: 0.7876 Epoch #10 Loss: 0.588 Accuracy: 0.8122 Epoch #11 Loss: 0.5736 Accuracy: 0.8244 Epoch #12 Loss: 0.5501 Accuracy: 0.835 Epoch #13 Loss: 0.5312 Accuracy: 0.8494 Epoch #14 Loss: 0.5108 Accuracy: 0.847 Epoch #15 Loss: 0.5047 Accuracy: 0.8305 Epoch #16 Loss: 0.4969 Accuracy: 0.8367 Epoch #17 Loss: 0.4766 Accuracy: 0.8019 Epoch #18 Loss: 0.4663 Accuracy: 0.8555 Epoch #19 Loss: 0.4654 Accuracy: 0.8535 Epoch #20 Loss: 0.4491 Accuracy: 0.8722 Epoch #21 Loss: 0.4398 Accuracy: 0.8579 Epoch #22 Loss: 0.4373 Accuracy: 0.8768 Epoch #23 Loss: 0.4362 Accuracy: 0.8375 Epoch #24 Loss: 0.4235 Accuracy: 0.8765 Epoch #25 Loss: 0.4211 Accuracy: 0.8749 Epoch #26 Loss: 0.4167 Accuracy: 0.8379 Epoch #27 Loss: 0.4098 Accuracy: 0.8618 Epoch #28 Loss: 0.4002 Accuracy: 0.8837 Epoch #29 Loss: 0.3974 Accuracy: 0.8702 Epoch #30 Loss: 0.3911 Accuracy: 0.8827 Epoch #31 Loss: 0.3914 Accuracy: 0.8854 Epoch #32 Loss: 0.3893 Accuracy: 0.8752 Epoch #33 Loss: 0.3807 Accuracy: 0.8861 Epoch #34 Loss: 0.3773 Accuracy: 0.8909 Epoch #35 Loss: 0.3733 Accuracy: 0.8909 Epoch #36 Loss: 0.3733 Accuracy: 0.8849 Epoch #37 Loss: 0.3669 Accuracy: 0.8615 Epoch #38 Loss: 0.3641 Accuracy: 0.8708 Epoch #39 Loss: 0.3621 Accuracy: 0.8679 Epoch #40 Loss: 0.3572 Accuracy: 0.8912 Epoch #41 Loss: 0.3553 Accuracy: 0.8609 Epoch #42 Loss: 0.3526 Accuracy: 0.8836 Epoch #43 Loss: 0.3485 Accuracy: 0.8936 Epoch #44 Loss: 0.3473 Accuracy: 0.9019 Epoch #45 Loss: 0.3433 Accuracy: 0.8969 Epoch #46 Loss: 0.3404 Accuracy: 0.9049 Epoch #47 Loss: 0.3385 Accuracy: 0.8957 Epoch #48 Loss: 0.3361 Accuracy: 0.8951 Epoch #49 Loss: 0.331 Accuracy: 0.8816 Epoch #50 Loss: 0.3327 Accuracy: 0.8783 Epoch #51 Loss: 0.334 Accuracy: 0.8942 Epoch #52 Loss: 0.3297 Accuracy: 0.9052 Epoch #53 Loss: 0.33 Accuracy: 0.9071 Epoch #54 Loss: 0.3244 Accuracy: 0.905 Epoch #55 Loss: 0.3213 Accuracy: 0.9092 Epoch #56 Loss: 0.3224 Accuracy: 0.911 Epoch #57 Loss: 0.319 Accuracy: 0.9026 Epoch #58 Loss: 0.3161 Accuracy: 0.8915 Epoch #59 Loss: 0.316 Accuracy: 0.9078 Epoch #60 Loss: 0.3127 Accuracy: 0.9003 Epoch #61 Loss: 0.3118 Accuracy: 0.9074 Epoch #62 Loss: 0.3106 Accuracy: 0.8938 Epoch #63 Loss: 0.3125 Accuracy: 0.9112 Epoch #64 Loss: 0.3076 Accuracy: 0.9062 Epoch #65 Loss: 0.307 Accuracy: 0.9005 Epoch #66 Loss: 0.3056 Accuracy: 0.8538 Epoch #67 Loss: 0.3034 Accuracy: 0.9003 Epoch #68 Loss: 0.302 Accuracy: 0.8929 Epoch #69 Loss: 0.2992 Accuracy: 0.9079 Epoch #70 Loss: 0.2992 Accuracy: 0.9103 Epoch #71 Loss: 0.2953 Accuracy: 0.9105 Epoch #72 Loss: 0.2958 Accuracy: 0.914 Epoch #73 Loss: 0.2951 Accuracy: 0.9104 Epoch #74 Loss: 0.2921 Accuracy: 0.9086 Epoch #75 Loss: 0.2933 Accuracy: 0.8952 Epoch #76 Loss: 0.2889 Accuracy: 0.9094 Epoch #77 Loss: 0.2878 Accuracy: 0.8972 Epoch #78 Loss: 0.2882 Accuracy: 0.908 Epoch #79 Loss: 0.2867 Accuracy: 0.9147 Epoch #80 Loss: 0.2867 Accuracy: 0.9047 Epoch #81 Loss: 0.2856 Accuracy: 0.9044 Epoch #82 Loss: 0.2822 Accuracy: 0.9166 Epoch #83 Loss: 0.2823 Accuracy: 0.9145 Epoch #84 Loss: 0.2803 Accuracy: 0.9143 Epoch #85 Loss: 0.2796 Accuracy: 0.9025 Epoch #86 Loss: 0.2805 Accuracy: 0.9007 Epoch #87 Loss: 0.2797 Accuracy: 0.9041 Epoch #88 Loss: 0.2775 Accuracy: 0.9074 Epoch #89 Loss: 0.2768 Accuracy: 0.9158 Epoch #90 Loss: 0.2758 Accuracy: 0.9051 Epoch #91 Loss: 0.2744 Accuracy: 0.9112 Epoch #92 Loss: 0.2721 Accuracy: 0.9094 Epoch #93 Loss: 0.2719 Accuracy: 0.8801 Epoch #94 Loss: 0.2721 Accuracy: 0.9114 Epoch #95 Loss: 0.2695 Accuracy: 0.9176 Epoch #96 Loss: 0.2674 Accuracy: 0.9097 Epoch #97 Loss: 0.2689 Accuracy: 0.9131 Epoch #98 Loss: 0.2671 Accuracy: 0.9178 Epoch #99 Loss: 0.2668 Accuracy: 0.9119 . . 91%! (toggle the output to see the various training losses and accuracy scores over each epoch). Let&#39;s also test our beloved sample &#39;5&#39; prediction. . val_5 = (tensor(Image.open(testing[&#39;5&#39;][0])).float()/255).view(-1, 28*28) a = my_nn.predict(val_5) a . (tensor([5]), tensor([[8.7256e-06, 6.7960e-08, 2.6373e-04, 1.8036e-02, 1.8940e-16, 9.4832e-01, 2.3715e-09, 6.1468e-06, 3.3328e-02, 4.1378e-05]], grad_fn=&lt;SoftmaxBackward&gt;)) . It gives us a confidence of 95% that the digit is 5. Nice! Now let&#39;s discuss our results in more detail. . Analyzing Results . Deep Neural Networks need to train longer as there are more parameters to tune. In our case, our 2-layer network after 100 epochs reached 91-92% accuracy with a learning rate of 0.1. Aside from the depth (layers) and breadth (parameters) of the model, learning rate also affects the number of epochs it takes as it &quot;weighs down&quot; the updating &quot;power&quot; of the optimizer so as to not overshoot the minimum loss. All of these decisions are called hyper-parameters: parameters that are used to configured our model. They are called differents o as not to confuse them with our underlying model&#39;s parameters (weights and baises). . By looking at the results above, we see how the training loss is still has room for improvement (remember, we&#39;re trying to get that as close to 0 as possible). When training a model it&#39;s important to consider over-fitting vs model accuracy. If your validation accuracy and training loss are both moving in the direction you want, it means you still have room to train! It doesn&#39;t make sense to continue training if instead you see the validation accuracy degrading consistently as that would be a clear sign of over-fitting on the training data. . To visualize all this better, let&#39;s create a quick function to visualize how our training loss and validation accuracy evolve over time (epochs). . def plot_results(my_nn): import matplotlib.pyplot as plt x = range(len(my_nn.accuracy_scores)) y1 = my_nn.accuracy_scores y2 = my_nn.training_losses fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(x, y1, &#39;g-&#39;) ax2.plot(x, y2, &#39;b-&#39;) ax1.set_xlabel(&#39;Epochs&#39;) ax1.set_ylabel(&#39;Validation Accuracy&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Training Loss&#39;, color=&#39;b&#39;) plt.title(f&quot;Results with lr={my_nn.lr}&quot;) plt.show() . . plot_results(my_nn) . The graph above shows how, over the course of 100 epochs, our training loss (in blue) and our validation accuracy (in green) have evolved. You&#39;ll see that our validation accuracy jumps around--that&#39;s because our model is only training on the training dataset, and our validation scores are measured on the validation dataset, so they don&#39;t get fed back into the model! This helps keep the model &quot;honest&quot;, and gives us an idea of our our model will perform on unseen data. As both our accuracy and loss are trending in the right direction, we could experiment extending the epoch count and tweaking the learning rate. . Learning Rates . Below are the same graphs with different learning rates, showing how they affect the model&#39;s performance over 100 epochs. As you can see, the higher the learning rate, the faster the accuracy improves, but it causes more jitter as the optimizing steps can overshoot across the loss function. The trick here is to pick one proportional to the amount of epochs you want to train (lower for more epochs, smaller for less epochs). Modern libraries (eg fast.ai) have convenient learning rate finders to take the guess work out. . Note that if you fit your model, and then try to fit it again, it will start from the previous weights, so you should probably pick a smaller learning rate the second time around as the &quot;big&quot; movements have already been done. . . Notice how the lower the learning rate, the smoother the training, but slower the ramp up and accuracy. . Epochs . Now let&#39;s experiment by using a somewhat large 0.5 learning rate and training for 2.5x more epochs (250 total). . my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 10) ) my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.5, epochs=250 ) . Epoch #0 Loss: 1.7751 Accuracy: 0.6389 Epoch #1 Loss: 0.9891 Accuracy: 0.6981 Epoch #2 Loss: 0.7729 Accuracy: 0.7354 Epoch #3 Loss: 0.6788 Accuracy: 0.7412 Epoch #4 Loss: 0.613 Accuracy: 0.8305 Epoch #5 Loss: 0.5669 Accuracy: 0.7853 Epoch #6 Loss: 0.5323 Accuracy: 0.8262 Epoch #7 Loss: 0.4896 Accuracy: 0.8599 Epoch #8 Loss: 0.4714 Accuracy: 0.8681 Epoch #9 Loss: 0.4435 Accuracy: 0.873 Epoch #10 Loss: 0.4322 Accuracy: 0.8764 Epoch #11 Loss: 0.4081 Accuracy: 0.8563 Epoch #12 Loss: 0.3952 Accuracy: 0.8856 Epoch #13 Loss: 0.3825 Accuracy: 0.8837 Epoch #14 Loss: 0.3687 Accuracy: 0.8929 Epoch #15 Loss: 0.3569 Accuracy: 0.8975 Epoch #16 Loss: 0.344 Accuracy: 0.8987 Epoch #17 Loss: 0.3389 Accuracy: 0.9014 Epoch #18 Loss: 0.3296 Accuracy: 0.9057 Epoch #19 Loss: 0.3199 Accuracy: 0.9066 Epoch #20 Loss: 0.3178 Accuracy: 0.9041 Epoch #21 Loss: 0.3072 Accuracy: 0.8971 Epoch #22 Loss: 0.302 Accuracy: 0.899 Epoch #23 Loss: 0.3025 Accuracy: 0.9099 Epoch #24 Loss: 0.2941 Accuracy: 0.9134 Epoch #25 Loss: 0.2913 Accuracy: 0.9169 Epoch #26 Loss: 0.2833 Accuracy: 0.8979 Epoch #27 Loss: 0.2781 Accuracy: 0.9188 Epoch #28 Loss: 0.2759 Accuracy: 0.9042 Epoch #29 Loss: 0.2744 Accuracy: 0.9164 Epoch #30 Loss: 0.2717 Accuracy: 0.9194 Epoch #31 Loss: 0.2653 Accuracy: 0.9067 Epoch #32 Loss: 0.2633 Accuracy: 0.9209 Epoch #33 Loss: 0.2592 Accuracy: 0.9164 Epoch #34 Loss: 0.255 Accuracy: 0.9213 Epoch #35 Loss: 0.255 Accuracy: 0.8712 Epoch #36 Loss: 0.2509 Accuracy: 0.9276 Epoch #37 Loss: 0.2467 Accuracy: 0.9143 Epoch #38 Loss: 0.2446 Accuracy: 0.9174 Epoch #39 Loss: 0.2424 Accuracy: 0.9229 Epoch #40 Loss: 0.2427 Accuracy: 0.91 Epoch #41 Loss: 0.2387 Accuracy: 0.9168 Epoch #42 Loss: 0.2385 Accuracy: 0.9262 Epoch #43 Loss: 0.233 Accuracy: 0.911 Epoch #44 Loss: 0.2315 Accuracy: 0.9234 Epoch #45 Loss: 0.2321 Accuracy: 0.9221 Epoch #46 Loss: 0.228 Accuracy: 0.9074 Epoch #47 Loss: 0.2272 Accuracy: 0.9271 Epoch #48 Loss: 0.2257 Accuracy: 0.9249 Epoch #49 Loss: 0.2232 Accuracy: 0.9246 Epoch #50 Loss: 0.2239 Accuracy: 0.9265 Epoch #51 Loss: 0.2205 Accuracy: 0.9291 Epoch #52 Loss: 0.2181 Accuracy: 0.914 Epoch #53 Loss: 0.2172 Accuracy: 0.9203 Epoch #54 Loss: 0.2151 Accuracy: 0.9182 Epoch #55 Loss: 0.2154 Accuracy: 0.9282 Epoch #56 Loss: 0.215 Accuracy: 0.9323 Epoch #57 Loss: 0.2115 Accuracy: 0.9291 Epoch #58 Loss: 0.2086 Accuracy: 0.9319 Epoch #59 Loss: 0.2092 Accuracy: 0.9187 Epoch #60 Loss: 0.2074 Accuracy: 0.9312 Epoch #61 Loss: 0.2067 Accuracy: 0.926 Epoch #62 Loss: 0.2053 Accuracy: 0.936 Epoch #63 Loss: 0.2039 Accuracy: 0.9301 Epoch #64 Loss: 0.2027 Accuracy: 0.9297 Epoch #65 Loss: 0.2007 Accuracy: 0.934 Epoch #66 Loss: 0.1998 Accuracy: 0.9223 Epoch #67 Loss: 0.2011 Accuracy: 0.9325 Epoch #68 Loss: 0.1985 Accuracy: 0.8839 Epoch #69 Loss: 0.199 Accuracy: 0.918 Epoch #70 Loss: 0.1978 Accuracy: 0.9266 Epoch #71 Loss: 0.1965 Accuracy: 0.9358 Epoch #72 Loss: 0.1954 Accuracy: 0.9352 Epoch #73 Loss: 0.1931 Accuracy: 0.9331 Epoch #74 Loss: 0.1925 Accuracy: 0.9264 Epoch #75 Loss: 0.1929 Accuracy: 0.9303 Epoch #76 Loss: 0.1907 Accuracy: 0.9367 Epoch #77 Loss: 0.1906 Accuracy: 0.9142 Epoch #78 Loss: 0.1906 Accuracy: 0.9322 Epoch #79 Loss: 0.1887 Accuracy: 0.9319 Epoch #80 Loss: 0.187 Accuracy: 0.9371 Epoch #81 Loss: 0.1875 Accuracy: 0.9312 Epoch #82 Loss: 0.1848 Accuracy: 0.9226 Epoch #83 Loss: 0.1835 Accuracy: 0.9337 Epoch #84 Loss: 0.1837 Accuracy: 0.9375 Epoch #85 Loss: 0.1836 Accuracy: 0.9378 Epoch #86 Loss: 0.1829 Accuracy: 0.9172 Epoch #87 Loss: 0.1828 Accuracy: 0.935 Epoch #88 Loss: 0.1811 Accuracy: 0.93 Epoch #89 Loss: 0.1806 Accuracy: 0.9255 Epoch #90 Loss: 0.1786 Accuracy: 0.9409 Epoch #91 Loss: 0.1798 Accuracy: 0.9369 Epoch #92 Loss: 0.1777 Accuracy: 0.9365 Epoch #93 Loss: 0.1774 Accuracy: 0.9386 Epoch #94 Loss: 0.176 Accuracy: 0.9317 Epoch #95 Loss: 0.1775 Accuracy: 0.939 Epoch #96 Loss: 0.1745 Accuracy: 0.9374 Epoch #97 Loss: 0.1752 Accuracy: 0.9396 Epoch #98 Loss: 0.1733 Accuracy: 0.9371 Epoch #99 Loss: 0.1734 Accuracy: 0.9242 Epoch #100 Loss: 0.1725 Accuracy: 0.9359 Epoch #101 Loss: 0.174 Accuracy: 0.9354 Epoch #102 Loss: 0.1716 Accuracy: 0.8946 Epoch #103 Loss: 0.1701 Accuracy: 0.9367 Epoch #104 Loss: 0.1712 Accuracy: 0.9365 Epoch #105 Loss: 0.1681 Accuracy: 0.9356 Epoch #106 Loss: 0.1673 Accuracy: 0.9351 Epoch #107 Loss: 0.1679 Accuracy: 0.9362 Epoch #108 Loss: 0.1677 Accuracy: 0.9446 Epoch #109 Loss: 0.1669 Accuracy: 0.9212 Epoch #110 Loss: 0.166 Accuracy: 0.9394 Epoch #111 Loss: 0.1648 Accuracy: 0.9427 Epoch #112 Loss: 0.1647 Accuracy: 0.9346 Epoch #113 Loss: 0.1641 Accuracy: 0.938 Epoch #114 Loss: 0.1635 Accuracy: 0.9395 Epoch #115 Loss: 0.1631 Accuracy: 0.9348 Epoch #116 Loss: 0.1634 Accuracy: 0.94 Epoch #117 Loss: 0.1619 Accuracy: 0.9359 Epoch #118 Loss: 0.1617 Accuracy: 0.9387 Epoch #119 Loss: 0.1615 Accuracy: 0.941 Epoch #120 Loss: 0.1605 Accuracy: 0.9393 Epoch #121 Loss: 0.1594 Accuracy: 0.9404 Epoch #122 Loss: 0.1589 Accuracy: 0.9408 Epoch #123 Loss: 0.1586 Accuracy: 0.9412 Epoch #124 Loss: 0.1577 Accuracy: 0.9394 Epoch #125 Loss: 0.1577 Accuracy: 0.937 Epoch #126 Loss: 0.1582 Accuracy: 0.9396 Epoch #127 Loss: 0.1568 Accuracy: 0.9353 Epoch #128 Loss: 0.1571 Accuracy: 0.9432 Epoch #129 Loss: 0.1561 Accuracy: 0.9309 Epoch #130 Loss: 0.1553 Accuracy: 0.9401 Epoch #131 Loss: 0.1557 Accuracy: 0.9359 Epoch #132 Loss: 0.1532 Accuracy: 0.9406 Epoch #133 Loss: 0.1532 Accuracy: 0.937 Epoch #134 Loss: 0.1541 Accuracy: 0.9371 Epoch #135 Loss: 0.1526 Accuracy: 0.9378 Epoch #136 Loss: 0.1518 Accuracy: 0.9429 Epoch #137 Loss: 0.152 Accuracy: 0.9403 Epoch #138 Loss: 0.1517 Accuracy: 0.9401 Epoch #139 Loss: 0.1507 Accuracy: 0.9383 Epoch #140 Loss: 0.1511 Accuracy: 0.9437 Epoch #141 Loss: 0.1498 Accuracy: 0.9434 Epoch #142 Loss: 0.1498 Accuracy: 0.9444 Epoch #143 Loss: 0.1502 Accuracy: 0.9408 Epoch #144 Loss: 0.149 Accuracy: 0.9457 Epoch #145 Loss: 0.1475 Accuracy: 0.9382 Epoch #146 Loss: 0.1483 Accuracy: 0.9436 Epoch #147 Loss: 0.1476 Accuracy: 0.9399 Epoch #148 Loss: 0.1469 Accuracy: 0.9373 Epoch #149 Loss: 0.1466 Accuracy: 0.9434 Epoch #150 Loss: 0.1458 Accuracy: 0.9418 Epoch #151 Loss: 0.1468 Accuracy: 0.941 Epoch #152 Loss: 0.1457 Accuracy: 0.9421 Epoch #153 Loss: 0.1459 Accuracy: 0.9434 Epoch #154 Loss: 0.1457 Accuracy: 0.943 Epoch #155 Loss: 0.1445 Accuracy: 0.944 Epoch #156 Loss: 0.1438 Accuracy: 0.9339 Epoch #157 Loss: 0.1442 Accuracy: 0.9331 Epoch #158 Loss: 0.1432 Accuracy: 0.9449 Epoch #159 Loss: 0.1437 Accuracy: 0.9409 Epoch #160 Loss: 0.1429 Accuracy: 0.9454 Epoch #161 Loss: 0.1426 Accuracy: 0.9386 Epoch #162 Loss: 0.1423 Accuracy: 0.9441 Epoch #163 Loss: 0.1415 Accuracy: 0.9434 Epoch #164 Loss: 0.1409 Accuracy: 0.9437 Epoch #165 Loss: 0.1414 Accuracy: 0.9368 Epoch #166 Loss: 0.1413 Accuracy: 0.9432 Epoch #167 Loss: 0.1405 Accuracy: 0.9458 Epoch #168 Loss: 0.14 Accuracy: 0.9451 Epoch #169 Loss: 0.1401 Accuracy: 0.9401 Epoch #170 Loss: 0.1391 Accuracy: 0.946 Epoch #171 Loss: 0.1391 Accuracy: 0.9427 Epoch #172 Loss: 0.1385 Accuracy: 0.9467 Epoch #173 Loss: 0.1383 Accuracy: 0.944 Epoch #174 Loss: 0.138 Accuracy: 0.9447 Epoch #175 Loss: 0.1378 Accuracy: 0.944 Epoch #176 Loss: 0.1367 Accuracy: 0.9349 Epoch #177 Loss: 0.1369 Accuracy: 0.943 Epoch #178 Loss: 0.1375 Accuracy: 0.9405 Epoch #179 Loss: 0.1365 Accuracy: 0.9472 Epoch #180 Loss: 0.1364 Accuracy: 0.9111 Epoch #181 Loss: 0.1372 Accuracy: 0.9347 Epoch #182 Loss: 0.1349 Accuracy: 0.9419 Epoch #183 Loss: 0.1344 Accuracy: 0.947 Epoch #184 Loss: 0.1343 Accuracy: 0.9422 Epoch #185 Loss: 0.1337 Accuracy: 0.9431 Epoch #186 Loss: 0.1338 Accuracy: 0.9449 Epoch #187 Loss: 0.1329 Accuracy: 0.9446 Epoch #188 Loss: 0.1337 Accuracy: 0.9471 Epoch #189 Loss: 0.1334 Accuracy: 0.9419 Epoch #190 Loss: 0.1332 Accuracy: 0.945 Epoch #191 Loss: 0.1331 Accuracy: 0.9424 Epoch #192 Loss: 0.1324 Accuracy: 0.9388 Epoch #193 Loss: 0.1319 Accuracy: 0.9441 Epoch #194 Loss: 0.1322 Accuracy: 0.9436 Epoch #195 Loss: 0.1315 Accuracy: 0.9414 Epoch #196 Loss: 0.1313 Accuracy: 0.9418 Epoch #197 Loss: 0.1309 Accuracy: 0.9415 Epoch #198 Loss: 0.1308 Accuracy: 0.9414 Epoch #199 Loss: 0.1304 Accuracy: 0.9364 Epoch #200 Loss: 0.1299 Accuracy: 0.9451 Epoch #201 Loss: 0.1282 Accuracy: 0.9452 Epoch #202 Loss: 0.1289 Accuracy: 0.9432 Epoch #203 Loss: 0.1285 Accuracy: 0.946 Epoch #204 Loss: 0.1285 Accuracy: 0.9405 Epoch #205 Loss: 0.1284 Accuracy: 0.9372 Epoch #206 Loss: 0.1291 Accuracy: 0.9208 Epoch #207 Loss: 0.1286 Accuracy: 0.9455 Epoch #208 Loss: 0.1275 Accuracy: 0.946 Epoch #209 Loss: 0.1274 Accuracy: 0.9418 Epoch #210 Loss: 0.1262 Accuracy: 0.9455 Epoch #211 Loss: 0.1276 Accuracy: 0.9294 Epoch #212 Loss: 0.1265 Accuracy: 0.9461 Epoch #213 Loss: 0.1262 Accuracy: 0.9446 Epoch #214 Loss: 0.1262 Accuracy: 0.9404 Epoch #215 Loss: 0.1264 Accuracy: 0.9455 Epoch #216 Loss: 0.1255 Accuracy: 0.94 Epoch #217 Loss: 0.1254 Accuracy: 0.9456 Epoch #218 Loss: 0.1251 Accuracy: 0.9431 Epoch #219 Loss: 0.125 Accuracy: 0.9403 Epoch #220 Loss: 0.1251 Accuracy: 0.9384 Epoch #221 Loss: 0.1238 Accuracy: 0.9441 Epoch #222 Loss: 0.1246 Accuracy: 0.9445 Epoch #223 Loss: 0.1244 Accuracy: 0.9426 Epoch #224 Loss: 0.1243 Accuracy: 0.9451 Epoch #225 Loss: 0.1228 Accuracy: 0.9445 Epoch #226 Loss: 0.1238 Accuracy: 0.9471 Epoch #227 Loss: 0.1229 Accuracy: 0.9419 Epoch #228 Loss: 0.1226 Accuracy: 0.9423 Epoch #229 Loss: 0.1217 Accuracy: 0.9407 Epoch #230 Loss: 0.1216 Accuracy: 0.9418 Epoch #231 Loss: 0.1222 Accuracy: 0.9415 Epoch #232 Loss: 0.1211 Accuracy: 0.942 Epoch #233 Loss: 0.1221 Accuracy: 0.937 Epoch #234 Loss: 0.1217 Accuracy: 0.9425 Epoch #235 Loss: 0.1208 Accuracy: 0.9453 Epoch #236 Loss: 0.1208 Accuracy: 0.9472 Epoch #237 Loss: 0.1204 Accuracy: 0.9354 Epoch #238 Loss: 0.1201 Accuracy: 0.9421 Epoch #239 Loss: 0.1191 Accuracy: 0.9463 Epoch #240 Loss: 0.1196 Accuracy: 0.9423 Epoch #241 Loss: 0.12 Accuracy: 0.9432 Epoch #242 Loss: 0.1188 Accuracy: 0.9446 Epoch #243 Loss: 0.1187 Accuracy: 0.9443 Epoch #244 Loss: 0.1181 Accuracy: 0.9442 Epoch #245 Loss: 0.1185 Accuracy: 0.9457 Epoch #246 Loss: 0.118 Accuracy: 0.9451 Epoch #247 Loss: 0.1175 Accuracy: 0.943 Epoch #248 Loss: 0.1181 Accuracy: 0.9462 Epoch #249 Loss: 0.117 Accuracy: 0.9393 . . plot_results(my_nn) . While overall we achieved better accuracy (around 94%), we can start to see that after epoch ~175 the accuracy flattens out without significant improvement. This is a sign that now are model is just overfitting or that our learning rate is too coarse to fine-tune the model above ~94% accuracy. We&#39;ll see this issue a lot clearer if we plot the results again starting from epochs #100 onwards. . import matplotlib.pyplot as plt x = range(len(my_nn.accuracy_scores)) y1 = my_nn.accuracy_scores y2 = my_nn.training_losses fig, ax1 = plt.subplots() ax2 = ax1.twinx() ax1.plot(x[100:], y1[100:], &#39;g-&#39;) ax2.plot(x[100:], y2[100:], &#39;b-&#39;) ax1.set_xlabel(&#39;Epochs&#39;) ax1.set_ylabel(&#39;Validation Accuracy&#39;, color=&#39;g&#39;) ax2.set_ylabel(&#39;Training Loss&#39;, color=&#39;b&#39;) plt.title(f&quot;Results with lr={my_nn.lr}, from epoch 100&quot;) plt.show() . . Lastly, let&#39;s also check what happens if we train with lower learning rates (0.005 and 0.01) and train for even longer (500 epochs). Given the large number of outputs I will omit the code snippets, but you get the hand of it by now! . . In the case of lr=0.005, after 500 epochs final accuracy was around 88%, and the loss around 0.39 due to the low steps. The trends of both lines suggest it could do with more training. . . For lr=0.01, we were able to achieve better accuracy (91%) but the loss function still hadn&#39;t completely plateaued (roughly around 0.30). Let&#39;s try again with a larger learning rate of 0.1 (10x bigger!) . . With lr=0.01 were able to reach 93% accuracy and get the loss down to 0.18! but as you can see, the larger the learning rate, the rougher our learning curve (in terms of accuracy). This is because our parameters jump around much more. Let&#39;s choose a learning curve inbetween 0.01 and 0.1 (i.e. 0.05) and train for 1000 epochs! . . With lr=0.05, after 1000 epochs we ended up with around 94.5% accuracy. However we already reached 94.5% accuracy back in epochs 700-800. When you train for an arbitrary number of epochs it could be that depending on your chosen configuration, your model reaches an optimal accuracy somewhere in the middle of the training. This is why many frameworks implement an option called early stopping rounds whereby you can instruct the model to stop training when accuracy hasn&#39;t improved after $n$ rounds. From the image above it seems like there isn&#39;t much more room to grow as the curves flatten out after epochs 750 or so. However, if we zoom in on our graph, while rickety, we can see there&#39;s still a clear trend on both metrics and could be worth fitting for a second cycle, perhaps with a lower learning rate. . . Layers . I can already hear you asking: &quot;ok, I got the point about learning rates and epochs, but what about the depth?&quot;. Well, let&#39;s find out! Let&#39;s add a hidden layer with 20 input parameters and 20 output parameters. . Let&#39;s train it with a learning rate of 0.05 and let&#39;s leave it running for a long time (2000 epochs), to see how it evolves. . my_nn = DeepClassifier( LinearModel(28*28, 20), LinearModel(20, 20), LinearModel(20, 10) ) my_nn.fit( train_dl=train_dl, valid_dl=valid_dl, lr=0.05, epochs=2000 ) . . First of all, let&#39;s pat ourselves on the back as we just ran our first real deep neural network with a hidden layer! From the graph above we can see our model&#39;s performance. It reached 95% accuracy and a loss of around 0.127. Here&#39;s a better look at the graph from epochs 1000 to 2000. . . Note that we could add as many layers as we wanted, but remember the tradeoff between complexity and training time and over-fitting! I encourage you to try experimenting with different layers and tweak our hyper-parameters accordingly. . Hopefully all these experiments show that machine learning is a craft of delicately balancing different constraints and making trade-offs, with only loose guidelines to guide our decision making! It is definitely not an exact science with strict, clear cut rules. There are so many hyper-parameters to take into consideration: batch sizes, learning rates, epochs, dataset size, layer count, parameter count, and variables to account for (quality of data, computing resources, etc). And that&#39;s not including other important decisions such as what type of loss function to use (spoiler alert, cross-entropy loss is only one of many!), what activation function to use, etc. This is is why machine learning is a field where there&#39;s plenty of room for experimentation and is full of unexplored territory. . Conclusions . This marks the end of this series of building a Deep Neural Network from scratch. I realize these are two very long posts, but rather than split them up for the sake of splitting them up, I decided to condense everything into 2 parts so it&#39;s easier to keep track of links and follow along step by step. As linked above, the final DeepClassifier code is available here. Follow the steps in the series and experiment with all the possible parameter choices! . I really hope you enjoyed the series and learned something along the way. If you want to reach out, feel free to find my email in the &quot;About&quot; page or leave a comment below. With that, I&#39;ll close and wish you all the best in your machine learning journey! . Aknowledgements . This series could not have been possible without the incredible resource that the fast.ai course is! I highly recommend you check it out as you will learn most of what&#39;s covered here and more! Most of what we covered in this series is part of Lessons 4 and 5 of the 2020 course. A big thank you to Jeremy and the fast.ai team for creating such a wonderful and inspiring course. .",
            "url": "https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html",
            "relUrl": "/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Implementing a Deep Neural Network from Scratch - Part 1",
            "content": "Introduction . One of the best ways to truly understand an algorithm, an API, or really any practical concept, is to build it yourself. In this blog post, we will implement a Deep Neural Network Multi-Label Text Classifier from scratch using Python, debunking the impression that modern Machine Learning is an obscure black box full of mind-bendingly complicated math. . Our goal will be to create a Neural Network that can distinguish all 10 handwritten digits (using the industry standard MNIST dataset). This is also known as a Multi-Label Classifier (a single-label classifier only has two labels, such as cat or dog, hotdog or not hotdog, etc). I&#39;ll be basing this post on Lesson 4 of the excellent fast.ai tutorial on Deep Learning, where Jeremy Howard shows how to distinguish whether a digit is a 3 or a 7 (a single label classifier). We&#39;ll need to change some things to make it work for all ten digits (0-9). . To truly test your understanding, I encourage you to try this yourself with a different type of dataset. . Terminology . Most concepts that appear complicated are actually quite simple once you look at its building blocks. Complexity often arises from an aggregation of simplicity, and with Neural Networks this is exactly the case. The foundation of a Neural Network is collection of neurons. A neuron takes in a value, does a mathematical operation and outputs another value. That&#39;s it. The reason Neural Networks are powerful, is in the how: how the neurons work together, and how the output values in each neuron are calculated. . If you remember from high school math, one of the simplest equations is that of a linear function (i.e. the equation of a straight line): . $y = textbf{a}x + textbf{b}$ . $ textbf{a}$ and $ textbf{b}$ are constants (i.e. numbers like 2 or 3). For any value $x$ that we feed in, $ textbf{a}$ influences that slope (or gradient) of the line, $ textbf{b}$ determines the offset of the line along the $y$ axis. . . Here is an interactive example where you can see how $x$ is affected by changes in $ textbf{a}$ and $ textbf{b}$. . Guess what: each neuron in a Neural Network is a linear function almost identical to the above. The main differences lie in how we call each parameter and how the values of the function are affected once we start connecting neurons together. This is why we refer to the neurons as &quot;linear units&quot;. From here on out, we&#39;ll call them linear units or LU(s). . The function for a linear unit is equivalent to the one we saw earlier: . $y = wx + b$ . Almost identical right? The only difference is that $a$ turned into a $w$. This is because at the heart of a linear unit, the input is affected by a weight $w$ and a bias $b$. Here&#39;s another way to look at it, visually: . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G bias bias neuron aka L.U. neuron aka L.U. bias&#45;&gt;neuron aka L.U. output output neuron aka L.U.&#45;&gt;output input input weight weight input&#45;&gt;weight weight&#45;&gt;neuron aka L.U. Why so much fuss about Neural Networks? Well, the power of neural networks is that theoretically they are able to approximate any function, at any level of accuracy. This is called the Universal Approximation Theorem. If you&#39;re interested in diving deeper into how this works, I recommend reading this post that visually (and interactively) shows how Neural Networks are able to approximate any function. . We&#39;ve made some bold claims: something that can replicate any known function with any level of theoretical accuracy...and just thanks to a simple linear function? Well, yes! And no. As mentioned ealier, the real magic of a Neural Network lies not in how each neuron (linear unit) behaves independently, but how they behave as a whole. We&#39;ll see how this works as we build our Neural Network. . By the way, if you need a little more context on the topics covered above, I highly recommend the Kaggle course on the subject. . Getting Started . Now that we&#39;ve understood what lies at the foundation of a Neural Network, we can start implementing it. Let&#39;s begin by importing and taking a look at the data we will be using. . !pip install -Uq fastbook from fastbook import * # Download the MNIST dataset using fast.ai&#39;s convenient function path = untar_data(URLs.MNIST) . Our data is made up of two directories: training/ and testing/. In each folder, the PNGs of the digits (0 to 9) are grouped into folders themselves. The training dataset has a total of 60,000 reference images and the testing a total of 10,000 (so roughly a 15-16% validation split). . mnist_png/ (70,000 files) ├── training/ (60,000 files) │ ├── 0/ │ │ ├── 44897.png │ │ ├── 28230.png │ │ └── ... │ ├── 1/ │ │ ├── 44680.png │ │ ├── 37659.png │ │ └── ... │ ├── ... │ └── 9/ └── testing/ (10,000 files) └── ... (same structure as training/) . The reason we need the two folders is that we&#39;ll &quot;learn&quot; how to recognize the digits using the larger training set, and measure our performance accuracy against the testing dataset. If we were to test our performance on the dataset we learn from, we would always inevitably get 100% accuracy as the model learns to distinguish individual files. This sounds great, but the reality is it would perform terribly in a production environment when it encounters images of digits it has never seen before. . training = { f&#39;{num}&#39; : (path/f&#39;training/{num}&#39;).ls().sorted() for num in range(10) } testing = { f&#39;{num}&#39; : (path/f&#39;testing/{num}&#39;).ls().sorted() for num in range(10) } . zero_path = training[&#39;0&#39;][0] zero = Image.open(zero_path) zero . In order to work with our images, we need to convert this image into a numerical representation. Usually, a 2D image is represented by a matrix array (i.e. a Python list, a Javascript array, etc). There are a couple Python libraries that have extended the standard list object to be a bit more flexible (e.g. a numpy array). The object we&#39;ll be using is a PyTorch tensor. PyTorch is a popular Deep Learning Python Library. While tensor may sound like a fancy name, it&#39;s nothing more than an array that has a couple extra features. From here on out, when we refer to a tensor, we mean an array. Let&#39;s convert our zero to a tensor and visualize it numerically: . tensor(zero) . . You don&#39;t have to be Neo (from The Matrix) to make out the zero above made up of numbers. This is because each value of the array represents a pixel value, in the case of grey scale, from 0 to 255 (from pure black to pure white). . Given that this is a 2D matrix, we can access specific parts of the tensor using notation such as zero[5:10,5:10] which will output the rows from index 5 (included) to 10 (excluded) and the same for the columns. . z_t = tensor(zero) z_t[6:10,10:15] # This will output rows 7-10 (index 6-9) and columns 11-15 (index 10-14) . tensor([[ 0, 0, 0, 54, 227], [ 0, 10, 60, 224, 252], [ 0, 163, 252, 252, 252], [ 51, 238, 253, 253, 190]], dtype=torch.uint8) . Let&#39;s visualize the tensor a little better: . df = pd.DataFrame(z_t) df.style.set_properties(**{&#39;font-size&#39;:&#39;5pt&#39;}).background_gradient(&#39;Greys&#39;) . . This is the anatomy of our digits: 28 rows long by 28 columns wide, making a total of 784 pixels each. Each pixel is represented by a value from 0-255. . Before we start, let&#39;s first ponder on how we could, without knowing anything but the structure of the data, teach a computer how to differentiate between the digits. We&#39;ll do that in the next section, and use it as a baseline to measure our Neural Network against. . Establishing a Baseline . We&#39;ll begin our implementation by implementing something that isn&#39;t a neural network. It sounds counter-intuitive, but in order to get a good understanding of the performance of our model, we should always have a simple baseline with which to compare it against. Usually a good baseline is something simple, easy to create, and easy to understand. Seeing as how we&#39;re building everything from scratch, we may as well build our baseline from scratch as well. . In our case, there are several ways to approach this problem without knowing a single thing about machine learning. One approach could be just comparing the pixel similarity of the digits. This can be done by counting the average number of non-0 pixels for each image, or measuring the average pixel value of each image, or the average sum of pixel values, etc). Let&#39;s pick comparing average pixel values as the &quot;algorithm&quot; for our baseline algorithm. . Average Pixel Value . To calculate average pixel value, we will take the average value of each pixel coordinate across every respective digit&#39;s training image&#39;s tensor. For example, we take all the &#39;5&#39; image tensors, we look at the first coordinate (1,1) in each tensor and we take the average value. We repeat the process for each coordinate until we create a map of average values across the 28x28 grid of pixels. An easy way to do this is to create a tensor that &quot;houses&quot; all of the individual image tensors. This will take our tensor from 2 dimensions, to 3 (also called a rank-3 tensor). This allows us to work with all of the tensors as a single stack, and conduct mathematical operations such as tensor.mean() without looping. We&#39;ll see later why that&#39;s not just convenient, but very important for performance. . Going into the third dimenion sounds complicated, but all we&#39;re really doing is nesting each of the digit&#39;s various PNGs&#39; tensors into a larger tensor. We do this with PyTorch&#39;s stack method which does just that. While we&#39;re at it, we&#39;ll also normalize our pixel values to be from 0 to 1 by dividing by 255 after converting them to float. . # For each digit from 0 to 9, we create a stacked tensor # containing the tensors for each PNG image, after converting # each pixel value to a float and dividing it by 255. training_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in training[f&#39;{num}&#39;] ]) for num in range(10) ] # Preview the shape of the stacked tensors of the &quot;zero&quot; digit PNGs training_tensors[0].shape . torch.Size([5923, 28, 28]) . We now have a list of stacked tensors (i.e. a list of stacked tensors of each digit). Each stacked tensor has $n$ tensors for each PNG image (rows), and each row contains a 28x28 tensor representing that PNG&#39;s pixel values normalized from 0 to 1. . We can check this by inspecting the first item in our list of training_tensors: the stacked tensor of &quot;zero&quot; PNGs has shape torch.Size([5923, 28, 28]): because it has 5923 rows (equal to the number of training images), each containing a 28x28 tensor (equal to the pixels in each image). If you&#39;re having trouble picturing a 3D tensor in your head, think of it like an array of array of arrays (or a list of list of lists in Python). . Calculating Average Pixel values . The convenient part of having used tensors, is that now we can quickly calculate the mean across a desired dimension (in our case: flatten each stack into a single 28x28 tensor containing the average values of each pixel in the stack). . Tip: Think of it like printing out all the PNG images for a specific digit and stacking each page one on top of the other. That&#8217;s exactly what we did with out stacked tensor. And now, with special X-Ray vision, we are going to reduce the stack to a single page by taking the average value for each printed pixel across the stack of pages. . training_means = [t.mean(0) for t in training_tensors] # Let&#39;s display the &quot;mean&quot; tensor of the digits f, axarr = plt.subplots(2,5) digit = 0 for i in range(2): for j in range(5): axarr[i,j].imshow(training_means[digit], cmap=&quot;gray&quot;) digit += 1 . Pretty cool right? That is the &quot;average&quot; pixel value for each digit, representing in a way the ideal digit according to our baseline algorithm that will compare the validation digits against this mean. The value we will be measuring is called distance. Our validation digits will of course be very different from these reference means, but hopefully the pixel values will be nearest in distance to the mean pixel value of the correct digit! Let&#39;s see if that&#39;s the case... . Measuring Distance . In order to measure the distance between our reference means and the validation digits, we need to subtract them. However we can&#39;t simply subtract them, as that would create negative numbers and our pixel values go from 0 to 1. . To avoid dealing with negative numbers, there are two common measures of distance that get rid of negative numbers altogether: . mean absolute difference (aka L1 Norm) | root mean squared error (aka RMSE or L2 Norm). | . If these sound confusing you&#39;ll see how simple they are in just a second. . Mean Absolute Difference: $mean( abs( a - b ) )$. For each prediction, we subtract our prediction from the actual value, turn that difference into a positive number (i.e. remove the negative sign if there is one) and then average all the these differences together together. . Root Mean Squared Error / RMSE: $ sqrt[]{mean((a-b)^2)}$. For each prediction, we subtract our prediction from the actual value, square the result (so as to turn it into a positive number) and then take the square root, so as to &quot;cancel out&quot; the squaring. . In code, they are even easier: . def mean_absolute_difference(a, b): return (a - b).abs().mean() def rmse(a, b): return ((a - b)**2).mean().sqrt() . Now that we&#39;ve defined them, let&#39;s pick a random image from the validation set (e.g. the first image in the &#39;5&#39; in the testing list) and calculate its distance from the training set&#39;s reference mean. . val_5 = tensor(Image.open(testing[&#39;5&#39;][0])).float()/255 # val_5.shape =&gt; torch.Size([28, 28]) # Let&#39;s compare it to the reference mean of 5 mean_absolute_difference(training_means[5], val_5), rmse(training_means[5], val_5) . (tensor(0.1683), tensor(0.2988)) . mad_all = [mean_absolute_difference(training_means[num], val_5) for num in range(10)] rmse_all = [rmse(training_means[num], val_5) for num in range(10)] . Remember, we want the lowest distance (either measured via Mean Absolute Difference, or RMSE) to be the one between our PNG and the digit&#39;s &quot;reference mean&quot; (i.e. the mean we calculated representing the average of all the training digits of that type). . We can already see that for this particular 5 in our dataset, our distance measures are actually lower for other numbers (e.g. 3, 8). This isn&#39;t desired, but also isn&#39;t too surprising, as the handwritten digits are visually very similar. Let&#39;s see if this holds up on average for all the 5&#39;s, and whether our algorithm will perform better for other digits. . We&#39;ll continue by loading all of our validation images: . validation_tensors = [ torch.stack([ tensor(Image.open(digit)).float()/255 for digit in testing[f&#39;{num}&#39;] ]) for num in range(10) ] # Preview the shape of the stacked validation tensors of the &quot;zero&quot; digit validation_tensors[0].shape . torch.Size([980, 28, 28]) . We need to write a function that takes each tensor stack in our validation_tensors list (one for each digit), and for each tensor in the stack measures the distance from the corresponding mean. . Thanks to PyTorch&#39;s efficient use of broadcasting, we can feed in a stack as $a$ and a single tensor as $b$ (or viceversa) and PyTorch will automatically stack the single tensor multiple times to match the length of the stack. This allows the computation to be done using low-level C on the GPU (i.e. in parallel) and is several orders of magnitude faster than a regular Python for loop. Thankfully, our distance function doesn&#39;t need to change a whole lot, the only change being to add along which axes to calculate the mean in our tensor stack (see commented code below). We&#39;ll need another couple of functions to help us deal with multiple digits and multiple tensor stacks and aggregate all the scores. . You can read the source code below, but here&#39;s a quick explanation: for each stack of tensors (e.g. tensors of all of the &#39;3&#39; images) in our validation set, we calculate the distance from each digit&#39;s reference mean and check that the lowest distance is that of the correct digit&#39;s reference mean. . def distance(a, b): # Let&#39;s go with mean absolute error for now return (a - b).abs().mean((-1,-2)) # the last two axes (e.g. 28x28) def is_correct(tensors, means, correct_index): # make a list of the wrong digits&#39; means indices wrong_means = [i for i in range(10)] wrong_means.pop(correct_index) # calculate the distance from the correct reference mean correct_distance = distance(tensors, means[correct_index]) # calculate and compare the distance to each wrong digit&#39;s reference mean # and then checking that the distance is the lowest across all digits. # each &#39;wdX&#39; contains a tensor of Booleans -- eg tensor([True, False, ... ]) wd1 = correct_distance &lt; distance(tensors, means[wrong_means[0]]) wd2 = correct_distance &lt; distance(tensors, means[wrong_means[1]]) wd3 = correct_distance &lt; distance(tensors, means[wrong_means[2]]) wd4 = correct_distance &lt; distance(tensors, means[wrong_means[3]]) wd5 = correct_distance &lt; distance(tensors, means[wrong_means[4]]) wd6 = correct_distance &lt; distance(tensors, means[wrong_means[5]]) wd7 = correct_distance &lt; distance(tensors, means[wrong_means[6]]) wd8 = correct_distance &lt; distance(tensors, means[wrong_means[7]]) wd9 = correct_distance &lt; distance(tensors, means[wrong_means[8]]) # now we &#39;and&#39; all the Boolean tensors together wdf = torch.bitwise_and(wd1, torch.bitwise_and(wd2, torch.bitwise_and(wd3, torch.bitwise_and(wd4, torch.bitwise_and(wd5, torch.bitwise_and(wd6, torch.bitwise_and(wd7, torch.bitwise_and(wd8, wd9)))))))) return wdf.float().mean() def accuracy(stacks, means): accuracy_map = [is_correct(stacks[i], means, i) for i in range(10)] accuracy_tot = (sum(accuracy_map) / len(accuracy_map)).item() return accuracy_tot, accuracy_map . tot, map = accuracy(validation_tensors, training_means) print(&quot;Tot. Accuracy: {:.1%}&quot;.format(tot)) print(&quot; n&quot;.join([&quot;Digit {} : {:.1%}&quot;.format(i, map[i]) for i in range(10)])) # Plot results for a visual inspection plt.title(&quot;Baseline Performance Across Digits&quot;) plt.axhline(tot, color=&#39;red&#39;, linewidth=2) plt.xticks(range(0, 10)) plt.xlabel(&#39;Digits&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.bar(range(len(map)), map, align=&#39;center&#39;) plt.plot(tot) plt.show() . Tot. Accuracy: 66.1% Digit 0 : 81.5% Digit 1 : 99.8% Digit 2 : 42.3% Digit 3 : 60.9% Digit 4 : 66.8% Digit 5 : 32.6% Digit 6 : 78.7% Digit 7 : 76.5% Digit 8 : 44.3% Digit 9 : 77.6% . It seems our simple mathematical comparison of average pixel values already achieves 66.1% overall accuracy across the validation set! This is impressive considering some digits have similar strokes (e.g. 3, 5 and 8). You can see from the bar chart above, some digits perform better than others, with the &#39;1&#39; digit recognition achieving near perfect accuracy (99.8%) in stark contrast to the &#39;5&#39; digit (32.6%). . So we&#39;ve covered terminology, we&#39;ve established a simple baseline based on pixel arithmetic. Let&#39;s start building this darn deep neural network! . Thinking Like a Neural Network . While &quot;Deep Learning&quot; sounds sophisticated, the underlying learning process under the hood doesn&#39;t sound quite as futuristic: &quot;try, rinse, repeat&quot;. However, the real magic lies in the details, more specifically in how it &quot;rinses&quot; each cycle. After every cycle, it will adjust its own parameters to be &quot;better&quot;. We&#39;ll see how it does this in just a second. Now let&#39;s reflect on our simple baseline method. . Our baseline approach uses a very deterministic, binary algorithm for deciding whether a digit is correct or not: it will compare pixel distance with each reference mean and pick the mean where the distance is lowest. Considering that the input pixels and reference pixels never change, for each image our model will either be correct 100% of the time, or it will fail 100% of the time. There is no feedback loop, no way for the model to adjust along the way and no &quot;randomness&quot;. . Remember that our Linear Units have parameters. More specifically, each Linear Unit has a weight and a bias: . $y = textbf{w}x + textbf{b}$ . The beauty of parameters is that they can be tweaked, and the output $y$ will be different, much like we saw in our linear equation at the beginning of the post. . With this simple equation as the engine for its neurons, a Neural Network is able to approximate anything. The approximation process is possible due to the iterative nature of trying a set of parameters, adjusting them and trying again. But how, you may be wondering, does a model choose the new set of parameters? Is it a random choice? Is it decided by a function? The answer is an inclusive &quot;yes&quot;: it starts off with random values and gradually adjusts them according to a function: the loss function. The loss function measures how accurate the prediction is. The optimizer function then updates the parameters so as to minimize the loss function. It does this by looking at the rate of change (the gradient, aka slope) of each parameter as it was fed into the loss function and updates them (also called &quot;to step&quot;) in the direction that will minimize the loss function. And this process has a fancy name, called Stochastic Gradient Descent. . Descent because we want to get to make our way to the lower end of the loss function, | Gradient because we use the gradient of each parameter to find in what direction to update them to make that descent, and | Stochastic because it involves a litte bit of randomness, meaning it&#39;s not deterministic. | . Let&#39;s summarize all these steps again: . We initialize the parameters (weights and biases for each input) to random values | Make a prediction using those parameters | Measure the accuracy of the prediction using a loss function | We calculate how to update our parameters so as to minimize the loss function (remember: lower is better) using the gradients of the parameters&#39; change as a result of the loss function. This is done via the optimizer function. | We update our parameters and predict again, repeating the cycle until we are satisfied or the model stops improving significantly. | &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G initialize initialize predict predict initialize&#45;&gt;predict measure loss measure loss predict&#45;&gt;measure loss step based on gradient step based on gradient measure loss&#45;&gt;step based on gradient step based on gradient&#45;&gt;predict repeat stop stop step based on gradient&#45;&gt;stop Let&#39;s begin by implementing something that follows these rules. The reason I call it something is that you&#39;ll see later why it&#39;s not exactly a Deep Neural Network. But it will help us learn along the way and take it step by step. . Primordial Neural Network . Our first Neural Network will start with a parameter for each pixel value of the input image ($28 times 28 = 784$), and will be fed directly into our loss function to measure our prediction accuracy. That means that our output needs to be a list of 10 probabilities--each probability corresponding to the likelihood that our reference image corresponds to a specific digit. These probabilities will need to sum to 1, as the classification is mutually exclusive (i.e. a &#39;3&#39; can only be a &#39;3&#39;). Other multi-label classification approaches are inclusive, meaning each input could have multiple labels (e.g. deciding if an image contains a person and/or a car and/or any other object). . How do we condense all of our parameters into a list of probabilities? Say hello to my little friend: Softmax. . The Softmax Function . The beautify of the Softmax function is that it normalizes a group of values from 0 to 1 in a way so that each value is interrelated with the others: together they sum to 1. This is perfect for generating a list of interrelated probabilities. If we were creating non-mutually exclusive classification, we would use Softmax&#39;s cousin Sigmoid, that normalizes each value from 0 to 1 independently. I&#39;ll leave diving deeper on the topic of Softmax vs Sigmoid as homework--you can start with this brilliant blog post. . Let&#39;s admire Softmax in all of its glory: . $ text{Softmax}(x_i) = frac{e^{x_i}}{ sum_j{e^{x_j}}}$ . Don&#39;t worry, it&#39;s a lot simpler than it looks, and is even simpler once we code it. Here&#39;s a quick explanation of how our Softmax works in three steps: . Given a list of values, we calculate the exponential for each value, that is: $e^{n}$. Where $n$ is our value and $e$ is Euler&#39;s number, a special number in Mathematics, and at the heart of the exponential function. | We sum the exponential values (the Sigma symbol $ sum$ means sum). | Take each value in Step 1. and divide it by the sum obtained in Step 2. | As Jeremy Howard says: a mathematical concept often sounds complicated until you code it. So let&#39;s code it. Below is my quick softmax implementation: . def softmax(v): &quot;&quot;&quot; Given an input vector v, returns an output vector containing the softmax values for each element in the vector &quot;&quot;&quot; exp_v = torch.exp(v.float()) exp_sum = exp_v.sum() return exp_v / exp_sum . softmax(tensor([1,2,3])) . tensor([0.0900, 0.2447, 0.6652]) . Datasets . Now that we have Softmax in our toolbelt, we can move on with the implementation. Let&#39;s start by preparing our data. We&#39;ll take our tensor stacks. Let&#39;s reduce the dimensions of our data by transforming each image tensor from a matrix of 28x28 to a vector (a list) of size 28*28 (meaning each pixel is in a series, not in a 2D coordinate space) . train_x = torch.cat(training_tensors).view(-1, 28*28) valid_x = torch.cat(validation_tensors).view(-1, 28*28) . train_x.shape, valid_x.shape . (torch.Size([60000, 784]), torch.Size([10000, 784])) . As you can see from the output of train_x.shape, we went from a rank-3 tensor (60000, 28, 28) to a rank-2 tensor (60000, 784). We now want as many outputs from our model as number of labels (in our case digits). So we need to flag for each training image which is the correct digit. To do this we&#39;ll populate a tensor of length 10 as our corresponding target labels (also referred to as $y$ variables. In that case our inputs are usually referred to as $X$, representing our features) and add a &quot;1&quot; to flag the corresponding index in the tensor corresponding to our digit. You can see what it looks like for each digit below. . Each row represents the correct label representation for each digit. For example, the first row is the output labels valid for all the &#39;zero&#39; tensors (index 0 is set to 1, and the rest are 0), the second being the output labels for all the &#39;one&#39; tensors (index 1 is set to 1 and the rest are 0), etc. In a way, the 1&#39;s location inside the indicates 100% probability that that index represents the correct digit. Ideally, we want our model to output the exact same output. . # Every digit&#39;s tensor stack will have a label # with the corresponding index flagged as &#39;1&#39;. [[0]*i + [1] + [0]*(9-i) for i in range(10)] . [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]] . Let&#39;s associate a target label for each tensor in our stack, with the &#39;1&#39; corresponding to the digit it represents. . train_y = torch.from_numpy(np.concatenate([[[0]*i + [1] + [0]*(9-i)]*len(training[f&#39;{i}&#39;]) for i in range(10)])) valid_y = torch.from_numpy(np.concatenate([[[0]*i + [1] + [0]*(9-i)]*len(testing[f&#39;{i}&#39;]) for i in range(10)])) . train_y.shape, valid_y.shape, train_y[0] . (torch.Size([60000, 10]), torch.Size([10000, 10]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])) . Let&#39;s now merge them into a dataset. A dataset contains both $X$ and $y$ variables so that the model knows what $y$ to compare the prediction of $X$ against. We&#39;ll populate a training dataset with which to train the parameters, and a validation dataset that the parameters will never &quot;learn&quot; from, with which to benchmark our training against. A model may be very good at learning the peculiarities of its training data but may end up performing very poorly on its validation data as the features learned are not generalizable. This phenomenon is referred to as over-fitting. . # into datasets by zipping them so as to create a list of tuples (image tensor vector, label ) dset = list(zip(train_x, train_y)) dset_valid = list(zip(valid_x, valid_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])) . Parameters . What we&#39;ll do now is start defining functions that we&#39;ll end up merging into a single model class. The first function we&#39;ll need is a function that simply initiates random numbers, and these random numbers will be our weights and biases for each pixel. . We&#39;ll use the .requires_grad_() in-place operation to ask PyTorch to track the gradients, as we&#39;ll need them later to backtrack and update our parameters according to the loss function. . def init_params(sizeD1, sizeD2=0): if sizeD2 == 0: return (torch.randn(sizeD1)).requires_grad_() else: return (torch.randn(sizeD1, sizeD2)).requires_grad_() . weights = init_params(28*28, 10) # the &#39;w&#39; of the linear equation bias = init_params(10) # the &#39;b&#39; of the linear equation . weights.shape, bias.shape . (torch.Size([784, 10]), torch.Size([10])) . Linear Equation . Next we need a function that performs the famous Linear Unit (neuron) operation that we&#39;ve been talking about since the beginning of the post. It&#39;s as simple as: . def linear_eq(x): return x@weights + bias . Next we&#39;ll feed our training data train_x into our linear equation, which will associate the parameters we created earlier via matrix multiplication. That&#39;s what the @ stands for in our function above. This gives us a first set of predictions (preds) that should perform quite poorly, more or less as well as picking the digits at random. . preds = linear_eq(train_x) preds . tensor([[-11.9867, -4.2706, 4.6806, ..., -1.3517, -0.0395, 13.5840], [ -6.6797, -4.4483, 1.4827, ..., -8.9409, 2.2570, 10.0881], [-22.7994, 2.2295, 11.1160, ..., -1.8488, 25.5523, 22.3240], ..., [-16.7386, 4.0141, 3.9273, ..., 1.5960, 9.7961, 12.3801], [-14.1562, -3.5679, 1.7541, ..., 1.6506, 16.5308, 10.3213], [-11.3688, 4.5965, 5.7170, ..., 10.0353, 4.4967, 9.5438]], grad_fn=&lt;AddBackward0&gt;) . _, max_indices = preds.max(-1) # get the index of max value along 2nd dimension _, tag_indices = train_y.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match corrects.float().mean() # calculate mean . tensor(0.1204) . We see here that randomly initialized parameters will perform more or less like...well...a random choice. In our test above we got a 12% performance rate versus a random chance of 10% (choosing a random number from 1 to 10). This is expected and a good sense check that our model parameters are initialized randomly. Now comes the fun part. Let&#39;s create a loss function against which it can measure itself. After that, we&#39;ll see how can create an optimizer function that will decide how to update the parameters to minimize the loss function, so as to continuously improve cycle after cycle. . We&#39;ll first softmax our parameters so that they are between 0 and 1, and we&#39;ll check whether the outputs match the targets. . # so we can work in multiple dimensions easily. # It works exactly like our own softmax() def loss_function(predictions, targets): sm = nn.Softmax(dim=-1) # instantiate PyTorch&#39;s softmax in the 2nd dimension predictions = sm(predictions) # calculate the softmax across the 2nd dimension return torch.where(targets==1, 1-predictions, predictions).mean(-1) . loss_function(preds, train_y) . torch.Size([60000, 10]) . tensor([0.2000, 0.2000, 0.2000, ..., 0.0141, 0.1996, 0.1299], grad_fn=&lt;MeanBackward1&gt;) . The Optimizer . Once our model instantiates random parameter values, makes a prediction and measures the first prediction against the loss function, we now have &quot;report card&quot; for how it performed. Staying with that analogy, now is the time to read our report card (our &quot;grades&quot; are the distances from the target value) and fix our parameters so we get a better grade the next time around, using the teacher&#39;s feedback (our gradients). . We first need to get our gradients, and then update our parameters using a simple formula in Stochastic Gradient Descent. This is what the formula looks like for each parameter $w$: . $ w := w - eta nabla Q({w}) $ . As usual, this looks super complicated until you see it in code: . loss = loss_function(preds, yb) # Get the gradients loss.mean().backward() # Optimize our parameters for p in [weights, bias]: p.data -= p.grad*lr # this line is the formula above p.grad.zero_() . All we&#39;re doing is just subtracting the gradient! To get the gradient, p.grad, we need to use PyTorch&#39;s .backward() functionality on the output of our loss_function, $Q$, so it can calculate the gradients for all the parameters in our weight and bias tensors as a result of the loss function. This is done automatically thanks to the fact that we added .requires_grad_() earlier to our parameters. . If you have a keen eye, you&#39;ll see in the code that we muliply the p.grad by a value called lr. This is the learning rate, $ eta$ in the formula. This is just a weighing factor we use to reduce the amount of movement along the loss function from one update to the next. Typical values range from 0.001 to 0.1. Why is it important to not go too much in one direction? If you imagine a loss function like a parabola, our optimal point is at the bottom of the parabola. If we overshoot, we could be bouncing around from one side of the parabola to the other. A smaller step-based approach helps prevent that. . Batches . Before we combine everything together into a model, let&#39;s introduce the concept of batches. Rather than measure predictions across all the dataset before estimating our performance, research has found that doing it with batches of training data is significantly faster and yields good results. This is also referred to as mini-batches. It&#39;s a compromise between having to run your function against all the values in the training dataset every time (therefore requiring a lot of resources when you have millions of items to process), or doing it for each single item in the dataset, which would be fast, but would not be representative of the group. Either extremes are bad for different reasons, so a good compromise is selecting a random batch of samples every time, and running the prediction-loss-step cycle on that batch thereby updating the parameters. A good batch size is small enough to be performant and large enough to be somewhat representative of the overall dataset. When we complete a round of batches (i.e. all of our samples are used in training exactly once), we have completed an epoch. . For batching, we&#39;ll use fast.ai&#39;s DataLoader. While it offers a lot of additional functionality, all we&#39;ll use it for is as an iterator that will split up our inputs into batches to feed into the model until all the samples are fed through. . dl = DataLoader(dset, batch_size=128, shuffle=True) valid_dl = DataLoader(dset_valid, batch_size=128, shuffle=True) . Our MNIST Model . Now that our dataset is conveniently split up into batches, we can combine all the different functionality we saw up until now and create an MNIST Model class to train. Below is a (naive and specific) implementation of a Model specifically trained to deal with the characteristics of our MNIST task. . Our class has the following characteristics (feel free to skip ahead and just read the source code directly) . Inputs . This is what the model needs to instantiate: . train_dl: our training DataLoader | valid_dl: our validation DataLoader | epochs: the number of epochs (complete rounds of the training data to perform) | lr: our learning rate | verbose: which just flags whether we want to print out feedback. | . Functions . These are the functions our model uses to perform the operations. They are prefixed with an _ to differentiate them from our methods. . _print(): is just a simple print function that only prints if flag verbose is true. | _init_params(): our initializes parameters. | _linear_eq(): our linear equation above. | _loss_function(): our loss function above. | _calc_grad(): takes the input training data, runs it through _linear_eq to get predictions and calculates the loss via the _loss_function. Then calls .backwards() on the loss results to get the gradients. | _batch_accuracy(): softmaxes a prediction and compares it against the corresponding label. That is, the index with the max value in the vector (highest probability) should be the same as the index with the &#39;1&#39; flag in our label vector. True if it&#39;s the case, False otherwise. Then returns the mean result, representing our batch accuracy. This is used in our _validate_epoch function and only used with validation data. | _validate_epoch(): once a training epoch is complete, we run our model through each batch of our validation data and cumulatively aggregate accuracies of each batch obtained via _batch_accuracy into an overall epoch score. | . Methods . Our model has two main methods: train and predict. . train: for each epoch, for each batch performs the training (via _calc_grad), the optimizing and validates each epoch (via _validate_epoch). | predict: similar to _batch_accuracy but only works on a single input image. It expects an image tensor as input and runs the tensor through its current parameters, outputting its predicted digit as well as the probabilities for other digits. | . class MNISTLinearRegression: def __init__(self, train_dl, valid_dl, epochs, lr, verbose): self.lr = lr self.train_dl = train_dl self.valid_dl = valid_dl self.epochs = epochs self.weights, self.bias = self._init_params() self.softmax = nn.Softmax(dim=-1) self.accuracy_scores = [] self.verbose = verbose def train(self): for i in range(self.epochs): for xb, yb in self.train_dl: self._calc_grad(xb, yb) for p in [self.weights, self.bias]: p.data -= p.grad*self.lr p.grad.zero_() self._validate_epoch(i) def predict(self, image_tensor): probabilities = self.softmax(self._linear_eq(image_tensor)) _, prediction = probabilities.max(-1) # Return digit and vector of probabilities return prediction, probabilities def _calc_grad(self, xb, yb): preds = self._linear_eq(xb) loss = self._loss_function(preds, yb) loss.mean().backward() def _batch_accuracy(self, xb, yb): predictions = self.softmax(xb) _, max_indices = xb.max(-1) # get the index of max value along 2nd dimension _, tag_indices = yb.max(-1) # get index of flag in our label tensors corrects = max_indices == tag_indices # check whether they match return corrects.float().mean() # calculate mean def _validate_epoch(self, i): accs = [self._batch_accuracy(self._linear_eq(xb), yb) for xb,yb in self.valid_dl] score = round(torch.stack(accs).mean().item(), 4) self.accuracy_scores.append(score) self._print(f&#39;Epoch #{i}&#39;, score) def _linear_eq(self, x): return x@self.weights + self.bias def _loss_function(self, predictions, targets): predictions = self.softmax(predictions) return torch.where(targets==1, 1-predictions, predictions).mean(-1) def _print(self, *args): if self.verbose: print(*args) # Linear regression using SGD def _init_params(*args): return (torch.randn(28*28, 10)).requires_grad_(), (torch.randn(10)).requires_grad_() . . Now that we&#39;ve created it, let&#39;s try it in action! . model = MNISTLinearRegression(dl, valid_dl, 50, 1, True) . model.train() . Epoch #0 0.225 Epoch #1 0.2896 Epoch #2 0.335 Epoch #3 0.3665 Epoch #4 0.4239 Epoch #5 0.4645 Epoch #6 0.486 Epoch #7 0.5055 Epoch #8 0.5214 Epoch #9 0.5243 Epoch #10 0.5343 Epoch #11 0.5356 Epoch #12 0.5389 Epoch #13 0.5466 Epoch #14 0.5463 Epoch #15 0.5468 Epoch #16 0.5506 Epoch #17 0.5522 Epoch #18 0.553 Epoch #19 0.5556 Epoch #20 0.5578 Epoch #21 0.5579 Epoch #22 0.5573 Epoch #23 0.5617 Epoch #24 0.5599 Epoch #25 0.5615 Epoch #26 0.5599 Epoch #27 0.5615 Epoch #28 0.5604 Epoch #29 0.5602 Epoch #30 0.5666 Epoch #31 0.5702 Epoch #32 0.5837 Epoch #33 0.6356 Epoch #34 0.6464 Epoch #35 0.6619 Epoch #36 0.677 Epoch #37 0.6907 Epoch #38 0.6961 Epoch #39 0.7083 Epoch #40 0.7107 Epoch #41 0.7188 Epoch #42 0.7252 Epoch #43 0.7276 Epoch #44 0.7353 Epoch #45 0.7479 Epoch #46 0.7646 Epoch #47 0.7808 Epoch #48 0.7929 Epoch #49 0.8005 . Not bad! After 50 epochs we reached 80% accuracy, way ahead of our baseline! I tried running this multiple times and the results vary, depending on the initial parameters and the learning rate. . Remember the example &#39;5&#39; we used earlier in the post to try out our baseline average pixel value approach? If you remember, it didn&#39;t even work properly as the distance to the reference means &#39;3&#39; and &#39;8&#39; was closer! Let&#39;s take that same image and see what our model predicts. . val_5 = (tensor(Image.open(testing[&#39;5&#39;][0])).float()/255).view(-1, 28*28) a = model.predict(val_5) a . (tensor([5]), tensor([[1.6423e-13, 7.6946e-22, 1.9228e-15, 2.0405e-04, 1.1598e-18, 9.9980e-01, 6.9507e-23, 2.2210e-20, 5.8148e-25, 1.8752e-18]], grad_fn=&lt;SoftmaxBackward&gt;)) . Yay. It works! It correctly predicted the &#39;5&#39; digit, with probability 99.98%. . Next Steps . You may have noticed that we called our model a &quot;MNISTLinearRegressor&quot;. I have to break it to you, what we&#39;ve done up to now is essentially create a linear regressor with a self-correcting capability through Stochastic Gradient Descent...not a deep neural network. . We&#39;ve also got some aspects of our current model we need to improve: . our loss function applies Softmax naively, which makes for a very &quot;hit or miss&quot; training performance depending on our initial parameters. The drawback of Softmax is it tends to be &quot;harsh&quot; on parameters by exasperating one over the rest. This is great when making final predictions and want 1 probability to shine through, but not when we want a nice stable loss function to help the model improve. We&#39;ll see how we can convert our naive loss function into an industry standard one, by softening Softmax (ironic, isn&#39;t it?). This will help get us better and more stable training accuracy in less than 4-5 epochs! | our model is also quite &quot;rigid&quot; in the sense that it&#39;s only built for MNIST. We&#39;ll generalize it into a general Linear Model, that accepts any number of labels in a much more elegant way! | Lastly, a neural network is non-linear. How do we break this linearity and make our model orders of magnitude more powerful? | . All of this coming up in Part 2! . Part 2 .",
            "url": "https://muttoni.github.io/blog/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html",
            "relUrl": "/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Understanding DataBlocks and DataLoaders in fast.ai",
            "content": "Introduction . Coming from SciKit and TensorFlow, when I first started working with PyTorch and fast.ai I quickly realized they have a very opinionated (but convenient!) way to deal with data, through the DataBlocks and DataLoaders APIs. In this post we will quickly go over what they are (you can check the official documentation if you want to dive a little deeper), and understand how they work together. If you are not 100% clear about the difference between a Datablock and a DataLoader, this blog post hopefully will shed some light. . DataBlock . A Data block is nothing more than a pipeline for data assembly. When you initially create a DataBlock, you won’t need to specify any data. What you will need to specify, however, is a set of rules for how to treat your data when it does flow in. It doesn’t care about what you’ll do with it, it just cares about how you want it gathered, classified and split. . In order to create a Data block you need to specify . what types of data to expect for your input (aka features) and target variables (aka labels) | how to get the data | how to differentiate features from the target variables, | how to split the data for training (train &amp; validation set) | Let’s see how to do that. Below is an example DataBlock that we would create if we were looking to create a Convolutional Neural Network (CNN) to recognize different species of cats (i.e. a classification model). . cats = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . The four main steps mentioned above are exactly the first four (required) arguments of a DataBlock: . blocks: is where you define the types of data your model will work with. Usually you will specify at least two blocks: one that represents your independent (input) variable, and one that represents your dependent (target) variable. You can also specify multiple input/output variables. | get_items: a function that will actually go and pick up the data when necessary (more on this later) | splitter: how to split up the data in a training and validation set. The seed is optional and only added for replicability | get_y: how to extract the target (dependent) variable from the data. In the case of our cat classifier, this will be by looking at the parent folder, and fast.ai provides a built in function called parent_label.. | item_tfms is an optional argument that we can include to specify any additional processing that needs to be carried out when we flow our data through. In this case, we will resize all images to 128x128. We can specify other transforms, such as item_tfms=Resize(128, ResizeMethod.Squish)) which will resize and squish our images to fit, or item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;) to resize and pad any leftover space with black. This method is incredibly powerful as it also supports data augmentation. This is beyond the scope of this blog post, but just know that item_tfms allows you to pre-process your data before it hits your model. | In that snippet of code, what we’ve done is specify a template through which to create DataLoaders. What are they you might ask? Well, read on! . Tip: For more information on DataBlocks, I highly recommend Aman’s blog post covering the DataBlocks API. This is also where I got the image for the DataBlocks overview. . DataLoader and DataLoaders . Now that we’ve defined a DataBlock, and we’ve specified exactly how our data needs to be structured, categorized and processed, we can start actually feeding in the data for our model to train on. We load this data in with…you guessed it… a Data loader. This is where DataLoaders come in. A DataLoaders is an iterator class that our DataBlock will call to load data according to the rules that we’ve specified in specific chunks (called batch size). . A DataLoader in fast.ai is a superset of the PyTorch DataLoader, with more helpful callbacks and flexibility. Whereas the Data block knows how to structure the data, the Loader knows how to work with it in the context of training a machine learning model – i.e. how much to feed to the model at once (batch size), how many processes to spawn to load the data, how much memory to allocate and many more. . A DataLoaders (note the plural), is a thin class that automatically generates multiple DataLoader (singular) objects based on the rules specified in our DataBlock. . Conclusion and TLDR . We’ve seen what they are, now let’s re-iterate how they work together and what their differences are: . A DataBlock is the data pipeline. A template that we create that has NO data, but has all the context on how to work with it. For example, how to split up the data, the data types of our features and targets/labels, how to extract the labels from the underlying data (e.g. folder name). | A DataLoader doesn’t care about preparing data, it expects the data ready to go and only cares about how to load the data (e.g. whether in parallel or in a single process) as well as feeding the data to the model in batches (i.e. batch size) | A DataLoaders is a thin wrapper for more than one DataLoader. | . In the context of training a model, you will first create a DataBlock, specify all data processing pipelines, and then load your data through your DataBlock via the .dataloaders property, like so: . path = Path(&#39;cats&#39;) #e.g. a dir structure such as &#39;cats/sphynx/001.jpg&#39; dls = cats.dataloaders(path) # our data is ready to be fed into a model learn = cnn_leaner(dls, resent18, metrics=error_rate) learn.fine_tune(5) . Hope this post helps clarify what they are and how these two structures work together. As my knowledge of fast.ai grows, I’ll be sure to update this post .",
            "url": "https://muttoni.github.io/blog/machine-learning/fastai/2020/12/26/datablocks-vs-dataloaders.html",
            "relUrl": "/machine-learning/fastai/2020/12/26/datablocks-vs-dataloaders.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Event Gallery",
            "content": "Events . Over the last couple years I spoke at A LOT of events (over 300+). That’s what happens when you become a Technical Evangelist! I’ve been terrible at keeping track of photos from all the events, but will add as I come across them! . Notable speaking engagements: AWS re:Invent, Web Summit, IFA Berlin, Droidcon, Codemotion, Codetalks Hamburg, Cambridge University, Imperial College London, Euroconsumer, University College London, Alexa workshops in USA (Seattle), UK/IE (England, Ireland, Scotland), Germany (Berlin, Cologne, Frankfurt), Italy, Spain and hundreds more! . . .",
            "url": "https://muttoni.github.io/blog/events/2020/12/22/event-gallery.html",
            "relUrl": "/events/2020/12/22/event-gallery.html",
            "date": " • Dec 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hi, thanks for taking the time to visit my blog. If you don’t know me, my name is Andrea Muttoni. Over the last 6 years I’ve been working at Amazon where I’ve covered all sorts of roles: from Kindle book pricing, to Technical Product Management and more recently Solution Architecture / Technical Evangelism. Prior to Amazon, I co-founded a software development agency mainly focused on the creative sector where I had the immense opportunity to learn several roles at once: software engineering, UI/UX design, sales and product management. In a world of specialists, I am an undying generalist, with a bad habit of diving into new things and trying to learn them inside out (and often failing). . Combining technology with creativity is something that never ceases to intrigue me. I started this blog to document at least a small fraction of the adventures that I embark on exploring the bottomless wonders of technology (and music). Two areas where I will always feel like a Morty1, inspired and dragged along by all the genius Ricks of the world. . Main areas of interest . Technical Product Management | Fullstack Development (From UI/UX design to actually implementing JS components and backend APIs) | Public speaking &amp; Developer Relations | Machine Learning | Music production | Anything that combines any of the above. | . Experience . Technical Experience: I have worked on all sorts of software projects over the years: full-stack web applications, browser extensions, physical server administration, cloud-based serverless API development, crawlers, machine learning models. I enjoy keeping up with new technologies, as it helps me discover new ways to tackle a problem, or appreciate “boring” solutions even more…win-win! HN is my NYT. Writing code for me is a means to an end – the real objective is creating something that’s usable and beautiful (in that order). For me, that flow that makes one effortlessly focus for days on end without distractions comes from two things: building things and composing music. . Business Experience: Not that you asked, but I don’t believe in “business” vs. “tech”. I think the world would be a better place if everyone had some technical skills: people would be more efficient and more in tune with what a machine can accomplish (as opposed to getting frustrated or giving up on the damn computer altogether!). I also believe the world (and countless work environments) would be a better place if engineers were more integrated into the business process. With that out of the way, my main business experience is focused around all the facets of product management, which I have done for several years at Amazon. My ideal product management role would have oversight on all the aspects of a software product: UX/UI (in that order), API design, Developer documentation and of course software architecture and functionality. The world needs more technical business people! (or business-y tech people, if you prefer) . Misc: I also really enjoy public speaking. As a Tech Evangelist for Amazon, I spoke at more than 300 events in all shapes and sizes: keynotes with thousands of people, all the way to small hands-on coding workshops with 20 developers. When COVID subsides, I’d love to pick up speaking again. Digital events are just not the same. Notable speaking engagements: AWS re:Invent, Web Summit, IFA Berlin, Droidcon, Codemotion, Codetalks Hamburg, Cambridge University, Imperial College London, Euroconsumer, University College London, Alexa developer workshops in USA, England, Ireland, Scotland, Germany, Italy, Spain and hundreds of other events! Tech conferences are just awesome, especially for the communities that are born around them. . If you’ve had enough of my blabbering and prefer a traditional CV, you can find me on LinkedIn. . Certifications . What brings you here? Have a question? Want to chat? Let me know by shooting me a note2 . Check my favicon. (Morty is a character from the show Rick and Morty) &#8617; . | You can reach me at my last name followed by my first name, at (f+1)mail. You can figure it out! Hopefully bots can’t…but probably will. &#8617; . |",
          "url": "https://muttoni.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://muttoni.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}