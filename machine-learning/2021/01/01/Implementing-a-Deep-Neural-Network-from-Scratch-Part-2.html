<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Implementing a Deep Neural Network from Scratch - Part 2 | Andrea Muttoni</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Implementing a Deep Neural Network from Scratch - Part 2" />
<meta name="author" content="Andrea Muttoni" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this series we build a Neural Network from the ground up, covering basic concepts along the way. In Part 2 we add a better loss function and transform our Linear Classifier into a real Deep Neural Network." />
<meta property="og:description" content="In this series we build a Neural Network from the ground up, covering basic concepts along the way. In Part 2 we add a better loss function and transform our Linear Classifier into a real Deep Neural Network." />
<link rel="canonical" href="https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html" />
<meta property="og:url" content="https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html" />
<meta property="og:site_name" content="Andrea Muttoni" />
<meta property="og:image" content="https://muttoni.github.io/blog/images/nn-from-scratch-part-2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-01T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html","@type":"BlogPosting","headline":"Implementing a Deep Neural Network from Scratch - Part 2","dateModified":"2021-01-01T00:00:00-06:00","datePublished":"2021-01-01T00:00:00-06:00","image":"https://muttoni.github.io/blog/images/nn-from-scratch-part-2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html"},"author":{"@type":"Person","name":"Andrea Muttoni"},"description":"In this series we build a Neural Network from the ground up, covering basic concepts along the way. In Part 2 we add a better loss function and transform our Linear Classifier into a real Deep Neural Network.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://muttoni.github.io/blog/feed.xml" title="Andrea Muttoni" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BLM1GZKCEL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BLM1GZKCEL');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Andrea Muttoni</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Implementing a Deep Neural Network from Scratch - Part 2</h1><p class="page-description">In this series we build a Neural Network from the ground up, covering basic concepts along the way. In Part 2 we add a better loss function and transform our Linear Classifier into a real Deep Neural Network.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-01T00:00:00-06:00" itemprop="datePublished">
        Jan 1, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      49 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#machine-learning">machine-learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Getting-Started">Getting Started </a></li>
<li class="toc-entry toc-h2"><a href="#Improving-our-Label-Structure">Improving our Label Structure </a></li>
<li class="toc-entry toc-h2"><a href="#Softmax-Needs-a-Friend">Softmax Needs a Friend </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Log:-a-Softmax's-Best-Friend">Log: a Softmax&#39;s Best Friend </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Cross-Entropy-Loss">Cross-Entropy Loss </a></li>
<li class="toc-entry toc-h2"><a href="#Generalizing-our-Linear-Model">Generalizing our Linear Model </a></li>
<li class="toc-entry toc-h2"><a href="#Activating-our-Network!">Activating our Network! </a>
<ul>
<li class="toc-entry toc-h3"><a href="#The-Activation-Function">The Activation Function </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Implementing-a-Deep-Neural-Network">Implementing a Deep Neural Network </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Layers">Layers </a></li>
<li class="toc-entry toc-h3"><a href="#Fit">Fit </a></li>
<li class="toc-entry toc-h3"><a href="#Forward-&-Backward">Forward &amp; Backward </a></li>
<li class="toc-entry toc-h3"><a href="#Putting-It-All-Together">Putting It All Together </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="# Testing-our-Deep-Classifier"> Testing our Deep Classifier </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Analyzing-Results">Analyzing Results </a></li>
<li class="toc-entry toc-h3"><a href="#Learning-Rates">Learning Rates </a></li>
<li class="toc-entry toc-h3"><a href="#Epochs">Epochs </a></li>
<li class="toc-entry toc-h3"><a href="#Layers">Layers </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Conclusions">Conclusions </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Aknowledgements">Aknowledgements </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-01-Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>This post is the second post of a two-part series. If you landed on this page directly, I recommend following the whole journey and start by reading <a href="https://muttoni.github.io/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html">Part 1</a> first.
</div>
<p>This is Part 2 of our "Implementing a Deep Neural Network from Scratch" series. In this part we will improve on our Linear classifier started in <a href="/blog/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html">Part 1</a> and turn our Linear Model into a fully-functioning Deep Neural Network!</p>
<p>As a reminder, our goal is to create a Neural Network that can distinguish all 10 handwritten digits (using the industry standard <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset). This is also known as a Multi-Label Classifier (a traditional Classifier only has two labels, such as cat or dog, hotdog or not hotdog, etc). In Part 1 we were able to train a simple linear regressor achieving roughly 80% accuracy after 50 epochs. We'll see in this part how to make it even more powerful with just a simple tweaks, as well as turn it into a Deep Neural Network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-Started">
<a class="anchor" href="#Getting-Started" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started<a class="anchor-link" href="#Getting-Started"> </a>
</h2>
<p>Below is a recap of all the preparatory steps to setup our data pipeline. In order: we download the data, generate a list of file paths, create training and validation tensor stacks from the file paths, convert those tensors from rank-2 (2D matrix) tensors (i.e. size: 28, 28) to rank-1 (1D vector) tensors (i.e. size: 784).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Requirements</span>
<span class="o">!</span>pip install -Uq fastbook
<span class="kn">from</span> <span class="nn">fastbook</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Download the data</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST</span><span class="p">)</span>

<span class="c1"># Import the paths of our training and testing images</span>
<span class="n">training</span> <span class="o">=</span> <span class="p">{</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span> <span class="p">:</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sa">f</span><span class="s1">'training/</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="p">}</span>
<span class="n">testing</span> <span class="o">=</span> <span class="p">{</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span> <span class="p">:</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sa">f</span><span class="s1">'testing/</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="p">}</span>

<span class="c1"># Prepare training tensor stacks</span>
<span class="n">training_tensors</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
            <span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">digit</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span> <span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="n">training</span><span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span>
          <span class="p">])</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
      <span class="p">]</span>

<span class="n">validation_tensors</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
            <span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">digit</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span> <span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="n">testing</span><span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span>
          <span class="p">])</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
      <span class="p">]</span>


<span class="c1"># Convert our 2D image tensors (28, 28) into 1D vectors</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">training_tensors</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">valid_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">validation_tensors</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before generating our labels, we'll make a change in our labeling structure so as to be more efficient in space complexity as well as be more elegant and extendable for any number of labels in the future.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Improving-our-Label-Structure">
<a class="anchor" href="#Improving-our-Label-Structure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improving our Label Structure<a class="anchor-link" href="#Improving-our-Label-Structure"> </a>
</h2>
<p>Our labels in Part 1 were represented by a vector of size 10 with a '1' flag indicating the correct digit at the corresponding index, and '0' everywhere else. In our loss function, we then compared whether the max index of our predictions was equivalent to the index in our target labels where the index was '1'. Here's a quick example of how our old labels worked:</p>
<p>A target label $y$ for the digit '3':</p>

<pre><code>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
          ^ a '1' flagging the correct digit at its index (3)</code></pre>
<p>An example prediction from our model:</p>

<pre><code>[2.0e-3, 1.3e-7, 4.0e-8, 9.9e-1, 2.0e-5, 1.6e-4, 4.4e-9, 7.0e-9, 1.3e-6, 2.1e-4, 4.0e-7]
                         ^ index of max value corresponds to the y label index, 
                           indicating a correct prediction.</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is, however, a much more efficient way of representing our labels. Can you guess what it is? If you guessed assigning a value to each of our digits (e.g. an integer corresponding to that digit) and just have that be our $y$ target label, then you guessed right! But in our loss function, how can we match/compare a prediction vector of 10 values to a target label, if the label contains only an integer (e.g. "3").</p>
<p>To answer this question we need to go back to our beloved <strong>Softmax</strong>.</p>
<p>$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j{e^{x_j}}}$</p>
<p>This is a great time to read <a href="/blog/machine-learning/2020/12/28/Implementing-a-Deep-Neural-Network-from-Scratch-Part-1.html">Part 1</a> if you haven't already done so. Here's a quick reminder of how our Softmax works in three steps:</p>
<ol>
<li>Given a list of values, we calculate the exponential for each value, that is: $e^{n}$. Where $n$ is our value and $e$ is Euler's number, a <a href="https://en.wikipedia.org/wiki/Eulers_number">special number in Mathematics</a>, and at the heart of the exponential function.</li>
<li>We sum the exponential values (the Sigma symbol $\sum$ means <em>sum</em>).</li>
<li>Take each value in Step 1. and divide it by the sum obtained in Step 2. </li>
</ol>
<p>The pecularity of Softmax is that when we softmax a list of values, since they are all divided by their sum, <strong>all the values sum to 1</strong>. This is incredibly useful as it means we only really need 1 value from the list, i.e. in our case the prediction corresponding to the correct label, as the rest of the values are just 1 - that value. If that value increases (i.e. the probability of our correct digit prediction increases), it also means that every other value in the prediction vector is consequently decreasing. This is why Softmax is a critical part of any multi-label classifier where only 1 label out of a group is correct (e.g. model of car), as it helps "hone in" on a single value, and work around that.</p>
<p>Practically, this means we can simplify our label structure and go from a vector of flags to a single integer representing the index of our possible labels. In the case of our MNIST dataset, the labels and indexes match, and will both go from 0 to 10. This means that if our $y$ variable is '0', our digit is '0', and we should look at the prediction value of our output vector at index '0'. The same goes for every other number. In cases were our $y$ is not a digit, we would still assign an integer to each possible label and proceed in exactly the same manner.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">training</span><span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]))</span>
<span class="n">valid_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">testing</span><span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In our loss and accuracy functions, we'll also need to update the way we compare results. All it takes is a little code change (our loss function will change a little bit in the next section, for now just focus on the updated labeling mechanism).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span> 

  <span class="c1"># softmax the vectors, each containing 10 probabilities</span>
  <span class="n">sm_preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span> 
  
  <span class="c1"># generate a range equal to length of prediction batch</span>
  <span class="n">idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>

  <span class="c1"># from each vector, only pick the correct prediction </span>
  <span class="n">results</span> <span class="o">=</span> <span class="n">sm_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="p">]</span>

  <span class="o">...</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This nifty accessing technique, coupled with Softmax's characteristic of summing to one, means that with only need a single value from our predictions, not the whole vector, to calculate the prediction loss, as every other value in our prediction vector is completely co-dependent and therefore irrelevant.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now load our dataset and dataloaders as we did in Part 1:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create datasets to feed into the dataloaders</span>
<span class="n">dset</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">))</span>
<span class="n">dset_valid</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">valid_x</span><span class="p">,</span> <span class="n">valid_y</span><span class="p">))</span>

<span class="c1"># Setup our dataloders</span>
<span class="n">dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset_valid</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Softmax-Needs-a-Friend">
<a class="anchor" href="#Softmax-Needs-a-Friend" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmax Needs a Friend<a class="anchor-link" href="#Softmax-Needs-a-Friend"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we can proceed to improve and generalize our MNIST model from Part 1, we need to continue talking about Softmax. Yes, I know, by now you probably think I have a secret fetish for Softmax, and you're probably right. But Softmax needs a little TLC in order to function orders of magnitude better.</p>
<p>What the hell am I going on about? Well, Softmax is based on the exponential function $e^x$, which means that as our $x$ value increases, the exponential values increase...exponentially. The consequence of this is that a parameter that is slightly larger than the rest, when softmax'd, will tend to be exaggerated, squishing the rest. Here is an example (toggle to code block to see the softmax implementation):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Quick softmax implementation, expecting a list of values (rank-1 tensor)</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Given an input vector v, returns an output vector containing</span>
<span class="sd">  the softmax values for each element in the vector</span>
<span class="sd">  """</span>
  <span class="n">exp_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">exp_v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">exp_v</span> <span class="o">/</span> <span class="n">exp_sum</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">softmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.0420, 0.1142, 0.8438])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice how in the example above, the '4', when softmax'd, takes a larger-than-proportional share of the probability. As the difference gets larger, this becomes even more evident. This is great when making final predictions as we want the highest probability to shine through, but not when we want a nice stable loss function to help the model improve. As soon as a parameter is slightly larger than the rest, it will be hard for the others to compete and "catch up". We'll see how we can convert our naive loss function into an industry standard one, by <em>softening</em> Softmax (ironic, isn't it?). We'll then compare performance and you'll see how a very small change will help get us better and more stable training accuracy in just a few epochs!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Log:-a-Softmax's-Best-Friend">
<a class="anchor" href="#Log:-a-Softmax's-Best-Friend" aria-hidden="true"><span class="octicon octicon-link"></span></a>Log: a Softmax's Best Friend<a class="anchor-link" href="#Log:-a-Softmax's-Best-Friend"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I've never liked logs, unless they are made of wood. But it seems logs (aka logarithms) are a crucial part of everyday life, as they help represent exponential values in linear form. I won't dive too deep into logarithms, as there are plenty of resources that will do a better job at explaining them. Feel free to look them up for a couple minutes before continuing this post just so you have a quick primer.</p>
<p>Since Softmax stems from the exponential function ($e^x$), by taking the log of the softmax'd values we are able to represent numers that are distanced exponentially in a more linear way. This is what I mean by "softening" Softmax, as rather than dealing with the raw values we are dealing with their exponents in base e. This helps stretch the values that were otherwise confined between 0 and 1, to a much larger range...what is the largest range imaginable? Infinity! Actually, for softmax it's 0 to -Infinity to be precise, and we'll see why in just a second.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Our log function, in the context of Softmax, is always in base $e$, as our values are raised to the power of $e$. In Mathematics, logarithms involving $e$ (i.e. in <em>base</em> $e$) are often referred to as $ln$. 
</div>
<p>Our log function output (y-axis) can be intuitively thought of as: <em>"to the power of what must we raise $e$ in order to get $x$</em>. This is why log is not defined for $x &lt; 0$ (remember: $e$ is positive, roughly equal to 2.718, so an exponent will never be able to change the sign of a positive number).</p>
<p>From the log plot above, we can see that $log_e(x)$ outputs values from -Infinity as $x$ approaches 0, and approaches 0 when $x$ approaches 1. Consequently, our Softmax values (that cumulatively sum to 1, and therefore always between 0 and 1) will <strong>always be negative</strong>. Let's take a look at our previous example with <code>[1,2,4]</code>, softmax them again and then take the log. Notice how the results are negative, and the relative ratios between them have softened.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-3.1698, -2.1698, -0.1698])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can multiply very value by -1 so as to deal with positive numbers, without changing the fact that the highest original number (4) is now the closest to 0...that is...closest to our minimum.</p>
<p>So we now have a function that squeezes original parameters into co-dependent probabilties (via softmax), then stretches them into a broader logarithmic range (via log) to help linearize the rate of growth, and <em>at the same time</em> making it so that higher values turn into something that can be <em>minimized</em> (approach zero). Now doesn't that sound perfect for a loss function?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cross-Entropy-Loss">
<a class="anchor" href="#Cross-Entropy-Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-Entropy Loss<a class="anchor-link" href="#Cross-Entropy-Loss"> </a>
</h2>
<p>What we've just seen above is a very popular loss function used in classification called <strong>Cross-Entropy Loss</strong>. It allows us to leverage the useful characteristics of softmax while retaining a relatively linear curve for parameter values so as to facilitate training. We'll see that by making this small change in our previous model, our training will be much more effective.</p>
<p>For reference, here is our <strong>old</strong> code (feel free to toggle it).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MNISTLinearClassifier</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">train_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">valid_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_calc_grad</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_tensor</span><span class="p">):</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Return digit and vector of probabilities</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">probabilities</span>

  <span class="k">def</span> <span class="nf">_calc_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span> 
    <span class="n">_</span><span class="p">,</span> <span class="n">max_indices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># get the index of max value along 2nd dimension</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">tag_indices</span> <span class="o">=</span> <span class="n">yb</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># get index of flag in our label tensors</span>
    <span class="n">corrects</span> <span class="o">=</span> <span class="n">max_indices</span> <span class="o">==</span> <span class="n">tag_indices</span> <span class="c1"># check whether they match</span>
    <span class="k">return</span> <span class="n">corrects</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># calculate mean</span>

  <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_linear_eq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="nd">@self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

  <span class="k">def</span> <span class="nf">_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

  <span class="c1"># Linear regression using SGD</span>
  <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below is our <strong>new</strong>, updated code. We'll leverage the new label structure and implement a new loss function by performing all the steps discussed in the previous section: log our softmax values, and turn them positive. Everything else is pretty much identical (aside from our <code>_batch_accuracy</code> function that needed updating due our new labeling structure too). Feel free to diff it into diffchecker to see what changed.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MNISTLinearClassifier</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">train_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">valid_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_calc_grad</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">]:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_tensor</span><span class="p">):</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">probabilities</span>

  <span class="k">def</span> <span class="nf">_calc_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">max_indices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">corrects</span> <span class="o">=</span> <span class="n">max_indices</span> <span class="o">==</span> <span class="n">yb</span>
    <span class="k">return</span> <span class="n">corrects</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear_eq</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_linear_eq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="nd">@self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

  <span class="k">def</span> <span class="nf">_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span> <span class="c1"># Cross-Entropy Loss</span>
    <span class="n">log_sm_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_sm_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now remember our old model took 50 epochs (on a good run) to reach 80% accuracy. If we try our new model now, it will consistently beat 80% accuracy within the first epoch! That's a considerable improvement thanks to a "log" and a negative sign. Goes to show how critical a proper loss function is in training a model effectively.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MNISTLinearClassifier</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch #0 0.8151
Epoch #1 0.8507
Epoch #2 0.8656
Epoch #3 0.8777
Epoch #4 0.8789
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It already beat our original model within the first epoch (81.5% accuracy vs 80% after 50 epochs) and reached ~88% accuracy within 5 epochs. That's awesome! We could try experimenting with various batch sizes and learning rates to see how accurate we can get our model within an arbitrary number of epochs, but this is beyond the scope of this post. Let's clean up our code further and turn it into a general Linear Classifier.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generalizing-our-Linear-Model">
<a class="anchor" href="#Generalizing-our-Linear-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Generalizing our Linear Model<a class="anchor-link" href="#Generalizing-our-Linear-Model"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Up until now, we wanted to build a MNIST classifier. Before we move on and "deepen" our neural network, let's clean up our code, modularize it and turn it into a general Linear Classifier that can be applied to any dataset, with any number of labels.</p>
<p><strong>Linear Model</strong></p>
<p>We'll start by generalizing our Linear equation into a standalone "LinearModel". It will simply initialize a tensor with a desired number of input and output parameters (weights and biases) with random values. The <code>model</code> method will carry out the actual linear equation, and the <code>parameters</code> method will return the weights and biases.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearModel</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

  <span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="nd">@self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

  <span class="k">def</span> <span class="nf">_init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Optimizer</strong></p>
<p>We'll then proceed to strip out our optimizer. Our optimizer will have a <code>step</code> method that will carry out the update for each parameter based on its gradient and an arbitrary learning rate <code>lr</code> (e.g. 0.01).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SGD_Optimizer</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span> 
  
  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's rewrite our Classifier to use these two newly created classes, and run it again on the MNIST dataset to check that it works:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearClassifier</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">softmax</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">train_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">valid_dl</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">LinearModel</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_calc_grad</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_tensor</span><span class="p">):</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">probabilities</span>

  <span class="k">def</span> <span class="nf">_calc_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">x_label</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">corrects</span> <span class="o">=</span> <span class="n">x_label</span> <span class="o">==</span> <span class="n">yb</span>
    <span class="k">return</span> <span class="n">corrects</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">log_sm_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_sm_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset_valid</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LinearClassifier</span><span class="p">(</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> 
    <span class="n">output_shape</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">valid_dl</span><span class="o">=</span><span class="n">valid_dl</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
    <span class="n">optimizer</span><span class="o">=</span><span class="n">SGD_Optimizer</span><span class="p">,</span>
    <span class="n">softmax</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch #0 0.7352
Epoch #1 0.8014
Epoch #2 0.8286
Epoch #3 0.8457
Epoch #4 0.8532
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like before, we could try experimenting with various batch sizes and learning rates -- here I used a <code>batch_size</code> of 128 and a learning rate <code>lr</code> of 0.1. The advantage of smaller batch sizes is that it allows the model to have more chances to perform gradient descent on the parameters before finishing an epoch, but the gradients may be less representative of the total dataset as they are based on fewer samples. The learning rate is also a critical aspect to experiment with as it determines how much of a step is done at every optimization run. If the learning rate is too big, we risk overshooting the minimum loss and potentially never reaching it, if the learning rate is too low, we won't make significant progress in training. Keep in mind you don't always need to guess, as some libraries offer a way to test different learning rates. For example, fast.ai offers a <a href="https://fastai1.fast.ai/callbacks.lr_finder.html">learning rate finder</a> in its Learner class. I also wrote a post on <a href="/blog/machine-learning/2021/01/08/Implementing-a-Learning-Rate-Finder-from-Scratch.html">implementing a learning rate finder from scratch</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Activating-our-Network!">
<a class="anchor" href="#Activating-our-Network!" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activating our Network!<a class="anchor-link" href="#Activating-our-Network!"> </a>
</h2>
<p>As mentioned at the conclusion of Part 1, what we've done up to now is create a linear classifier with a self-correcting capability through Stochastic Gradient Descent...<em>not</em> a deep neural network. That is, because a neural network is non-linear, and it is composed of multiple layers. How do we break the linearity and add layers?</p>
<p>Is it by adding one or more linear models after the first one? Yes! These are called layers (and any intermediate layers before our output layer are called "hidden layers"). But adding more layers is not enough. If we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. We need something more inbetween each linear classifier that breaks beyond the sad confines of a linear world: an <strong>activation function</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we tackle the activation function, let's recap what <strong>activations</strong> are. Activations are the calculated outputs from our linear equation(s). This means that in our Linear Model above, by creating 10 parameters for each pixel (remember our input weights are of shape (28*28, 10), our linear equation will output 10 <em>activations</em> per image. Since our Linear Classifier was comprised of only 1 layer of parameters, our 10 activations also correspond to the 10 desired <em>outputs</em> of our model, i.e. when softmax'd each of the 10 items in the activation vector represent the probabilities for each digit [0-9] in our classification task. As we'll see in just a moment, the activations don't necessarily need to equal to our outputs, as there can be several <em>layers</em> of linear models in a deep neural network (hence the name: "deep").</p>
<p>So one layer can output an arbitrary number of features (in practice the activations of that layer) and then feed those features into a second layer as inputs, that will output another set of features for the next layer, and so on, until the last layer where the activations correspond to our final output.</p>
<p>However, as we said earlier, if we were to add another linear model right after our first one, the weights and biases would still end up being...linear... as any sequence of linear equations can be summarized as a single linear equation with a set of parameters representing the average of the various layers. So how can we add more than 1 linear layers and break the inherent linearity? That's where the <strong>activation function</strong> comes into play.</p>
<h3 id="The-Activation-Function">
<a class="anchor" href="#The-Activation-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Activation Function<a class="anchor-link" href="#The-Activation-Function"> </a>
</h3>
<p>The activation function acts as a filter through which input parameters are transformed from one layer to the next, so as to break the linear relationship. Any function that is not a linear equation technically acts as an non-linearity (e.g. a log function, an exponential function, etc). You can see a collection of activation functions <a href="https://en.wikipedia.org/wiki/Activation_function">here</a>.</p>
<p>Among those, one function that works really well, is as simple as can be: the Rectifier function (also known as <strong>Rectified Linear Unit</strong>, or <strong>ReLU</strong>).</p>
<p>$f(x) = max(x, 0)$</p>
<p>That's it! The ReLU will simply replace any <em>negative</em> input values with 0 (or you can read it as: <em>rectify</em> any negative values <em>to</em> 0). Other input values remain completely unchanged. In code, this is as simple as:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">ReLU</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">))</span>

<span class="n">plot_function</span><span class="p">(</span><span class="n">ReLU</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"The ReLU Function"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This simple transformation is enough to break linearity! Feel free to read more about the <a href="https://en.wikipedia.org/wiki/Rectifier_%5C(neural_networks%5C">ReLU function</a>), but for our purposes this is all we need to continue.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implementing-a-Deep-Neural-Network">
<a class="anchor" href="#Implementing-a-Deep-Neural-Network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementing a Deep Neural Network<a class="anchor-link" href="#Implementing-a-Deep-Neural-Network"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/neural_network.png" alt="" title="A Neural Network, source: Wikipedia"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Equipped with our handy ReLU, we are ready to transform our Linear Model into a <strong>real</strong> Neural Network and allow for more than one linear layer! Now remember, a Neural Network is comprised of multiple linear layers (2 being the minimum not counting the activation function: one layer for the inputs, the activation, and the second layer for the outputs). Any number of layers can be added inbetween and are called <strong>hidden layers</strong>. There can be as many layers as we want, although the deeper the network, the more sophisticated it will be, meaning it will take longer to train, and the increased number of parameters can absorb a lot more peculiarities from each training image potentially over-fitting the data and not performing as well in validation, testing and production. We'll touch more on this further on in the post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we're going to be doing now, is effectively implement a very simple version of PyTorch's <code>nn.Sequential</code>. In <code>nn.Sequential</code>, we can specify $n$ layers and what activation function to use. The <strong>activations</strong> of any prior layer correspond to the <strong>inputs</strong> of the next layer. The last layer will always output the activations to be run through the <strong>loss function</strong>, exactly like in our LinearClassifier.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Example 1</span>
<span class="n">simple_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">),</span> <span class="c1"># an input layer of 28*28 with 30 activations</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>           <span class="c1"># the ReLU activation</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>     <span class="c1"># another linear layer with 30 inputs and 10 activations</span>
<span class="p">)</span>

<span class="c1"># Example 2</span>
<span class="n">deeper_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span><span class="c1"># an input layer of 28*28 with 100 activations</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>           <span class="c1"># the ReLU activation</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">30</span><span class="p">),</span>   <span class="c1"># a hidden layer of 100 inputs and 30 activations</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>           <span class="c1"># the ReLU activation</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>     <span class="c1"># another linear layer with 30 inputs and 10 activations</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So how can we replicate something similar to the above? The good news is that we don't need to change a whole lot from our original LinearClassifier, except to introduce the concept of <strong>layers</strong> to our Classifier, as well as add the ReLU function. Other parts of the code (<code>SGD_Optimizer</code> and <code>LinearModel</code>) can be re-used as is!</p>
<h3 id="Layers">
<a class="anchor" href="#Layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layers<a class="anchor-link" href="#Layers"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Taking inspiration from PyTorch's <code>nn.Sequential</code> mentioned above, we'll accept layers in a similar fashion, except we'll ReLU any input and hidden layers (everything but the last layer) automatically. That means we'll need to implement our DeepClassifier so as to be able to setup our model in the following way:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="c1"># Example 1 (note: we still need to implement DeepClassifier)</span>
<span class="n">simple_nn</span> <span class="o">=</span> <span class="n">DeepClassifier</span><span class="p">(</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="c1"># We'll ReLU automatically after Layer 1.</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>     <span class="c1"># LinearModel is the same one we used previously</span>
<span class="p">)</span>

<span class="c1"># Example 2</span>
<span class="n">deeper_nn</span> <span class="o">=</span> <span class="n">DeepClassifier</span><span class="p">(</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="c1"># first layer, ReLU'd automatically</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>    <span class="c1"># hidden layer, also ReLU'd automatically </span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>     <span class="c1"># output layer</span>
<span class="p">)</span>
</pre></div>
<p>We'll begin by changing the name of our class from <code>LinearClassifier</code> to <code>DeepClassifier</code>. Finally! We'll also add a <code>self.layers</code> property in the <code>__init__</code> method that stores the incoming layers provided as a list of arguments when we setup the model. I'd also like to start logging/saving training loss to compare against our validation accuracy stored in <code>self.accuracy_scores</code> (remember it saves results from the <code>_validate_epoch</code> function that runs our weights against the validation data after each epoch). To do this we'll also add a <code>self.training_loss</code> list to save to.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">layers</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span> <span class="c1"># add a layers property</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fit">
<a class="anchor" href="#Fit" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fit<a class="anchor-link" href="#Fit"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll then change the terminology of our classifier so that instead of <code>train</code> we use <code>fit</code>. This is more consistent with other Deep Learning Libraries. In our <code>fit</code> method, we'll calculate how many layers <code>self.layers</code> has, and save the last layer's index (<code>self.last_layer_index</code>) so that we don't ReLU that layer later on. We'll also setup an optimizer for each layer that we can call <code>.step</code> from, using our previously defined <code>SGD_Optimizer</code> class. We'll also store the losses of each batch in an epoch to average out and compare against our validation accuracy.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'train_dl'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'valid_dl'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'epochs'</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'lr'</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'verbose'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># we'll average this after every epoch</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_layer_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="c1"># save last layer's index.</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="c1"># for each layer, create an Optimizer we can .step() later on</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SGD_Optimizer</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">))</span>

    <span class="c1"># our usual training loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">:</span>
        <span class="c1"># notice we use _forward now</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
  
      <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Forward-&amp;-Backward">
<a class="anchor" href="#Forward-&amp;-Backward" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forward &amp; Backward<a class="anchor-link" href="#Forward-&amp;-Backward"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll also introduce another structural change in the form of how our model operates. Rather than call <code>_calc_grad</code>, we'll instead group all of our linear calculations and activations in a <code>_forward</code> function, and all of our gradient calculations and optimizations in a <code>_backward</code> function. This makes more sense logically as we first calculate, then backtrack to get the gradients and update our parameters accordingly.</p>
<p>The <code>_forward</code> step loops through each layer in <code>self.layers</code> and calls each <code>LinearModel</code>'s <code>model</code> method (i.e. the famous linear equation <code>xb@w + b</code>). As long as the layer is not the last layer (<code>if layer_idx != self.last_layer_index:</code>), it will also rectify the outputs before passing them on to the next layer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">xb</span>
    <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_layer_index</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReLU</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The ReLU function is identicaly to the one created above. Pretty simple right?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">_ReLU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our backward step <code>_backward</code> entails running our final outputs through the loss function and then calling <code>.backward()</code> to calculate the gradients of each tensor automatically. We'll also take note of the loss output so we can average each epoch's average loss in our <code>_validate_epoch</code> function. We'll then loop through all of our optimizers in <code>self.optimizers</code> and <code>step()</code> them so as to update the parameters according to our learning rate.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="c1"># keep track of each batch's loss in a list</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">:</span> <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our loss function also changes slightly, as we'll switch to using <code>torch.log_softmax</code> directly instead of torch.log followed by torch.softmax. This is because our previous implementation caused issues with gradients returning NaN across multiple layers.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="c1"># we use torch.log_softmax directly</span>
    <span class="n">log_sm_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_sm_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can update our <code>_validate_epoch</code> function to condense the batch's losses into an average so we can compare each epoch's validation accuracy score vs the average training loss.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    
    <span class="c1"># stack all the saved losses from each batch and take the average</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    
    <span class="c1"># clear the array for the next epoch of batches</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># append to the training_losses list</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'Loss:'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="s1">'Accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Putting-It-All-Together">
<a class="anchor" href="#Putting-It-All-Together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting It All Together<a class="anchor-link" href="#Putting-It-All-Together"> </a>
</h3>
<p>And here is our DeepClassifier in all its glory!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DeepClassifier</span><span class="p">:</span>
  <span class="sd">"""</span>
<span class="sd">  A multi-layer Neural Network using ReLU activations and SGD</span>
<span class="sd">  params: layers to use (LinearModels)</span>
<span class="sd">  methods: fit(train_dl, valid_dl, epochs, lr)</span>
<span class="sd">  """</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">layers</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>

  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'train_dl'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'valid_dl'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'epochs'</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'lr'</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'verbose'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_layer_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SGD_Optimizer</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">:</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
  
      <span class="bp">self</span><span class="o">.</span><span class="n">_validate_epoch</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_tensor</span><span class="p">):</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">probabilities</span>

  <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">xb</span>
    <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_layer_index</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ReLU</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

  <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">:</span> <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">max_indices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">corrects</span> <span class="o">=</span> <span class="n">max_indices</span> <span class="o">==</span> <span class="n">yb</span> 
    <span class="k">return</span> <span class="n">corrects</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> 

  <span class="k">def</span> <span class="nf">_validate_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epoch_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'Loss:'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="s1">'Accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">log_sm_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_sm_preds</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_ReLU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The full code is available in this gist: <a href="https://gist.github.com/muttoni/d5ce076fdc83b8f82b9971c5c8bf6b2d">https://gist.github.com/muttoni/d5ce076fdc83b8f82b9971c5c8bf6b2d</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id=" Testing-our-Deep-Classifier">
<a class="anchor" href="#%C2%A0Testing-our-Deep-Classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a> Testing our Deep Classifier<a class="anchor-link" href="#%C2%A0Testing-our-Deep-Classifier"> </a>
</h2>
<p>Now that we've implemented our DeepClassifier, let's test its performance. Before we do, it's important to discuss the topic of complexity (i.e. depth and breadth). The deeper (more layers) and broader (more activations) we make our network, the more parameters and complexity our model will be able to learn, with the risk of potentially over-fitting our training data. The number of layers and parameters in a network should always be proportional to the amount of data and its complexity. There is no rule to apply, you just need to experiment and see. There's plenty of discussions around this topic so feel free to browse the web for people's rules of thumbs.</p>
<p>It's important to remember, that even the simplest (2 layer) neural network is able to theoretically approximate any function, and for most simple tasks is <strong>more than enough</strong>! You can try as an exercise to play around with layers and activation sizes to see how training is affected. Remember the accuracy scores outputed from all of the models created until now are always against <strong>the validation data</strong>, meaning that you may see accuracy decrease over time (a clear sign that the model is overfitting on the training data).</p>
<p>Anyways, enough talk, let's see it in action!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dset_valid</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">my_nn</span> <span class="o">=</span> <span class="n">DeepClassifier</span><span class="p">(</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">valid_dl</span><span class="o">=</span><span class="n">valid_dl</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch #0 Loss: 2.9898 Accuracy: 0.5713
Epoch #1 Loss: 1.2028 Accuracy: 0.6553
Epoch #2 Loss: 1.0008 Accuracy: 0.6754
Epoch #3 Loss: 0.8964 Accuracy: 0.7375
Epoch #4 Loss: 0.8207 Accuracy: 0.7498
Epoch #5 Loss: 0.756 Accuracy: 0.7441
Epoch #6 Loss: 0.7119 Accuracy: 0.7968
Epoch #7 Loss: 0.6837 Accuracy: 0.7902
Epoch #8 Loss: 0.642 Accuracy: 0.8166
Epoch #9 Loss: 0.6159 Accuracy: 0.7876
Epoch #10 Loss: 0.588 Accuracy: 0.8122
Epoch #11 Loss: 0.5736 Accuracy: 0.8244
Epoch #12 Loss: 0.5501 Accuracy: 0.835
Epoch #13 Loss: 0.5312 Accuracy: 0.8494
Epoch #14 Loss: 0.5108 Accuracy: 0.847
Epoch #15 Loss: 0.5047 Accuracy: 0.8305
Epoch #16 Loss: 0.4969 Accuracy: 0.8367
Epoch #17 Loss: 0.4766 Accuracy: 0.8019
Epoch #18 Loss: 0.4663 Accuracy: 0.8555
Epoch #19 Loss: 0.4654 Accuracy: 0.8535
Epoch #20 Loss: 0.4491 Accuracy: 0.8722
Epoch #21 Loss: 0.4398 Accuracy: 0.8579
Epoch #22 Loss: 0.4373 Accuracy: 0.8768
Epoch #23 Loss: 0.4362 Accuracy: 0.8375
Epoch #24 Loss: 0.4235 Accuracy: 0.8765
Epoch #25 Loss: 0.4211 Accuracy: 0.8749
Epoch #26 Loss: 0.4167 Accuracy: 0.8379
Epoch #27 Loss: 0.4098 Accuracy: 0.8618
Epoch #28 Loss: 0.4002 Accuracy: 0.8837
Epoch #29 Loss: 0.3974 Accuracy: 0.8702
Epoch #30 Loss: 0.3911 Accuracy: 0.8827
Epoch #31 Loss: 0.3914 Accuracy: 0.8854
Epoch #32 Loss: 0.3893 Accuracy: 0.8752
Epoch #33 Loss: 0.3807 Accuracy: 0.8861
Epoch #34 Loss: 0.3773 Accuracy: 0.8909
Epoch #35 Loss: 0.3733 Accuracy: 0.8909
Epoch #36 Loss: 0.3733 Accuracy: 0.8849
Epoch #37 Loss: 0.3669 Accuracy: 0.8615
Epoch #38 Loss: 0.3641 Accuracy: 0.8708
Epoch #39 Loss: 0.3621 Accuracy: 0.8679
Epoch #40 Loss: 0.3572 Accuracy: 0.8912
Epoch #41 Loss: 0.3553 Accuracy: 0.8609
Epoch #42 Loss: 0.3526 Accuracy: 0.8836
Epoch #43 Loss: 0.3485 Accuracy: 0.8936
Epoch #44 Loss: 0.3473 Accuracy: 0.9019
Epoch #45 Loss: 0.3433 Accuracy: 0.8969
Epoch #46 Loss: 0.3404 Accuracy: 0.9049
Epoch #47 Loss: 0.3385 Accuracy: 0.8957
Epoch #48 Loss: 0.3361 Accuracy: 0.8951
Epoch #49 Loss: 0.331 Accuracy: 0.8816
Epoch #50 Loss: 0.3327 Accuracy: 0.8783
Epoch #51 Loss: 0.334 Accuracy: 0.8942
Epoch #52 Loss: 0.3297 Accuracy: 0.9052
Epoch #53 Loss: 0.33 Accuracy: 0.9071
Epoch #54 Loss: 0.3244 Accuracy: 0.905
Epoch #55 Loss: 0.3213 Accuracy: 0.9092
Epoch #56 Loss: 0.3224 Accuracy: 0.911
Epoch #57 Loss: 0.319 Accuracy: 0.9026
Epoch #58 Loss: 0.3161 Accuracy: 0.8915
Epoch #59 Loss: 0.316 Accuracy: 0.9078
Epoch #60 Loss: 0.3127 Accuracy: 0.9003
Epoch #61 Loss: 0.3118 Accuracy: 0.9074
Epoch #62 Loss: 0.3106 Accuracy: 0.8938
Epoch #63 Loss: 0.3125 Accuracy: 0.9112
Epoch #64 Loss: 0.3076 Accuracy: 0.9062
Epoch #65 Loss: 0.307 Accuracy: 0.9005
Epoch #66 Loss: 0.3056 Accuracy: 0.8538
Epoch #67 Loss: 0.3034 Accuracy: 0.9003
Epoch #68 Loss: 0.302 Accuracy: 0.8929
Epoch #69 Loss: 0.2992 Accuracy: 0.9079
Epoch #70 Loss: 0.2992 Accuracy: 0.9103
Epoch #71 Loss: 0.2953 Accuracy: 0.9105
Epoch #72 Loss: 0.2958 Accuracy: 0.914
Epoch #73 Loss: 0.2951 Accuracy: 0.9104
Epoch #74 Loss: 0.2921 Accuracy: 0.9086
Epoch #75 Loss: 0.2933 Accuracy: 0.8952
Epoch #76 Loss: 0.2889 Accuracy: 0.9094
Epoch #77 Loss: 0.2878 Accuracy: 0.8972
Epoch #78 Loss: 0.2882 Accuracy: 0.908
Epoch #79 Loss: 0.2867 Accuracy: 0.9147
Epoch #80 Loss: 0.2867 Accuracy: 0.9047
Epoch #81 Loss: 0.2856 Accuracy: 0.9044
Epoch #82 Loss: 0.2822 Accuracy: 0.9166
Epoch #83 Loss: 0.2823 Accuracy: 0.9145
Epoch #84 Loss: 0.2803 Accuracy: 0.9143
Epoch #85 Loss: 0.2796 Accuracy: 0.9025
Epoch #86 Loss: 0.2805 Accuracy: 0.9007
Epoch #87 Loss: 0.2797 Accuracy: 0.9041
Epoch #88 Loss: 0.2775 Accuracy: 0.9074
Epoch #89 Loss: 0.2768 Accuracy: 0.9158
Epoch #90 Loss: 0.2758 Accuracy: 0.9051
Epoch #91 Loss: 0.2744 Accuracy: 0.9112
Epoch #92 Loss: 0.2721 Accuracy: 0.9094
Epoch #93 Loss: 0.2719 Accuracy: 0.8801
Epoch #94 Loss: 0.2721 Accuracy: 0.9114
Epoch #95 Loss: 0.2695 Accuracy: 0.9176
Epoch #96 Loss: 0.2674 Accuracy: 0.9097
Epoch #97 Loss: 0.2689 Accuracy: 0.9131
Epoch #98 Loss: 0.2671 Accuracy: 0.9178
Epoch #99 Loss: 0.2668 Accuracy: 0.9119
</pre>
</div>
</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>91%! (toggle the output to see the various training losses and accuracy scores over each epoch). Let's also test our beloved sample '5' prediction.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">val_5</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">testing</span><span class="p">[</span><span class="s1">'5'</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_5</span><span class="p">)</span>
<span class="n">a</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([5]),
 tensor([[8.7256e-06, 6.7960e-08, 2.6373e-04, 1.8036e-02, 1.8940e-16, 9.4832e-01, 2.3715e-09, 6.1468e-06, 3.3328e-02, 4.1378e-05]], grad_fn=&lt;SoftmaxBackward&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It gives us a confidence of 95% that the digit is 5. Nice! Now let's discuss our results in more detail.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Analyzing-Results">
<a class="anchor" href="#Analyzing-Results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Analyzing Results<a class="anchor-link" href="#Analyzing-Results"> </a>
</h3>
<p>Deep Neural Networks need to train longer as there are more parameters to tune. In our case, our 2-layer network after 100 epochs reached 91-92% accuracy with a learning rate of 0.1. Aside from the depth (layers) and breadth (parameters) of the model, learning rate also affects the number of epochs it takes as it "weighs down" the updating "power" of the optimizer so as to not overshoot the minimum loss. All of these decisions are called <strong>hyper-parameters</strong>: parameters that are used to configured our model. They are called differents o as not to confuse them with our underlying model's parameters (weights and baises).</p>
<p>By looking at the results above, we see how the training loss is still has room for improvement (remember, we're trying to get that as close to 0 as possible). When training a model it's important to consider over-fitting vs model accuracy. If your validation accuracy and training loss are both moving in the direction you want, it means you still have room to train! It doesn't make sense to continue training if instead you see the validation accuracy degrading consistently as that would be a clear sign of over-fitting on the training data.</p>
<p>To visualize all this better, let's create a quick function to visualize how our training loss and validation accuracy evolve over time (epochs).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">my_nn</span><span class="p">):</span>
  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

  <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">my_nn</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="p">))</span>
  <span class="n">y1</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">accuracy_scores</span>
  <span class="n">y2</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">training_losses</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s1">'g-'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s1">'b-'</span><span class="p">)</span>

  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Validation Accuracy'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Training Loss'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Results with lr=</span><span class="si">{</span><span class="n">my_nn</span><span class="o">.</span><span class="n">lr</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_results</span><span class="p">(</span><span class="n">my_nn</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The graph above shows how, over the course of 100 epochs, our training loss (in blue) and our validation accuracy (in green) have evolved. You'll see that our validation accuracy jumps around--that's because our model is only training on the training dataset, and our validation scores are measured on the validation dataset, so they don't get fed back into the model! This helps keep the model "honest", and gives us an idea of our our model will perform on unseen data.  As both our accuracy and loss are trending in the right direction, we could experiment extending the epoch count and tweaking the learning rate.</p>
<h3 id="Learning-Rates">
<a class="anchor" href="#Learning-Rates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Rates<a class="anchor-link" href="#Learning-Rates"> </a>
</h3>
<p>Below are the same graphs with different learning rates, showing how they affect the model's performance over 100 epochs. As you can see, the higher the learning rate, the faster the accuracy improves, but it causes more jitter as the optimizing steps can overshoot across the loss function. The trick here is to pick one proportional to the amount of epochs you want to train (lower for more epochs, smaller for less epochs). Modern libraries (eg fast.ai) have convenient learning rate finders to take the guess work out.</p>
<p>Note that if you fit your model, and then try to fit it again, it will start from the previous weights, so you should probably pick a smaller learning rate the second time around as the "big" movements have already been done.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-trio.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice how the lower the learning rate, the smoother the training, but slower the ramp up and accuracy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Epochs">
<a class="anchor" href="#Epochs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Epochs<a class="anchor-link" href="#Epochs"> </a>
</h3>
<p>Now let's experiment by using a somewhat large 0.5 learning rate and training for 2.5x more epochs (250 total).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_nn</span> <span class="o">=</span> <span class="n">DeepClassifier</span><span class="p">(</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">my_nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">valid_dl</span><span class="o">=</span><span class="n">valid_dl</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">250</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch #0 Loss: 1.7751 Accuracy: 0.6389
Epoch #1 Loss: 0.9891 Accuracy: 0.6981
Epoch #2 Loss: 0.7729 Accuracy: 0.7354
Epoch #3 Loss: 0.6788 Accuracy: 0.7412
Epoch #4 Loss: 0.613 Accuracy: 0.8305
Epoch #5 Loss: 0.5669 Accuracy: 0.7853
Epoch #6 Loss: 0.5323 Accuracy: 0.8262
Epoch #7 Loss: 0.4896 Accuracy: 0.8599
Epoch #8 Loss: 0.4714 Accuracy: 0.8681
Epoch #9 Loss: 0.4435 Accuracy: 0.873
Epoch #10 Loss: 0.4322 Accuracy: 0.8764
Epoch #11 Loss: 0.4081 Accuracy: 0.8563
Epoch #12 Loss: 0.3952 Accuracy: 0.8856
Epoch #13 Loss: 0.3825 Accuracy: 0.8837
Epoch #14 Loss: 0.3687 Accuracy: 0.8929
Epoch #15 Loss: 0.3569 Accuracy: 0.8975
Epoch #16 Loss: 0.344 Accuracy: 0.8987
Epoch #17 Loss: 0.3389 Accuracy: 0.9014
Epoch #18 Loss: 0.3296 Accuracy: 0.9057
Epoch #19 Loss: 0.3199 Accuracy: 0.9066
Epoch #20 Loss: 0.3178 Accuracy: 0.9041
Epoch #21 Loss: 0.3072 Accuracy: 0.8971
Epoch #22 Loss: 0.302 Accuracy: 0.899
Epoch #23 Loss: 0.3025 Accuracy: 0.9099
Epoch #24 Loss: 0.2941 Accuracy: 0.9134
Epoch #25 Loss: 0.2913 Accuracy: 0.9169
Epoch #26 Loss: 0.2833 Accuracy: 0.8979
Epoch #27 Loss: 0.2781 Accuracy: 0.9188
Epoch #28 Loss: 0.2759 Accuracy: 0.9042
Epoch #29 Loss: 0.2744 Accuracy: 0.9164
Epoch #30 Loss: 0.2717 Accuracy: 0.9194
Epoch #31 Loss: 0.2653 Accuracy: 0.9067
Epoch #32 Loss: 0.2633 Accuracy: 0.9209
Epoch #33 Loss: 0.2592 Accuracy: 0.9164
Epoch #34 Loss: 0.255 Accuracy: 0.9213
Epoch #35 Loss: 0.255 Accuracy: 0.8712
Epoch #36 Loss: 0.2509 Accuracy: 0.9276
Epoch #37 Loss: 0.2467 Accuracy: 0.9143
Epoch #38 Loss: 0.2446 Accuracy: 0.9174
Epoch #39 Loss: 0.2424 Accuracy: 0.9229
Epoch #40 Loss: 0.2427 Accuracy: 0.91
Epoch #41 Loss: 0.2387 Accuracy: 0.9168
Epoch #42 Loss: 0.2385 Accuracy: 0.9262
Epoch #43 Loss: 0.233 Accuracy: 0.911
Epoch #44 Loss: 0.2315 Accuracy: 0.9234
Epoch #45 Loss: 0.2321 Accuracy: 0.9221
Epoch #46 Loss: 0.228 Accuracy: 0.9074
Epoch #47 Loss: 0.2272 Accuracy: 0.9271
Epoch #48 Loss: 0.2257 Accuracy: 0.9249
Epoch #49 Loss: 0.2232 Accuracy: 0.9246
Epoch #50 Loss: 0.2239 Accuracy: 0.9265
Epoch #51 Loss: 0.2205 Accuracy: 0.9291
Epoch #52 Loss: 0.2181 Accuracy: 0.914
Epoch #53 Loss: 0.2172 Accuracy: 0.9203
Epoch #54 Loss: 0.2151 Accuracy: 0.9182
Epoch #55 Loss: 0.2154 Accuracy: 0.9282
Epoch #56 Loss: 0.215 Accuracy: 0.9323
Epoch #57 Loss: 0.2115 Accuracy: 0.9291
Epoch #58 Loss: 0.2086 Accuracy: 0.9319
Epoch #59 Loss: 0.2092 Accuracy: 0.9187
Epoch #60 Loss: 0.2074 Accuracy: 0.9312
Epoch #61 Loss: 0.2067 Accuracy: 0.926
Epoch #62 Loss: 0.2053 Accuracy: 0.936
Epoch #63 Loss: 0.2039 Accuracy: 0.9301
Epoch #64 Loss: 0.2027 Accuracy: 0.9297
Epoch #65 Loss: 0.2007 Accuracy: 0.934
Epoch #66 Loss: 0.1998 Accuracy: 0.9223
Epoch #67 Loss: 0.2011 Accuracy: 0.9325
Epoch #68 Loss: 0.1985 Accuracy: 0.8839
Epoch #69 Loss: 0.199 Accuracy: 0.918
Epoch #70 Loss: 0.1978 Accuracy: 0.9266
Epoch #71 Loss: 0.1965 Accuracy: 0.9358
Epoch #72 Loss: 0.1954 Accuracy: 0.9352
Epoch #73 Loss: 0.1931 Accuracy: 0.9331
Epoch #74 Loss: 0.1925 Accuracy: 0.9264
Epoch #75 Loss: 0.1929 Accuracy: 0.9303
Epoch #76 Loss: 0.1907 Accuracy: 0.9367
Epoch #77 Loss: 0.1906 Accuracy: 0.9142
Epoch #78 Loss: 0.1906 Accuracy: 0.9322
Epoch #79 Loss: 0.1887 Accuracy: 0.9319
Epoch #80 Loss: 0.187 Accuracy: 0.9371
Epoch #81 Loss: 0.1875 Accuracy: 0.9312
Epoch #82 Loss: 0.1848 Accuracy: 0.9226
Epoch #83 Loss: 0.1835 Accuracy: 0.9337
Epoch #84 Loss: 0.1837 Accuracy: 0.9375
Epoch #85 Loss: 0.1836 Accuracy: 0.9378
Epoch #86 Loss: 0.1829 Accuracy: 0.9172
Epoch #87 Loss: 0.1828 Accuracy: 0.935
Epoch #88 Loss: 0.1811 Accuracy: 0.93
Epoch #89 Loss: 0.1806 Accuracy: 0.9255
Epoch #90 Loss: 0.1786 Accuracy: 0.9409
Epoch #91 Loss: 0.1798 Accuracy: 0.9369
Epoch #92 Loss: 0.1777 Accuracy: 0.9365
Epoch #93 Loss: 0.1774 Accuracy: 0.9386
Epoch #94 Loss: 0.176 Accuracy: 0.9317
Epoch #95 Loss: 0.1775 Accuracy: 0.939
Epoch #96 Loss: 0.1745 Accuracy: 0.9374
Epoch #97 Loss: 0.1752 Accuracy: 0.9396
Epoch #98 Loss: 0.1733 Accuracy: 0.9371
Epoch #99 Loss: 0.1734 Accuracy: 0.9242
Epoch #100 Loss: 0.1725 Accuracy: 0.9359
Epoch #101 Loss: 0.174 Accuracy: 0.9354
Epoch #102 Loss: 0.1716 Accuracy: 0.8946
Epoch #103 Loss: 0.1701 Accuracy: 0.9367
Epoch #104 Loss: 0.1712 Accuracy: 0.9365
Epoch #105 Loss: 0.1681 Accuracy: 0.9356
Epoch #106 Loss: 0.1673 Accuracy: 0.9351
Epoch #107 Loss: 0.1679 Accuracy: 0.9362
Epoch #108 Loss: 0.1677 Accuracy: 0.9446
Epoch #109 Loss: 0.1669 Accuracy: 0.9212
Epoch #110 Loss: 0.166 Accuracy: 0.9394
Epoch #111 Loss: 0.1648 Accuracy: 0.9427
Epoch #112 Loss: 0.1647 Accuracy: 0.9346
Epoch #113 Loss: 0.1641 Accuracy: 0.938
Epoch #114 Loss: 0.1635 Accuracy: 0.9395
Epoch #115 Loss: 0.1631 Accuracy: 0.9348
Epoch #116 Loss: 0.1634 Accuracy: 0.94
Epoch #117 Loss: 0.1619 Accuracy: 0.9359
Epoch #118 Loss: 0.1617 Accuracy: 0.9387
Epoch #119 Loss: 0.1615 Accuracy: 0.941
Epoch #120 Loss: 0.1605 Accuracy: 0.9393
Epoch #121 Loss: 0.1594 Accuracy: 0.9404
Epoch #122 Loss: 0.1589 Accuracy: 0.9408
Epoch #123 Loss: 0.1586 Accuracy: 0.9412
Epoch #124 Loss: 0.1577 Accuracy: 0.9394
Epoch #125 Loss: 0.1577 Accuracy: 0.937
Epoch #126 Loss: 0.1582 Accuracy: 0.9396
Epoch #127 Loss: 0.1568 Accuracy: 0.9353
Epoch #128 Loss: 0.1571 Accuracy: 0.9432
Epoch #129 Loss: 0.1561 Accuracy: 0.9309
Epoch #130 Loss: 0.1553 Accuracy: 0.9401
Epoch #131 Loss: 0.1557 Accuracy: 0.9359
Epoch #132 Loss: 0.1532 Accuracy: 0.9406
Epoch #133 Loss: 0.1532 Accuracy: 0.937
Epoch #134 Loss: 0.1541 Accuracy: 0.9371
Epoch #135 Loss: 0.1526 Accuracy: 0.9378
Epoch #136 Loss: 0.1518 Accuracy: 0.9429
Epoch #137 Loss: 0.152 Accuracy: 0.9403
Epoch #138 Loss: 0.1517 Accuracy: 0.9401
Epoch #139 Loss: 0.1507 Accuracy: 0.9383
Epoch #140 Loss: 0.1511 Accuracy: 0.9437
Epoch #141 Loss: 0.1498 Accuracy: 0.9434
Epoch #142 Loss: 0.1498 Accuracy: 0.9444
Epoch #143 Loss: 0.1502 Accuracy: 0.9408
Epoch #144 Loss: 0.149 Accuracy: 0.9457
Epoch #145 Loss: 0.1475 Accuracy: 0.9382
Epoch #146 Loss: 0.1483 Accuracy: 0.9436
Epoch #147 Loss: 0.1476 Accuracy: 0.9399
Epoch #148 Loss: 0.1469 Accuracy: 0.9373
Epoch #149 Loss: 0.1466 Accuracy: 0.9434
Epoch #150 Loss: 0.1458 Accuracy: 0.9418
Epoch #151 Loss: 0.1468 Accuracy: 0.941
Epoch #152 Loss: 0.1457 Accuracy: 0.9421
Epoch #153 Loss: 0.1459 Accuracy: 0.9434
Epoch #154 Loss: 0.1457 Accuracy: 0.943
Epoch #155 Loss: 0.1445 Accuracy: 0.944
Epoch #156 Loss: 0.1438 Accuracy: 0.9339
Epoch #157 Loss: 0.1442 Accuracy: 0.9331
Epoch #158 Loss: 0.1432 Accuracy: 0.9449
Epoch #159 Loss: 0.1437 Accuracy: 0.9409
Epoch #160 Loss: 0.1429 Accuracy: 0.9454
Epoch #161 Loss: 0.1426 Accuracy: 0.9386
Epoch #162 Loss: 0.1423 Accuracy: 0.9441
Epoch #163 Loss: 0.1415 Accuracy: 0.9434
Epoch #164 Loss: 0.1409 Accuracy: 0.9437
Epoch #165 Loss: 0.1414 Accuracy: 0.9368
Epoch #166 Loss: 0.1413 Accuracy: 0.9432
Epoch #167 Loss: 0.1405 Accuracy: 0.9458
Epoch #168 Loss: 0.14 Accuracy: 0.9451
Epoch #169 Loss: 0.1401 Accuracy: 0.9401
Epoch #170 Loss: 0.1391 Accuracy: 0.946
Epoch #171 Loss: 0.1391 Accuracy: 0.9427
Epoch #172 Loss: 0.1385 Accuracy: 0.9467
Epoch #173 Loss: 0.1383 Accuracy: 0.944
Epoch #174 Loss: 0.138 Accuracy: 0.9447
Epoch #175 Loss: 0.1378 Accuracy: 0.944
Epoch #176 Loss: 0.1367 Accuracy: 0.9349
Epoch #177 Loss: 0.1369 Accuracy: 0.943
Epoch #178 Loss: 0.1375 Accuracy: 0.9405
Epoch #179 Loss: 0.1365 Accuracy: 0.9472
Epoch #180 Loss: 0.1364 Accuracy: 0.9111
Epoch #181 Loss: 0.1372 Accuracy: 0.9347
Epoch #182 Loss: 0.1349 Accuracy: 0.9419
Epoch #183 Loss: 0.1344 Accuracy: 0.947
Epoch #184 Loss: 0.1343 Accuracy: 0.9422
Epoch #185 Loss: 0.1337 Accuracy: 0.9431
Epoch #186 Loss: 0.1338 Accuracy: 0.9449
Epoch #187 Loss: 0.1329 Accuracy: 0.9446
Epoch #188 Loss: 0.1337 Accuracy: 0.9471
Epoch #189 Loss: 0.1334 Accuracy: 0.9419
Epoch #190 Loss: 0.1332 Accuracy: 0.945
Epoch #191 Loss: 0.1331 Accuracy: 0.9424
Epoch #192 Loss: 0.1324 Accuracy: 0.9388
Epoch #193 Loss: 0.1319 Accuracy: 0.9441
Epoch #194 Loss: 0.1322 Accuracy: 0.9436
Epoch #195 Loss: 0.1315 Accuracy: 0.9414
Epoch #196 Loss: 0.1313 Accuracy: 0.9418
Epoch #197 Loss: 0.1309 Accuracy: 0.9415
Epoch #198 Loss: 0.1308 Accuracy: 0.9414
Epoch #199 Loss: 0.1304 Accuracy: 0.9364
Epoch #200 Loss: 0.1299 Accuracy: 0.9451
Epoch #201 Loss: 0.1282 Accuracy: 0.9452
Epoch #202 Loss: 0.1289 Accuracy: 0.9432
Epoch #203 Loss: 0.1285 Accuracy: 0.946
Epoch #204 Loss: 0.1285 Accuracy: 0.9405
Epoch #205 Loss: 0.1284 Accuracy: 0.9372
Epoch #206 Loss: 0.1291 Accuracy: 0.9208
Epoch #207 Loss: 0.1286 Accuracy: 0.9455
Epoch #208 Loss: 0.1275 Accuracy: 0.946
Epoch #209 Loss: 0.1274 Accuracy: 0.9418
Epoch #210 Loss: 0.1262 Accuracy: 0.9455
Epoch #211 Loss: 0.1276 Accuracy: 0.9294
Epoch #212 Loss: 0.1265 Accuracy: 0.9461
Epoch #213 Loss: 0.1262 Accuracy: 0.9446
Epoch #214 Loss: 0.1262 Accuracy: 0.9404
Epoch #215 Loss: 0.1264 Accuracy: 0.9455
Epoch #216 Loss: 0.1255 Accuracy: 0.94
Epoch #217 Loss: 0.1254 Accuracy: 0.9456
Epoch #218 Loss: 0.1251 Accuracy: 0.9431
Epoch #219 Loss: 0.125 Accuracy: 0.9403
Epoch #220 Loss: 0.1251 Accuracy: 0.9384
Epoch #221 Loss: 0.1238 Accuracy: 0.9441
Epoch #222 Loss: 0.1246 Accuracy: 0.9445
Epoch #223 Loss: 0.1244 Accuracy: 0.9426
Epoch #224 Loss: 0.1243 Accuracy: 0.9451
Epoch #225 Loss: 0.1228 Accuracy: 0.9445
Epoch #226 Loss: 0.1238 Accuracy: 0.9471
Epoch #227 Loss: 0.1229 Accuracy: 0.9419
Epoch #228 Loss: 0.1226 Accuracy: 0.9423
Epoch #229 Loss: 0.1217 Accuracy: 0.9407
Epoch #230 Loss: 0.1216 Accuracy: 0.9418
Epoch #231 Loss: 0.1222 Accuracy: 0.9415
Epoch #232 Loss: 0.1211 Accuracy: 0.942
Epoch #233 Loss: 0.1221 Accuracy: 0.937
Epoch #234 Loss: 0.1217 Accuracy: 0.9425
Epoch #235 Loss: 0.1208 Accuracy: 0.9453
Epoch #236 Loss: 0.1208 Accuracy: 0.9472
Epoch #237 Loss: 0.1204 Accuracy: 0.9354
Epoch #238 Loss: 0.1201 Accuracy: 0.9421
Epoch #239 Loss: 0.1191 Accuracy: 0.9463
Epoch #240 Loss: 0.1196 Accuracy: 0.9423
Epoch #241 Loss: 0.12 Accuracy: 0.9432
Epoch #242 Loss: 0.1188 Accuracy: 0.9446
Epoch #243 Loss: 0.1187 Accuracy: 0.9443
Epoch #244 Loss: 0.1181 Accuracy: 0.9442
Epoch #245 Loss: 0.1185 Accuracy: 0.9457
Epoch #246 Loss: 0.118 Accuracy: 0.9451
Epoch #247 Loss: 0.1175 Accuracy: 0.943
Epoch #248 Loss: 0.1181 Accuracy: 0.9462
Epoch #249 Loss: 0.117 Accuracy: 0.9393
</pre>
</div>
</div>

</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_results</span><span class="p">(</span><span class="n">my_nn</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While overall we achieved better accuracy (around 94%), we can start to see that after epoch ~175 the accuracy flattens out without significant improvement. This is a sign that now are model is just overfitting or that our learning rate is too coarse to fine-tune the model above ~94% accuracy. We'll see this issue a lot clearer if we plot the results again starting from epochs #100 onwards.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

  <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">my_nn</span><span class="o">.</span><span class="n">accuracy_scores</span><span class="p">))</span>
  <span class="n">y1</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">accuracy_scores</span>
  <span class="n">y2</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">training_losses</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="n">y1</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="s1">'g-'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="n">y2</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span> <span class="s1">'b-'</span><span class="p">)</span>

  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Validation Accuracy'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Training Loss'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Results with lr=</span><span class="si">{</span><span class="n">my_nn</span><span class="o">.</span><span class="n">lr</span><span class="si">}</span><span class="s2">, from epoch 100"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lastly, let's also check what happens if we train with lower learning rates (0.005 and 0.01) and train for even longer (500 epochs). Given the large number of outputs I will omit the code snippets, but you get the hand of it by now!</p>
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-lr005.png" alt="" title="lr=0.005, trained for 500 epochs"></p>
<p>In the case of <code>lr=0.005</code>, after 500 epochs final accuracy was around 88%, and the loss around 0.39 due to the low steps. The trends of both lines suggest it could do with more training.</p>
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-lr01l.png" alt="" title="lr=0.01, trained for 500 epochs"></p>
<p>For <code>lr=0.01</code>, we were able to achieve better accuracy (91%) but the loss function still hadn't completely plateaued (roughly around 0.30). Let's try again with a larger learning rate of 0.1 (10x bigger!)</p>
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-lr10l.png" alt="" title="lr=0.1, trained for 500 epochs"></p>
<p>With <code>lr=0.01</code> were able to reach 93% accuracy and get the loss down to 0.18! but as you can see, the larger the learning rate, the rougher our learning curve (in terms of accuracy). This is because our parameters jump around much more. Let's choose a learning curve inbetween 0.01 and 0.1 (i.e. 0.05) and train for 1000 epochs!</p>
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-lr05l.png" alt="" title="lr=0.05, trained for 1000 epochs"></p>
<p>With <code>lr=0.05</code>, after 1000 epochs we ended up with around 94.5% accuracy. However we already reached 94.5% accuracy back in epochs 700-800. When you train for an arbitrary number of epochs it could be that depending on your chosen configuration, your model reaches an optimal accuracy somewhere in the middle of the training. This is why many frameworks implement an option called early stopping rounds whereby you can instruct the model to stop training when accuracy hasn't improved after $n$ rounds. From the image above it seems like there isn't much more room to grow as the curves flatten out after epochs 750 or so. However, if we zoom in on our graph, while rickety, we can see there's still a clear trend on both metrics and could be worth fitting for a second cycle, perhaps with a lower learning rate.</p>
<p><img src="/blog/images/copied_from_nb/images/2lyr-nn-results-lr05lz.png" alt="" title="lr=0.05, trained for 1000 epochs, showing epochs 600-1000"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Layers">
<a class="anchor" href="#Layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layers<a class="anchor-link" href="#Layers"> </a>
</h3>
<p>I can already hear you asking: "ok, I got the point about learning rates and epochs, but <strong>what about the depth</strong>?". Well, let's find out! Let's add a hidden layer with 20 input parameters and 20 output parameters.</p>
<p>Let's train it with a learning rate of 0.05 and let's leave it running for a long time (2000 epochs), to see how it evolves.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_nn</span> <span class="o">=</span> <span class="n">DeepClassifier</span><span class="p">(</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
  <span class="n">LinearModel</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">my_nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dl</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">valid_dl</span><span class="o">=</span><span class="n">valid_dl</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/images/copied_from_nb/images/3lyr-nn-results-lr05xl.png" alt="" title="1 hidden layer, lr=0.05, 2000 epochs"></p>
<p>First of all, let's pat ourselves on the back as we just ran our first real deep neural network with a hidden layer! From the graph above we can see our model's performance. It reached 95% accuracy and a loss of around 0.127. Here's a better look at the graph from epochs 1000 to 2000.</p>
<p><img src="/blog/images/copied_from_nb/images/3lyr-nn-results-lr05xlz.png" alt="" title="1 hidden layer, lr=0.05, 2000 epochs, from epoch 1000"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that we could add as many layers as we wanted, but remember the tradeoff between complexity and training time and over-fitting! I encourage you to try experimenting with different layers and tweak our hyper-parameters accordingly.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hopefully all these experiments show that machine learning is a craft of delicately balancing different constraints and making trade-offs, with only loose guidelines to guide our decision making! It is definitely not an exact science with strict, clear cut rules. There are so many hyper-parameters to take into consideration: batch sizes, learning rates, epochs, dataset size, layer count, parameter count, and variables to account for (quality of data, computing resources, etc). And that's not including other important decisions such as what type of loss function to use (spoiler alert, cross-entropy loss is only one of many!), what activation function to use, etc. This is is why machine learning is a field where there's plenty of room for experimentation and is full of unexplored territory.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusions">
<a class="anchor" href="#Conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions<a class="anchor-link" href="#Conclusions"> </a>
</h2>
<p>This marks the end of this series of building a Deep Neural Network from scratch. I realize these are two very long posts, but rather than split them up for the sake of splitting them up, I decided to condense everything into 2 parts so it's easier to keep track of links and follow along step by step. As linked above, the final <code>DeepClassifier</code> code is available <a href="https://gist.github.com/muttoni/d5ce076fdc83b8f82b9971c5c8bf6b2d">here</a>. Follow the steps in the series and experiment with all the possible parameter choices!</p>
<p>I really hope you enjoyed the series and learned something along the way. If you want to reach out, feel free to find my email in the "About" page or leave a comment below. With that, I'll close and wish you all the best in your machine learning journey!</p>
<h3 id="Aknowledgements">
<a class="anchor" href="#Aknowledgements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Aknowledgements<a class="anchor-link" href="#Aknowledgements"> </a>
</h3>
<p>This series could not have been possible without the incredible resource that the <a href="http://course.fast.ai/">fast.ai</a> course is! I highly recommend you check it out as you will learn most of what's covered here and more! Most of what we covered in this series is part of Lessons 4 and 5 of the 2020 course. A big thank you to Jeremy and the fast.ai team for creating such a wonderful and inspiring course.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="muttoni/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/machine-learning/2021/01/01/Implementing-a-Deep-Neural-Network-from-Scratch-Part-2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/blog/"></data>
    
    <div class="wrapper">
        <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/muttoni" title="muttoni"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/muttoni" title="muttoni"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/muttonia" title="muttonia"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>
        <div class="footer-col-wrapper">
            <div class="footer-col">
                <p class="feed-subscribe"  style="text-align:center;">
                    <a href="/blog/feed.xml">
                        <svg class="svg-icon orange">
                            <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
                        </svg><span>Subscribe</span>
                    </a>
                </p>
            </div>
        </div>
        
    </div>
    
</footer></body>

</html>
